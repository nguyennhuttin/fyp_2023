{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e6cd93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import argparse\n",
    "import importlib\n",
    "\n",
    "# torchim:\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "# from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import pytorch_warmup as warmup\n",
    "\n",
    "# data:\n",
    "import data\n",
    "from data.collate import collate_fn, gpu_collate, no_pad_collate\n",
    "from data.transforms import (\n",
    "    Compose, AddLengths, AudioSqueeze, TextPreprocess,\n",
    "    MaskSpectrogram, ToNumpy, BPEtexts, MelSpectrogram,\n",
    "    ToGpu, Pad, NormalizedMelSpectrogram\n",
    ")\n",
    "import youtokentome as yttm\n",
    "\n",
    "import torchaudio\n",
    "from audiomentations import (\n",
    "    TimeStretch, PitchShift, AddGaussianNoise\n",
    ")\n",
    "from functools import partial\n",
    "\n",
    "# model:\n",
    "from model import configs as quartznet_configs\n",
    "from model.quartznet import QuartzNet\n",
    "\n",
    "# utils:\n",
    "import yaml\n",
    "from easydict import EasyDict as edict\n",
    "from utils import fix_seeds, remove_from_dict, prepare_bpe\n",
    "import wandb\n",
    "from decoder import GreedyDecoder, BeamCTCDecoder\n",
    "\n",
    "import youtokentome as yttm\n",
    "\n",
    "\n",
    "# TODO: wrap to trainer class\n",
    "\n",
    "\n",
    "def train(config):\n",
    "    fix_seeds(seed=config.train.get('seed', 42))\n",
    "    dataset_module = importlib.import_module(\n",
    "        f'.{config.dataset.name}', data.__name__)\n",
    "    bpe = prepare_bpe(config)\n",
    "\n",
    "    transforms_train = Compose([\n",
    "        TextPreprocess(),\n",
    "        ToNumpy(),\n",
    "        BPEtexts(bpe=bpe, dropout_prob=config.bpe.get('dropout_prob', 0.05)),\n",
    "        AudioSqueeze(),\n",
    "        AddGaussianNoise(\n",
    "            min_amplitude=0.001,\n",
    "            max_amplitude=0.015,\n",
    "            p=0.5\n",
    "        ),\n",
    "        TimeStretch(\n",
    "            min_rate=0.8,\n",
    "            max_rate=1.25,\n",
    "            p=0.5\n",
    "        ),\n",
    "        PitchShift(\n",
    "            min_semitones=-4,\n",
    "            max_semitones=4,\n",
    "            p=0.5\n",
    "        )\n",
    "        # AddLengths()\n",
    "    ])\n",
    "\n",
    "    batch_transforms_train = Compose([\n",
    "        ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "        NormalizedMelSpectrogram(\n",
    "            sample_rate=config.dataset.get('sample_rate', 16000),\n",
    "            n_mels=config.model.feat_in,\n",
    "            normalize=config.dataset.get('normalize', None)\n",
    "        ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "        MaskSpectrogram(\n",
    "            probability=0.5,\n",
    "            time_mask_max_percentage=0.05,\n",
    "            frequency_mask_max_percentage=0.15\n",
    "        ),\n",
    "        AddLengths(),\n",
    "        Pad()\n",
    "    ])\n",
    "\n",
    "    transforms_val = Compose([\n",
    "        TextPreprocess(),\n",
    "        ToNumpy(),\n",
    "        BPEtexts(bpe=bpe),\n",
    "        AudioSqueeze()\n",
    "    ])\n",
    "\n",
    "    batch_transforms_val = Compose([\n",
    "        ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "        NormalizedMelSpectrogram(\n",
    "            sample_rate=config.dataset.get(\n",
    "                'sample_rate', 16000),  # for LJspeech\n",
    "            n_mels=config.model.feat_in,\n",
    "            normalize=config.dataset.get('normalize', None)\n",
    "        ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "        AddLengths(),\n",
    "        Pad()\n",
    "    ])\n",
    "\n",
    "    # load datasets\n",
    "    train_dataset = dataset_module.get_dataset(\n",
    "        config, transforms=transforms_train, part='train')\n",
    "    val_dataset = dataset_module.get_dataset(\n",
    "        config, transforms=transforms_val, part='val')\n",
    "    print(\"!!!\", config.train.get('num_workers', 4))\n",
    "    train_dataloader = DataLoader(train_dataset, \n",
    "                                  batch_size=config.train.get('batch_size', 1), collate_fn=no_pad_collate)\n",
    "\n",
    "    val_dataloader = DataLoader(val_dataset, \n",
    "                                batch_size=1, collate_fn=no_pad_collate)\n",
    "\n",
    "    model = QuartzNet(\n",
    "        model_config=getattr(\n",
    "            quartznet_configs, config.model.name, '_quartznet5x5_config'),\n",
    "        **remove_from_dict(config.model, ['name'])\n",
    "    )\n",
    "\n",
    "    print(model)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), **config.train.get('optimizer', {}))\n",
    "    num_steps = len(train_dataloader) * config.train.get('epochs', 10)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_steps)\n",
    "    # warmup_scheduler = warmup.UntunedLinearWarmup(optimizer)\n",
    "\n",
    "    if config.train.get('from_checkpoint', None) is not None:\n",
    "        model.load_weights(config.train.from_checkpoint)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "    # criterion = nn.CTCLoss(blank=config.model.vocab_size)\n",
    "    decoder = GreedyDecoder(bpe=bpe)\n",
    "\n",
    "    prev_wer = 1000\n",
    "    wandb.init(project=config.wandb.project, config=config)\n",
    "    wandb.watch(model, log=\"all\", log_freq=config.wandb.get(\n",
    "        'log_interval', 5000))\n",
    "    for epoch_idx in tqdm(range(config.train.get('epochs', 10))):\n",
    "        # train:\n",
    "        model.train()\n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            print(batch)\n",
    "            batch = batch_transforms_train(batch)\n",
    "            print(batch)\n",
    "            return\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch['audio'])\n",
    "            output_length = torch.ceil(\n",
    "                batch['input_lengths'].float() / model.stride).int()\n",
    "            loss = criterion(logits.permute(2, 0, 1).log_softmax(\n",
    "                dim=2), batch['text'], output_length, batch['target_lengths'])\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                model.parameters(), config.train.get('clip_grad_norm', 15))\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            # warmup_scheduler.dampen()\n",
    "\n",
    "            if batch_idx % config.wandb.get('log_interval', 5000) == 0:\n",
    "                target_strings = decoder.convert_to_strings(batch['text'])\n",
    "                decoded_output = decoder.decode(\n",
    "                    logits.permute(0, 2, 1).softmax(dim=2))\n",
    "                wer = np.mean([decoder.wer(true, pred)\n",
    "                              for true, pred in zip(target_strings, decoded_output)])\n",
    "                cer = np.mean([decoder.cer(true, pred)\n",
    "                              for true, pred in zip(target_strings, decoded_output)])\n",
    "                step = epoch_idx * \\\n",
    "                    len(train_dataloader) * train_dataloader.batch_size + \\\n",
    "                    batch_idx * train_dataloader.batch_size\n",
    "                wandb.log({\n",
    "                    \"train_loss\": loss.item(),\n",
    "                    \"train_wer\": wer,\n",
    "                    \"train_cer\": cer,\n",
    "                    \"train_samples\": wandb.Table(\n",
    "                        columns=['gt_text', 'pred_text'],\n",
    "                        data=zip(target_strings, decoded_output)\n",
    "                    )\n",
    "                }, step=step)\n",
    "\n",
    "        # validate:\n",
    "        model.eval()\n",
    "        val_stats = defaultdict(list)\n",
    "        for batch_idx, batch in enumerate(val_dataloader):\n",
    "            batch = batch_transforms_val(batch)\n",
    "            with torch.no_grad():\n",
    "                logits = model(batch['audio'])\n",
    "                output_length = torch.ceil(\n",
    "                    batch['input_lengths'].float() / model.stride).int()\n",
    "                loss = criterion(logits.permute(2, 0, 1).log_softmax(\n",
    "                    dim=2), batch['text'], output_length, batch['target_lengths'])\n",
    "\n",
    "            target_strings = decoder.convert_to_strings(batch['text'])\n",
    "            decoded_output = decoder.decode(\n",
    "                logits.permute(0, 2, 1).softmax(dim=2))\n",
    "            wer = np.mean([decoder.wer(true, pred)\n",
    "                          for true, pred in zip(target_strings, decoded_output)])\n",
    "            cer = np.mean([decoder.cer(true, pred)\n",
    "                          for true, pred in zip(target_strings, decoded_output)])\n",
    "            val_stats['val_loss'].append(loss.item())\n",
    "            val_stats['wer'].append(wer)\n",
    "            val_stats['cer'].append(cer)\n",
    "        for k, v in val_stats.items():\n",
    "            val_stats[k] = np.mean(v)\n",
    "        val_stats['val_samples'] = wandb.Table(\n",
    "            columns=['gt_text', 'pred_text'], data=zip(target_strings, decoded_output))\n",
    "        wandb.log(val_stats, step=step)\n",
    "\n",
    "        # save model, TODO: save optimizer:\n",
    "        if val_stats['wer'] < prev_wer:\n",
    "            os.makedirs(config.train.get(\n",
    "                'checkpoint_path', 'checkpoints'), exist_ok=True)\n",
    "            prev_wer = val_stats['wer']\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(config.train.get(\n",
    "                    'checkpoint_path', 'checkpoints'), f'model_{epoch_idx}_{prev_wer}.pth')\n",
    "            )\n",
    "            wandb.save(os.path.join(config.train.get(\n",
    "                'checkpoint_path', 'checkpoints'), f'model_{epoch_idx}_{prev_wer}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dfd6175",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='Training model.')\n",
    "parser.add_argument('--config', default='configs/train_LJSpeech.yaml',\n",
    "                    help='path to config file')\n",
    "args = parser.parse_args(\"\")\n",
    "with open(args.config, 'r') as f:\n",
    "    config = edict(yaml.safe_load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b679f49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__',\n",
      " '__contains__',\n",
      " '__delattr__',\n",
      " '__dict__',\n",
      " '__dir__',\n",
      " '__doc__',\n",
      " '__eq__',\n",
      " '__format__',\n",
      " '__ge__',\n",
      " '__getattribute__',\n",
      " '__gt__',\n",
      " '__hash__',\n",
      " '__init__',\n",
      " '__init_subclass__',\n",
      " '__le__',\n",
      " '__lt__',\n",
      " '__module__',\n",
      " '__ne__',\n",
      " '__new__',\n",
      " '__reduce__',\n",
      " '__reduce_ex__',\n",
      " '__repr__',\n",
      " '__setattr__',\n",
      " '__sizeof__',\n",
      " '__str__',\n",
      " '__subclasshook__',\n",
      " '__weakref__',\n",
      " '_get_args',\n",
      " '_get_kwargs',\n",
      " 'config']"
     ]
    }
   ],
   "source": [
    "dir(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa38cf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'configs/train_LJSpeech.yaml'"
     ]
    }
   ],
   "source": [
    "args.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e12a311",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seeds(seed=config.train.get('seed', 42))\n",
    "dataset_module = importlib.import_module(\n",
    "    f'.{config.dataset.name}', data.__name__)\n",
    "bpe = prepare_bpe(config)\n",
    "\n",
    "transforms_train = Compose([\n",
    "    TextPreprocess(),\n",
    "    ToNumpy(),\n",
    "    BPEtexts(bpe=bpe, dropout_prob=config.bpe.get('dropout_prob', 0.05)),\n",
    "    AudioSqueeze(),\n",
    "    AddGaussianNoise(\n",
    "        min_amplitude=0.001,\n",
    "        max_amplitude=0.015,\n",
    "        p=0.5\n",
    "    ),\n",
    "    TimeStretch(\n",
    "        min_rate=0.8,\n",
    "        max_rate=1.25,\n",
    "        p=0.5\n",
    "    ),\n",
    "    PitchShift(\n",
    "        min_semitones=-4,\n",
    "        max_semitones=4,\n",
    "        p=0.5\n",
    "    )\n",
    "    # AddLengths()\n",
    "])\n",
    "\n",
    "batch_transforms_train = Compose([\n",
    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    NormalizedMelSpectrogram(\n",
    "        sample_rate=config.dataset.get('sample_rate', 16000),\n",
    "        n_mels=config.model.feat_in,\n",
    "        normalize=config.dataset.get('normalize', None)\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    MaskSpectrogram(\n",
    "        probability=0.5,\n",
    "        time_mask_max_percentage=0.05,\n",
    "        frequency_mask_max_percentage=0.15\n",
    "    ),\n",
    "    AddLengths(),\n",
    "    Pad()\n",
    "])\n",
    "\n",
    "transforms_val = Compose([\n",
    "    TextPreprocess(),\n",
    "    ToNumpy(),\n",
    "    BPEtexts(bpe=bpe),\n",
    "    AudioSqueeze()\n",
    "])\n",
    "\n",
    "batch_transforms_val = Compose([\n",
    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    NormalizedMelSpectrogram(\n",
    "        sample_rate=config.dataset.get(\n",
    "            'sample_rate', 16000),  # for LJspeech\n",
    "        n_mels=config.model.feat_in,\n",
    "        normalize=config.dataset.get('normalize', None)\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    AddLengths(),\n",
    "    Pad()\n",
    "])\n",
    "\n",
    "# load datasets\n",
    "train_dataset = dataset_module.get_dataset(\n",
    "    config, transforms=transforms_train, part='train')\n",
    "val_dataset = dataset_module.get_dataset(\n",
    "    config, transforms=transforms_val, part='val')\n",
    "print(\"!!!\", config.train.get('num_workers', 4))\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                                batch_size=config.train.get('batch_size', 1), collate_fn=no_pad_collate)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=1, collate_fn=no_pad_collate)\n",
    "\n",
    "model = QuartzNet(\n",
    "    model_config=getattr(\n",
    "        quartznet_configs, config.model.name, '_quartznet5x5_config'),\n",
    "    **remove_from_dict(config.model, ['name'])\n",
    ")\n",
    "\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), **config.train.get('optimizer', {}))\n",
    "num_steps = len(train_dataloader) * config.train.get('epochs', 10)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=num_steps)\n",
    "# warmup_scheduler = warmup.UntunedLinearWarmup(optimizer)\n",
    "\n",
    "if config.train.get('from_checkpoint', None) is not None:\n",
    "    model.load_weights(config.train.from_checkpoint)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "# criterion = nn.CTCLoss(blank=config.model.vocab_size)\n",
    "decoder = GreedyDecoder(bpe=bpe)\n",
    "\n",
    "prev_wer = 1000\n",
    "wandb.init(project=config.wandb.project, config=config)\n",
    "wandb.watch(model, log=\"all\", log_freq=config.wandb.get(\n",
    "    'log_interval', 5000))\n",
    "for epoch_idx in tqdm(range(config.train.get('epochs', 10))):\n",
    "    # train:\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        print(batch)\n",
    "        batch = batch_transforms_train(batch)\n",
    "        print(batch)\n",
    "        return\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch['audio'])\n",
    "        output_length = torch.ceil(\n",
    "            batch['input_lengths'].float() / model.stride).int()\n",
    "        loss = criterion(logits.permute(2, 0, 1).log_softmax(\n",
    "            dim=2), batch['text'], output_length, batch['target_lengths'])\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), config.train.get('clip_grad_norm', 15))\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        # warmup_scheduler.dampen()\n",
    "\n",
    "        if batch_idx % config.wandb.get('log_interval', 5000) == 0:\n",
    "            target_strings = decoder.convert_to_strings(batch['text'])\n",
    "            decoded_output = decoder.decode(\n",
    "                logits.permute(0, 2, 1).softmax(dim=2))\n",
    "            wer = np.mean([decoder.wer(true, pred)\n",
    "                            for true, pred in zip(target_strings, decoded_output)])\n",
    "            cer = np.mean([decoder.cer(true, pred)\n",
    "                            for true, pred in zip(target_strings, decoded_output)])\n",
    "            step = epoch_idx * \\\n",
    "                len(train_dataloader) * train_dataloader.batch_size + \\\n",
    "                batch_idx * train_dataloader.batch_size\n",
    "            wandb.log({\n",
    "                \"train_loss\": loss.item(),\n",
    "                \"train_wer\": wer,\n",
    "                \"train_cer\": cer,\n",
    "                \"train_samples\": wandb.Table(\n",
    "                    columns=['gt_text', 'pred_text'],\n",
    "                    data=zip(target_strings, decoded_output)\n",
    "                )\n",
    "            }, step=step)\n",
    "\n",
    "    # validate:\n",
    "    model.eval()\n",
    "    val_stats = defaultdict(list)\n",
    "    for batch_idx, batch in enumerate(val_dataloader):\n",
    "        batch = batch_transforms_val(batch)\n",
    "        with torch.no_grad():\n",
    "            logits = model(batch['audio'])\n",
    "            output_length = torch.ceil(\n",
    "                batch['input_lengths'].float() / model.stride).int()\n",
    "            loss = criterion(logits.permute(2, 0, 1).log_softmax(\n",
    "                dim=2), batch['text'], output_length, batch['target_lengths'])\n",
    "\n",
    "        target_strings = decoder.convert_to_strings(batch['text'])\n",
    "        decoded_output = decoder.decode(\n",
    "            logits.permute(0, 2, 1).softmax(dim=2))\n",
    "        wer = np.mean([decoder.wer(true, pred)\n",
    "                        for true, pred in zip(target_strings, decoded_output)])\n",
    "        cer = np.mean([decoder.cer(true, pred)\n",
    "                        for true, pred in zip(target_strings, decoded_output)])\n",
    "        val_stats['val_loss'].append(loss.item())\n",
    "        val_stats['wer'].append(wer)\n",
    "        val_stats['cer'].append(cer)\n",
    "    for k, v in val_stats.items():\n",
    "        val_stats[k] = np.mean(v)\n",
    "    val_stats['val_samples'] = wandb.Table(\n",
    "        columns=['gt_text', 'pred_text'], data=zip(target_strings, decoded_output))\n",
    "    wandb.log(val_stats, step=step)\n",
    "\n",
    "    # save model, TODO: save optimizer:\n",
    "    if val_stats['wer'] < prev_wer:\n",
    "        os.makedirs(config.train.get(\n",
    "            'checkpoint_path', 'checkpoints'), exist_ok=True)\n",
    "        prev_wer = val_stats['wer']\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            os.path.join(config.train.get(\n",
    "                'checkpoint_path', 'checkpoints'), f'model_{epoch_idx}_{prev_wer}.pth')\n",
    "        )\n",
    "        wandb.save(os.path.join(config.train.get(\n",
    "            'checkpoint_path', 'checkpoints'), f'model_{epoch_idx}_{prev_wer}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3555c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import argparse\n",
    "import importlib\n",
    "\n",
    "# torchim:\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "# from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import pytorch_warmup as warmup\n",
    "\n",
    "# data:\n",
    "import data\n",
    "from data.collate import collate_fn, gpu_collate, no_pad_collate\n",
    "from data.transforms import (\n",
    "    Compose, AddLengths, AudioSqueeze, TextPreprocess,\n",
    "    MaskSpectrogram, ToNumpy, BPEtexts, MelSpectrogram,\n",
    "    ToGpu, Pad, NormalizedMelSpectrogram\n",
    ")\n",
    "import youtokentome as yttm\n",
    "\n",
    "import torchaudio\n",
    "from audiomentations import (\n",
    "    TimeStretch, PitchShift, AddGaussianNoise\n",
    ")\n",
    "from functools import partial\n",
    "\n",
    "# model:\n",
    "from model import configs as quartznet_configs\n",
    "from model.quartznet import QuartzNet\n",
    "\n",
    "# utils:\n",
    "import yaml\n",
    "from easydict import EasyDict as edict\n",
    "from utils import fix_seeds, remove_from_dict, prepare_bpe\n",
    "import wandb\n",
    "from decoder import GreedyDecoder, BeamCTCDecoder\n",
    "\n",
    "import youtokentome as yttm\n",
    "\n",
    "\n",
    "# TODO: wrap to trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c68ad265",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='Training model.')\n",
    "parser.add_argument('--config', default='configs/train_LJSpeech.yaml',\n",
    "                    help='path to config file')\n",
    "args = parser.parse_args(\"\")\n",
    "with open(args.config, 'r') as f:\n",
    "    config = edict(yaml.safe_load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "569135f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'root': 'DB/LJspeech',\n",
      "  'train_part': 0.95,\n",
      "  'name': 'ljspeech',\n",
      "  'sample_rate': 22050},\n",
      " 'bpe': {'train': True, 'model_path': 'yttm.bpe'},\n",
      " 'train': {'seed': 42,\n",
      "  'num_workers': 1,\n",
      "  'batch_size': 32,\n",
      "  'clip_grad_norm': 15,\n",
      "  'epochs': 42,\n",
      "  'optimizer': {'lr': 0.0005, 'weight_decay': 0.0001}},\n",
      " 'wandb': {'project': 'quartznet_ljspeech', 'log_interval': 20},\n",
      " 'model': {'name': '_quartznet5x5_config', 'vocab_size': 120, 'feat_in': 64}}"
     ]
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eb0f051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]"
     ]
    }
   ],
   "source": [
    "fix_seeds(seed=config.train.get('seed', 42))\n",
    "dataset_module = importlib.import_module(\n",
    "    f'.{config.dataset.name}', data.__name__)\n",
    "bpe = prepare_bpe(config)\n",
    "\n",
    "transforms_train = Compose([\n",
    "    TextPreprocess(),\n",
    "    ToNumpy(),\n",
    "    BPEtexts(bpe=bpe, dropout_prob=config.bpe.get('dropout_prob', 0.05)),\n",
    "    AudioSqueeze(),\n",
    "    AddGaussianNoise(\n",
    "        min_amplitude=0.001,\n",
    "        max_amplitude=0.015,\n",
    "        p=0.5\n",
    "    ),\n",
    "    TimeStretch(\n",
    "        min_rate=0.8,\n",
    "        max_rate=1.25,\n",
    "        p=0.5\n",
    "    ),\n",
    "    PitchShift(\n",
    "        min_semitones=-4,\n",
    "        max_semitones=4,\n",
    "        p=0.5\n",
    "    )\n",
    "    # AddLengths()\n",
    "])\n",
    "\n",
    "batch_transforms_train = Compose([\n",
    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    NormalizedMelSpectrogram(\n",
    "        sample_rate=config.dataset.get('sample_rate', 16000),\n",
    "        n_mels=config.model.feat_in,\n",
    "        normalize=config.dataset.get('normalize', None)\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    MaskSpectrogram(\n",
    "        probability=0.5,\n",
    "        time_mask_max_percentage=0.05,\n",
    "        frequency_mask_max_percentage=0.15\n",
    "    ),\n",
    "    AddLengths(),\n",
    "    Pad()\n",
    "])\n",
    "\n",
    "transforms_val = Compose([\n",
    "    TextPreprocess(),\n",
    "    ToNumpy(),\n",
    "    BPEtexts(bpe=bpe),\n",
    "    AudioSqueeze()\n",
    "])\n",
    "\n",
    "batch_transforms_val = Compose([\n",
    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    NormalizedMelSpectrogram(\n",
    "        sample_rate=config.dataset.get(\n",
    "            'sample_rate', 16000),  # for LJspeech\n",
    "        n_mels=config.model.feat_in,\n",
    "        normalize=config.dataset.get('normalize', None)\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    AddLengths(),\n",
    "    Pad()\n",
    "])\n",
    "\n",
    "# load datasets\n",
    "train_dataset = dataset_module.get_dataset(\n",
    "    config, transforms=transforms_train, part='train')\n",
    "val_dataset = dataset_module.get_dataset(\n",
    "    config, transforms=transforms_val, part='val')\n",
    "# print(\"!!!\", config.train.get('num_workers', 4))\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                                batch_size=config.train.get('batch_size', 1), collate_fn=no_pad_collate)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=1, collate_fn=no_pad_collate)\n",
    "\n",
    "model = QuartzNet(\n",
    "    model_config=getattr(\n",
    "        quartznet_configs, config.model.name, '_quartznet5x5_config'),\n",
    "    **remove_from_dict(config.model, ['name'])\n",
    ")\n",
    "\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), **config.train.get('optimizer', {}))\n",
    "num_steps = len(train_dataloader) * config.train.get('epochs', 10)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=num_steps)\n",
    "# warmup_scheduler = warmup.UntunedLinearWarmup(optimizer)\n",
    "\n",
    "if config.train.get('from_checkpoint', None) is not None:\n",
    "    model.load_weights(config.train.from_checkpoint)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "# criterion = nn.CTCLoss(blank=config.model.vocab_size)\n",
    "decoder = GreedyDecoder(bpe=bpe)\n",
    "\n",
    "prev_wer = 1000\n",
    "wandb.init(project=config.wandb.project, config=config)\n",
    "wandb.watch(model, log=\"all\", log_freq=config.wandb.get(\n",
    "    'log_interval', 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6256cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seeds(seed=config.train.get('seed', 42))\n",
    "dataset_module = importlib.import_module(\n",
    "    f'.{config.dataset.name}', data.__name__)\n",
    "bpe = prepare_bpe(config)\n",
    "\n",
    "transforms_train = Compose([\n",
    "    TextPreprocess(),\n",
    "    ToNumpy(),\n",
    "    BPEtexts(bpe=bpe, dropout_prob=config.bpe.get('dropout_prob', 0.05)),\n",
    "    AudioSqueeze(),\n",
    "    AddGaussianNoise(\n",
    "        min_amplitude=0.001,\n",
    "        max_amplitude=0.015,\n",
    "        p=0.5\n",
    "    ),\n",
    "    TimeStretch(\n",
    "        min_rate=0.8,\n",
    "        max_rate=1.25,\n",
    "        p=0.5\n",
    "    ),\n",
    "    PitchShift(\n",
    "        min_semitones=-4,\n",
    "        max_semitones=4,\n",
    "        p=0.5\n",
    "    )\n",
    "    # AddLengths()\n",
    "])\n",
    "\n",
    "batch_transforms_train = Compose([\n",
    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    NormalizedMelSpectrogram(\n",
    "        sample_rate=config.dataset.get('sample_rate', 16000),\n",
    "        n_mels=config.model.feat_in,\n",
    "        normalize=config.dataset.get('normalize', None)\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    MaskSpectrogram(\n",
    "        probability=0.5,\n",
    "        time_mask_max_percentage=0.05,\n",
    "        frequency_mask_max_percentage=0.15\n",
    "    ),\n",
    "    AddLengths(),\n",
    "    Pad()\n",
    "])\n",
    "\n",
    "transforms_val = Compose([\n",
    "    TextPreprocess(),\n",
    "    ToNumpy(),\n",
    "    BPEtexts(bpe=bpe),\n",
    "    AudioSqueeze()\n",
    "])\n",
    "\n",
    "batch_transforms_val = Compose([\n",
    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    NormalizedMelSpectrogram(\n",
    "        sample_rate=config.dataset.get(\n",
    "            'sample_rate', 16000),  # for LJspeech\n",
    "        n_mels=config.model.feat_in,\n",
    "        normalize=config.dataset.get('normalize', None)\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    AddLengths(),\n",
    "    Pad()\n",
    "])\n",
    "\n",
    "# load datasets\n",
    "train_dataset = dataset_module.get_dataset(\n",
    "    config, transforms=transforms_train, part='train')\n",
    "val_dataset = dataset_module.get_dataset(\n",
    "    config, transforms=transforms_val, part='val')\n",
    "# print(\"!!!\", config.train.get('num_workers', 4))\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                                batch_size=config.train.get('batch_size', 1), collate_fn=no_pad_collate)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=1, collate_fn=no_pad_collate)\n",
    "\n",
    "model = QuartzNet(\n",
    "    model_config=getattr(\n",
    "        quartznet_configs, config.model.name, '_quartznet5x5_config'),\n",
    "    **remove_from_dict(config.model, ['name'])\n",
    ")\n",
    "\n",
    "print(model)\n",
    "write_to_file(model,'model_structure.txt')\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), **config.train.get('optimizer', {}))\n",
    "num_steps = len(train_dataloader) * config.train.get('epochs', 10)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=num_steps)\n",
    "# warmup_scheduler = warmup.UntunedLinearWarmup(optimizer)\n",
    "\n",
    "if config.train.get('from_checkpoint', None) is not None:\n",
    "    model.load_weights(config.train.from_checkpoint)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "# criterion = nn.CTCLoss(blank=config.model.vocab_size)\n",
    "decoder = GreedyDecoder(bpe=bpe)\n",
    "\n",
    "prev_wer = 1000\n",
    "wandb.init(project=config.wandb.project, config=config)\n",
    "wandb.watch(model, log=\"all\", log_freq=config.wandb.get(\n",
    "    'log_interval', 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01915435",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='Training model.')\n",
    "parser.add_argument('--config', default='configs/train_LJSpeech.yaml',\n",
    "                    help='path to config file')\n",
    "args = parser.parse_args(\"\")\n",
    "with open(args.config, 'r') as f:\n",
    "    config = edict(yaml.safe_load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "637e444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(str_w, file_name = 'sth.txt', mode = 'w'):\n",
    "    with open(file_name) as f:\n",
    "        f.write(str_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1df6d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'root': 'DB/LJspeech',\n",
      "  'train_part': 0.95,\n",
      "  'name': 'ljspeech',\n",
      "  'sample_rate': 22050},\n",
      " 'bpe': {'train': True, 'model_path': 'yttm.bpe'},\n",
      " 'train': {'seed': 42,\n",
      "  'num_workers': 1,\n",
      "  'batch_size': 32,\n",
      "  'clip_grad_norm': 15,\n",
      "  'epochs': 42,\n",
      "  'optimizer': {'lr': 0.0005, 'weight_decay': 0.0001}},\n",
      " 'wandb': {'project': 'quartznet_ljspeech', 'log_interval': 20},\n",
      " 'model': {'name': '_quartznet5x5_config', 'vocab_size': 120, 'feat_in': 64}}"
     ]
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d85e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seeds(seed=config.train.get('seed', 42))\n",
    "dataset_module = importlib.import_module(\n",
    "    f'.{config.dataset.name}', data.__name__)\n",
    "bpe = prepare_bpe(config)\n",
    "\n",
    "transforms_train = Compose([\n",
    "    TextPreprocess(),\n",
    "    ToNumpy(),\n",
    "    BPEtexts(bpe=bpe, dropout_prob=config.bpe.get('dropout_prob', 0.05)),\n",
    "    AudioSqueeze(),\n",
    "    AddGaussianNoise(\n",
    "        min_amplitude=0.001,\n",
    "        max_amplitude=0.015,\n",
    "        p=0.5\n",
    "    ),\n",
    "    TimeStretch(\n",
    "        min_rate=0.8,\n",
    "        max_rate=1.25,\n",
    "        p=0.5\n",
    "    ),\n",
    "    PitchShift(\n",
    "        min_semitones=-4,\n",
    "        max_semitones=4,\n",
    "        p=0.5\n",
    "    )\n",
    "    # AddLengths()\n",
    "])\n",
    "\n",
    "batch_transforms_train = Compose([\n",
    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    NormalizedMelSpectrogram(\n",
    "        sample_rate=config.dataset.get('sample_rate', 16000),\n",
    "        n_mels=config.model.feat_in,\n",
    "        normalize=config.dataset.get('normalize', None)\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    MaskSpectrogram(\n",
    "        probability=0.5,\n",
    "        time_mask_max_percentage=0.05,\n",
    "        frequency_mask_max_percentage=0.15\n",
    "    ),\n",
    "    AddLengths(),\n",
    "    Pad()\n",
    "])\n",
    "\n",
    "transforms_val = Compose([\n",
    "    TextPreprocess(),\n",
    "    ToNumpy(),\n",
    "    BPEtexts(bpe=bpe),\n",
    "    AudioSqueeze()\n",
    "])\n",
    "\n",
    "batch_transforms_val = Compose([\n",
    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    NormalizedMelSpectrogram(\n",
    "        sample_rate=config.dataset.get(\n",
    "            'sample_rate', 16000),  # for LJspeech\n",
    "        n_mels=config.model.feat_in,\n",
    "        normalize=config.dataset.get('normalize', None)\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    AddLengths(),\n",
    "    Pad()\n",
    "])\n",
    "\n",
    "# load datasets\n",
    "train_dataset = dataset_module.get_dataset(\n",
    "    config, transforms=transforms_train, part='train')\n",
    "val_dataset = dataset_module.get_dataset(\n",
    "    config, transforms=transforms_val, part='val')\n",
    "# print(\"!!!\", config.train.get('num_workers', 4))\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                                batch_size=config.train.get('batch_size', 1), collate_fn=no_pad_collate)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=1, collate_fn=no_pad_collate)\n",
    "\n",
    "model = QuartzNet(\n",
    "    model_config=getattr(\n",
    "        quartznet_configs, config.model.name, '_quartznet5x5_config'),\n",
    "    **remove_from_dict(config.model, ['name'])\n",
    ")\n",
    "\n",
    "print(model)\n",
    "write_to_file(model,'model_structure.txt')\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), **config.train.get('optimizer', {}))\n",
    "num_steps = len(train_dataloader) * config.train.get('epochs', 10)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=num_steps)\n",
    "# warmup_scheduler = warmup.UntunedLinearWarmup(optimizer)\n",
    "\n",
    "if config.train.get('from_checkpoint', None) is not None:\n",
    "    model.load_weights(config.train.from_checkpoint)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "# criterion = nn.CTCLoss(blank=config.model.vocab_size)\n",
    "decoder = GreedyDecoder(bpe=bpe)\n",
    "\n",
    "prev_wer = 1000\n",
    "wandb.init(project=config.wandb.project, config=config)\n",
    "wandb.watch(model, log=\"all\", log_freq=config.wandb.get(\n",
    "    'log_interval', 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1386cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(str_w, file_name = 'sth.txt', mode = 'w'):\n",
    "    with open(file_name,mode) as f:\n",
    "        f.write(str_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48c044a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'root': 'DB/LJspeech',\n",
      "  'train_part': 0.95,\n",
      "  'name': 'ljspeech',\n",
      "  'sample_rate': 22050},\n",
      " 'bpe': {'train': True, 'model_path': 'yttm.bpe'},\n",
      " 'train': {'seed': 42,\n",
      "  'num_workers': 1,\n",
      "  'batch_size': 32,\n",
      "  'clip_grad_norm': 15,\n",
      "  'epochs': 42,\n",
      "  'optimizer': {'lr': 0.0005, 'weight_decay': 0.0001}},\n",
      " 'wandb': {'project': 'quartznet_ljspeech', 'log_interval': 20},\n",
      " 'model': {'vocab_size': 120, 'feat_in': 64}}"
     ]
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0f30e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seeds(seed=config.train.get('seed', 42))\n",
    "dataset_module = importlib.import_module(\n",
    "    f'.{config.dataset.name}', data.__name__)\n",
    "bpe = prepare_bpe(config)\n",
    "\n",
    "transforms_train = Compose([\n",
    "    TextPreprocess(),\n",
    "    ToNumpy(),\n",
    "    BPEtexts(bpe=bpe, dropout_prob=config.bpe.get('dropout_prob', 0.05)),\n",
    "    AudioSqueeze(),\n",
    "    AddGaussianNoise(\n",
    "        min_amplitude=0.001,\n",
    "        max_amplitude=0.015,\n",
    "        p=0.5\n",
    "    ),\n",
    "    TimeStretch(\n",
    "        min_rate=0.8,\n",
    "        max_rate=1.25,\n",
    "        p=0.5\n",
    "    ),\n",
    "    PitchShift(\n",
    "        min_semitones=-4,\n",
    "        max_semitones=4,\n",
    "        p=0.5\n",
    "    )\n",
    "    # AddLengths()\n",
    "])\n",
    "\n",
    "batch_transforms_train = Compose([\n",
    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    NormalizedMelSpectrogram(\n",
    "        sample_rate=config.dataset.get('sample_rate', 16000),\n",
    "        n_mels=config.model.feat_in,\n",
    "        normalize=config.dataset.get('normalize', None)\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    MaskSpectrogram(\n",
    "        probability=0.5,\n",
    "        time_mask_max_percentage=0.05,\n",
    "        frequency_mask_max_percentage=0.15\n",
    "    ),\n",
    "    AddLengths(),\n",
    "    Pad()\n",
    "])\n",
    "\n",
    "transforms_val = Compose([\n",
    "    TextPreprocess(),\n",
    "    ToNumpy(),\n",
    "    BPEtexts(bpe=bpe),\n",
    "    AudioSqueeze()\n",
    "])\n",
    "\n",
    "batch_transforms_val = Compose([\n",
    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    NormalizedMelSpectrogram(\n",
    "        sample_rate=config.dataset.get(\n",
    "            'sample_rate', 16000),  # for LJspeech\n",
    "        n_mels=config.model.feat_in,\n",
    "        normalize=config.dataset.get('normalize', None)\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    AddLengths(),\n",
    "    Pad()\n",
    "])\n",
    "\n",
    "# load datasets\n",
    "train_dataset = dataset_module.get_dataset(\n",
    "    config, transforms=transforms_train, part='train')\n",
    "val_dataset = dataset_module.get_dataset(\n",
    "    config, transforms=transforms_val, part='val')\n",
    "# print(\"!!!\", config.train.get('num_workers', 4))\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                                batch_size=config.train.get('batch_size', 1), collate_fn=no_pad_collate)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=1, collate_fn=no_pad_collate)\n",
    "\n",
    "model = QuartzNet(\n",
    "    model_config=getattr(\n",
    "        quartznet_configs, config.model.name, '_quartznet5x5_config'),\n",
    "    **remove_from_dict(config.model, ['name'])\n",
    ")\n",
    "\n",
    "print(model)\n",
    "write_to_file(model,'model_structure.txt')\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), **config.train.get('optimizer', {}))\n",
    "num_steps = len(train_dataloader) * config.train.get('epochs', 10)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=num_steps)\n",
    "# warmup_scheduler = warmup.UntunedLinearWarmup(optimizer)\n",
    "\n",
    "if config.train.get('from_checkpoint', None) is not None:\n",
    "    model.load_weights(config.train.from_checkpoint)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "# criterion = nn.CTCLoss(blank=config.model.vocab_size)\n",
    "decoder = GreedyDecoder(bpe=bpe)\n",
    "\n",
    "prev_wer = 1000\n",
    "wandb.init(project=config.wandb.project, config=config)\n",
    "wandb.watch(model, log=\"all\", log_freq=config.wandb.get(\n",
    "    'log_interval', 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "516cff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import argparse\n",
    "import importlib\n",
    "\n",
    "# torchim:\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "# from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import pytorch_warmup as warmup\n",
    "\n",
    "# data:\n",
    "import data\n",
    "from data.collate import collate_fn, gpu_collate, no_pad_collate\n",
    "from data.transforms import (\n",
    "    Compose, AddLengths, AudioSqueeze, TextPreprocess,\n",
    "    MaskSpectrogram, ToNumpy, BPEtexts, MelSpectrogram,\n",
    "    ToGpu, Pad, NormalizedMelSpectrogram\n",
    ")\n",
    "import youtokentome as yttm\n",
    "\n",
    "import torchaudio\n",
    "from audiomentations import (\n",
    "    TimeStretch, PitchShift, AddGaussianNoise\n",
    ")\n",
    "from functools import partial\n",
    "\n",
    "# model:\n",
    "from model import configs as quartznet_configs\n",
    "from model.quartznet import QuartzNet\n",
    "\n",
    "# utils:\n",
    "import yaml\n",
    "from easydict import EasyDict as edict\n",
    "from utils import fix_seeds, remove_from_dict, prepare_bpe\n",
    "import wandb\n",
    "from decoder import GreedyDecoder, BeamCTCDecoder\n",
    "\n",
    "import youtokentome as yttm\n",
    "\n",
    "\n",
    "# TODO: wrap to trainer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a54cb61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser(description='Training model.')\n",
    "parser.add_argument('--config', default='configs/train_LJSpeech.yaml',\n",
    "                    help='path to config file')\n",
    "args = parser.parse_args(\"\")\n",
    "with open(args.config, 'r') as f:\n",
    "    config = edict(yaml.safe_load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8984394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(str_w, file_name = 'sth.txt', mode = 'w'):\n",
    "    with open(file_name,mode) as f:\n",
    "        f.write(str_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ce741bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'root': 'DB/LJspeech',\n",
      "  'train_part': 0.95,\n",
      "  'name': 'ljspeech',\n",
      "  'sample_rate': 22050},\n",
      " 'bpe': {'train': True, 'model_path': 'yttm.bpe'},\n",
      " 'train': {'seed': 42,\n",
      "  'num_workers': 1,\n",
      "  'batch_size': 32,\n",
      "  'clip_grad_norm': 15,\n",
      "  'epochs': 42,\n",
      "  'optimizer': {'lr': 0.0005, 'weight_decay': 0.0001}},\n",
      " 'wandb': {'project': 'quartznet_ljspeech', 'log_interval': 20},\n",
      " 'model': {'name': '_quartznet5x5_config', 'vocab_size': 120, 'feat_in': 64}}"
     ]
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e0c75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seeds(seed=config.train.get('seed', 42))\n",
    "dataset_module = importlib.import_module(\n",
    "    f'.{config.dataset.name}', data.__name__)\n",
    "bpe = prepare_bpe(config)\n",
    "\n",
    "transforms_train = Compose([\n",
    "    TextPreprocess(),\n",
    "    ToNumpy(),\n",
    "    BPEtexts(bpe=bpe, dropout_prob=config.bpe.get('dropout_prob', 0.05)),\n",
    "    AudioSqueeze(),\n",
    "    AddGaussianNoise(\n",
    "        min_amplitude=0.001,\n",
    "        max_amplitude=0.015,\n",
    "        p=0.5\n",
    "    ),\n",
    "    TimeStretch(\n",
    "        min_rate=0.8,\n",
    "        max_rate=1.25,\n",
    "        p=0.5\n",
    "    ),\n",
    "    PitchShift(\n",
    "        min_semitones=-4,\n",
    "        max_semitones=4,\n",
    "        p=0.5\n",
    "    )\n",
    "    # AddLengths()\n",
    "])\n",
    "\n",
    "batch_transforms_train = Compose([\n",
    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    NormalizedMelSpectrogram(\n",
    "        sample_rate=config.dataset.get('sample_rate', 16000),\n",
    "        n_mels=config.model.feat_in,\n",
    "        normalize=config.dataset.get('normalize', None)\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    MaskSpectrogram(\n",
    "        probability=0.5,\n",
    "        time_mask_max_percentage=0.05,\n",
    "        frequency_mask_max_percentage=0.15\n",
    "    ),\n",
    "    AddLengths(),\n",
    "    Pad()\n",
    "])\n",
    "\n",
    "transforms_val = Compose([\n",
    "    TextPreprocess(),\n",
    "    ToNumpy(),\n",
    "    BPEtexts(bpe=bpe),\n",
    "    AudioSqueeze()\n",
    "])\n",
    "\n",
    "batch_transforms_val = Compose([\n",
    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    NormalizedMelSpectrogram(\n",
    "        sample_rate=config.dataset.get(\n",
    "            'sample_rate', 16000),  # for LJspeech\n",
    "        n_mels=config.model.feat_in,\n",
    "        normalize=config.dataset.get('normalize', None)\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    AddLengths(),\n",
    "    Pad()\n",
    "])\n",
    "\n",
    "# load datasets\n",
    "train_dataset = dataset_module.get_dataset(\n",
    "    config, transforms=transforms_train, part='train')\n",
    "val_dataset = dataset_module.get_dataset(\n",
    "    config, transforms=transforms_val, part='val')\n",
    "# print(\"!!!\", config.train.get('num_workers', 4))\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                                batch_size=config.train.get('batch_size', 1), collate_fn=no_pad_collate)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=1, collate_fn=no_pad_collate)\n",
    "\n",
    "model = QuartzNet(\n",
    "    model_config=getattr(\n",
    "        quartznet_configs, config.model.name, '_quartznet5x5_config'),\n",
    "    **remove_from_dict(config.model, ['name'])\n",
    ")\n",
    "\n",
    "print(model)\n",
    "write_to_file(model,'model_structure.txt')\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), **config.train.get('optimizer', {}))\n",
    "num_steps = len(train_dataloader) * config.train.get('epochs', 10)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=num_steps)\n",
    "# warmup_scheduler = warmup.UntunedLinearWarmup(optimizer)\n",
    "\n",
    "if config.train.get('from_checkpoint', None) is not None:\n",
    "    model.load_weights(config.train.from_checkpoint)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "# criterion = nn.CTCLoss(blank=config.model.vocab_size)\n",
    "decoder = GreedyDecoder(bpe=bpe)\n",
    "\n",
    "prev_wer = 1000\n",
    "wandb.init(project=config.wandb.project, config=config)\n",
    "wandb.watch(model, log=\"all\", log_freq=config.wandb.get(\n",
    "    'log_interval', 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91369cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(str_w, file_name = 'sth.txt', mode = 'w'):\n",
    "    with open(file_name,mode) as f:\n",
    "        f.write(str(str_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b53af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seeds(seed=config.train.get('seed', 42))\n",
    "dataset_module = importlib.import_module(\n",
    "    f'.{config.dataset.name}', data.__name__)\n",
    "bpe = prepare_bpe(config)\n",
    "\n",
    "transforms_train = Compose([\n",
    "    TextPreprocess(),\n",
    "    ToNumpy(),\n",
    "    BPEtexts(bpe=bpe, dropout_prob=config.bpe.get('dropout_prob', 0.05)),\n",
    "    AudioSqueeze(),\n",
    "    AddGaussianNoise(\n",
    "        min_amplitude=0.001,\n",
    "        max_amplitude=0.015,\n",
    "        p=0.5\n",
    "    ),\n",
    "    TimeStretch(\n",
    "        min_rate=0.8,\n",
    "        max_rate=1.25,\n",
    "        p=0.5\n",
    "    ),\n",
    "    PitchShift(\n",
    "        min_semitones=-4,\n",
    "        max_semitones=4,\n",
    "        p=0.5\n",
    "    )\n",
    "    # AddLengths()\n",
    "])\n",
    "\n",
    "batch_transforms_train = Compose([\n",
    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    NormalizedMelSpectrogram(\n",
    "        sample_rate=config.dataset.get('sample_rate', 16000),\n",
    "        n_mels=config.model.feat_in,\n",
    "        normalize=config.dataset.get('normalize', None)\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    MaskSpectrogram(\n",
    "        probability=0.5,\n",
    "        time_mask_max_percentage=0.05,\n",
    "        frequency_mask_max_percentage=0.15\n",
    "    ),\n",
    "    AddLengths(),\n",
    "    Pad()\n",
    "])\n",
    "\n",
    "transforms_val = Compose([\n",
    "    TextPreprocess(),\n",
    "    ToNumpy(),\n",
    "    BPEtexts(bpe=bpe),\n",
    "    AudioSqueeze()\n",
    "])\n",
    "\n",
    "batch_transforms_val = Compose([\n",
    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    NormalizedMelSpectrogram(\n",
    "        sample_rate=config.dataset.get(\n",
    "            'sample_rate', 16000),  # for LJspeech\n",
    "        n_mels=config.model.feat_in,\n",
    "        normalize=config.dataset.get('normalize', None)\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    AddLengths(),\n",
    "    Pad()\n",
    "])\n",
    "\n",
    "# load datasets\n",
    "train_dataset = dataset_module.get_dataset(\n",
    "    config, transforms=transforms_train, part='train')\n",
    "val_dataset = dataset_module.get_dataset(\n",
    "    config, transforms=transforms_val, part='val')\n",
    "# print(\"!!!\", config.train.get('num_workers', 4))\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                                batch_size=config.train.get('batch_size', 1), collate_fn=no_pad_collate)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=1, collate_fn=no_pad_collate)\n",
    "\n",
    "model = QuartzNet(\n",
    "    model_config=getattr(\n",
    "        quartznet_configs, config.model.name, '_quartznet5x5_config'),\n",
    "    **remove_from_dict(config.model, ['name'])\n",
    ")\n",
    "\n",
    "print(model)\n",
    "write_to_file(model,'model_structure.txt')\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), **config.train.get('optimizer', {}))\n",
    "num_steps = len(train_dataloader) * config.train.get('epochs', 10)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=num_steps)\n",
    "# warmup_scheduler = warmup.UntunedLinearWarmup(optimizer)\n",
    "\n",
    "if config.train.get('from_checkpoint', None) is not None:\n",
    "    model.load_weights(config.train.from_checkpoint)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "# criterion = nn.CTCLoss(blank=config.model.vocab_size)\n",
    "decoder = GreedyDecoder(bpe=bpe)\n",
    "\n",
    "prev_wer = 1000\n",
    "wandb.init(project=config.wandb.project, config=config)\n",
    "wandb.watch(model, log=\"all\", log_freq=config.wandb.get(\n",
    "    'log_interval', 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "427a4ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import argparse\n",
    "import importlib\n",
    "\n",
    "# torchim:\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "# from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import pytorch_warmup as warmup\n",
    "\n",
    "# data:\n",
    "import data\n",
    "from data.collate import collate_fn, gpu_collate, no_pad_collate\n",
    "from data.transforms import (\n",
    "    Compose, AddLengths, AudioSqueeze, TextPreprocess,\n",
    "    MaskSpectrogram, ToNumpy, BPEtexts, MelSpectrogram,\n",
    "    ToGpu, Pad, NormalizedMelSpectrogram\n",
    ")\n",
    "import youtokentome as yttm\n",
    "\n",
    "import torchaudio\n",
    "from audiomentations import (\n",
    "    TimeStretch, PitchShift, AddGaussianNoise\n",
    ")\n",
    "from functools import partial\n",
    "\n",
    "# model:\n",
    "from model import configs as quartznet_configs\n",
    "from model.quartznet import QuartzNet\n",
    "\n",
    "# utils:\n",
    "import yaml\n",
    "from easydict import EasyDict as edict\n",
    "from utils import fix_seeds, remove_from_dict, prepare_bpe\n",
    "import wandb\n",
    "from decoder import GreedyDecoder, BeamCTCDecoder\n",
    "\n",
    "import youtokentome as yttm\n",
    "\n",
    "\n",
    "# TODO: wrap to trainer class\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Training model.')\n",
    "parser.add_argument('--config', default='configs/train_LJSpeech.yaml',\n",
    "                    help='path to config file')\n",
    "args = parser.parse_args(\"\")\n",
    "with open(args.config, 'r') as f:\n",
    "    config = edict(yaml.safe_load(f))\n",
    "\n",
    "def write_to_file(str_w, file_name = 'sth.txt', mode = 'w'):\n",
    "    with open(file_name,mode) as f:\n",
    "        f.write(str(str_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d5ca063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'root': 'DB/LJspeech',\n",
      "  'train_part': 0.95,\n",
      "  'name': 'ljspeech',\n",
      "  'sample_rate': 22050},\n",
      " 'bpe': {'train': True, 'model_path': 'yttm.bpe'},\n",
      " 'train': {'seed': 42,\n",
      "  'num_workers': 1,\n",
      "  'batch_size': 32,\n",
      "  'clip_grad_norm': 15,\n",
      "  'epochs': 42,\n",
      "  'optimizer': {'lr': 0.0005, 'weight_decay': 0.0001}},\n",
      " 'wandb': {'project': 'quartznet_ljspeech', 'log_interval': 20},\n",
      " 'model': {'name': '_quartznet5x5_config', 'vocab_size': 120, 'feat_in': 64}}"
     ]
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ee07919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]"
     ]
    }
   ],
   "source": [
    "fix_seeds(seed=config.train.get('seed', 42))\n",
    "dataset_module = importlib.import_module(\n",
    "    f'.{config.dataset.name}', data.__name__)\n",
    "bpe = prepare_bpe(config)\n",
    "\n",
    "transforms_train = Compose([\n",
    "    TextPreprocess(),\n",
    "    ToNumpy(),\n",
    "    BPEtexts(bpe=bpe, dropout_prob=config.bpe.get('dropout_prob', 0.05)),\n",
    "    AudioSqueeze(),\n",
    "    AddGaussianNoise(\n",
    "        min_amplitude=0.001,\n",
    "        max_amplitude=0.015,\n",
    "        p=0.5\n",
    "    ),\n",
    "    TimeStretch(\n",
    "        min_rate=0.8,\n",
    "        max_rate=1.25,\n",
    "        p=0.5\n",
    "    ),\n",
    "    PitchShift(\n",
    "        min_semitones=-4,\n",
    "        max_semitones=4,\n",
    "        p=0.5\n",
    "    )\n",
    "    # AddLengths()\n",
    "])\n",
    "\n",
    "batch_transforms_train = Compose([\n",
    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    NormalizedMelSpectrogram(\n",
    "        sample_rate=config.dataset.get('sample_rate', 16000),\n",
    "        n_mels=config.model.feat_in,\n",
    "        normalize=config.dataset.get('normalize', None)\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    MaskSpectrogram(\n",
    "        probability=0.5,\n",
    "        time_mask_max_percentage=0.05,\n",
    "        frequency_mask_max_percentage=0.15\n",
    "    ),\n",
    "    AddLengths(),\n",
    "    Pad()\n",
    "])\n",
    "\n",
    "transforms_val = Compose([\n",
    "    TextPreprocess(),\n",
    "    ToNumpy(),\n",
    "    BPEtexts(bpe=bpe),\n",
    "    AudioSqueeze()\n",
    "])\n",
    "\n",
    "batch_transforms_val = Compose([\n",
    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    NormalizedMelSpectrogram(\n",
    "        sample_rate=config.dataset.get(\n",
    "            'sample_rate', 16000),  # for LJspeech\n",
    "        n_mels=config.model.feat_in,\n",
    "        normalize=config.dataset.get('normalize', None)\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    AddLengths(),\n",
    "    Pad()\n",
    "])\n",
    "\n",
    "# load datasets\n",
    "train_dataset = dataset_module.get_dataset(\n",
    "    config, transforms=transforms_train, part='train')\n",
    "val_dataset = dataset_module.get_dataset(\n",
    "    config, transforms=transforms_val, part='val')\n",
    "# print(\"!!!\", config.train.get('num_workers', 4))\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                                batch_size=config.train.get('batch_size', 1), collate_fn=no_pad_collate)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=1, collate_fn=no_pad_collate)\n",
    "\n",
    "model = QuartzNet(\n",
    "    model_config=getattr(\n",
    "        quartznet_configs, config.model.name, '_quartznet5x5_config'),\n",
    "    **remove_from_dict(config.model, ['name'])\n",
    ")\n",
    "\n",
    "print(model)\n",
    "write_to_file(model,'model_structure.txt')\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), **config.train.get('optimizer', {}))\n",
    "num_steps = len(train_dataloader) * config.train.get('epochs', 10)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=num_steps)\n",
    "# warmup_scheduler = warmup.UntunedLinearWarmup(optimizer)\n",
    "\n",
    "if config.train.get('from_checkpoint', None) is not None:\n",
    "    model.load_weights(config.train.from_checkpoint)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "# criterion = nn.CTCLoss(blank=config.model.vocab_size)\n",
    "decoder = GreedyDecoder(bpe=bpe)\n",
    "\n",
    "prev_wer = 1000\n",
    "wandb.init(project=config.wandb.project, config=config)\n",
    "wandb.watch(model, log=\"all\", log_freq=config.wandb.get(\n",
    "    'log_interval', 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a86a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import argparse\n",
    "import importlib\n",
    "\n",
    "# torchim:\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "# from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import pytorch_warmup as warmup\n",
    "\n",
    "# data:\n",
    "import data\n",
    "from data.collate import collate_fn, gpu_collate, no_pad_collate\n",
    "from data.transforms import (\n",
    "    Compose, AddLengths, AudioSqueeze, TextPreprocess,\n",
    "    MaskSpectrogram, ToNumpy, BPEtexts, MelSpectrogram,\n",
    "    ToGpu, Pad, NormalizedMelSpectrogram\n",
    ")\n",
    "import youtokentome as yttm\n",
    "\n",
    "import torchaudio\n",
    "from audiomentations import (\n",
    "    TimeStretch, PitchShift, AddGaussianNoise\n",
    ")\n",
    "from functools import partial\n",
    "\n",
    "# model:\n",
    "from model import configs as quartznet_configs\n",
    "from model.quartznet import QuartzNet\n",
    "\n",
    "# utils:\n",
    "import yaml\n",
    "from easydict import EasyDict as edict\n",
    "from utils import fix_seeds, remove_from_dict, prepare_bpe\n",
    "import wandb\n",
    "from decoder import GreedyDecoder, BeamCTCDecoder\n",
    "\n",
    "import youtokentome as yttm\n",
    "\n",
    "\n",
    "# TODO: wrap to trainer class\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Training model.')\n",
    "parser.add_argument('--config', default='configs/train_LJSpeech.yaml',\n",
    "                    help='path to config file')\n",
    "args = parser.parse_args(\"\")\n",
    "with open(args.config, 'r') as f:\n",
    "    config = edict(yaml.safe_load(f))\n",
    "\n",
    "def write_to_file(str_w, file_name = 'sth.txt', mode = 'w'):\n",
    "    with open(file_name,mode) as f:\n",
    "        f.write(str(str_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7606636f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'root': 'DB/LJspeech',\n",
      "  'train_part': 0.95,\n",
      "  'name': 'ljspeech',\n",
      "  'sample_rate': 22050},\n",
      " 'bpe': {'train': True, 'model_path': 'yttm.bpe'},\n",
      " 'train': {'seed': 42,\n",
      "  'num_workers': 1,\n",
      "  'batch_size': 32,\n",
      "  'clip_grad_norm': 15,\n",
      "  'epochs': 42,\n",
      "  'optimizer': {'lr': 0.0005, 'weight_decay': 0.0001}},\n",
      " 'wandb': {'project': 'quartznet_ljspeech', 'log_interval': 20},\n",
      " 'model': {'name': '_quartznet5x5_config', 'vocab_size': 120, 'feat_in': 64}}"
     ]
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "884da278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]"
     ]
    }
   ],
   "source": [
    "fix_seeds(seed=config.train.get('seed', 42))\n",
    "dataset_module = importlib.import_module(\n",
    "    f'.{config.dataset.name}', data.__name__)\n",
    "bpe = prepare_bpe(config)\n",
    "\n",
    "transforms_train = Compose([\n",
    "    TextPreprocess(),\n",
    "    ToNumpy(),\n",
    "    BPEtexts(bpe=bpe, dropout_prob=config.bpe.get('dropout_prob', 0.05)),\n",
    "    AudioSqueeze(),\n",
    "    AddGaussianNoise(\n",
    "        min_amplitude=0.001,\n",
    "        max_amplitude=0.015,\n",
    "        p=0.5\n",
    "    ),\n",
    "    TimeStretch(\n",
    "        min_rate=0.8,\n",
    "        max_rate=1.25,\n",
    "        p=0.5\n",
    "    ),\n",
    "    PitchShift(\n",
    "        min_semitones=-4,\n",
    "        max_semitones=4,\n",
    "        p=0.5\n",
    "    )\n",
    "    # AddLengths()\n",
    "])\n",
    "\n",
    "batch_transforms_train = Compose([\n",
    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    NormalizedMelSpectrogram(\n",
    "        sample_rate=config.dataset.get('sample_rate', 16000),\n",
    "        n_mels=config.model.feat_in,\n",
    "        normalize=config.dataset.get('normalize', None)\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    MaskSpectrogram(\n",
    "        probability=0.5,\n",
    "        time_mask_max_percentage=0.05,\n",
    "        frequency_mask_max_percentage=0.15\n",
    "    ),\n",
    "    AddLengths(),\n",
    "    Pad()\n",
    "])\n",
    "\n",
    "transforms_val = Compose([\n",
    "    TextPreprocess(),\n",
    "    ToNumpy(),\n",
    "    BPEtexts(bpe=bpe),\n",
    "    AudioSqueeze()\n",
    "])\n",
    "\n",
    "batch_transforms_val = Compose([\n",
    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    NormalizedMelSpectrogram(\n",
    "        sample_rate=config.dataset.get(\n",
    "            'sample_rate', 16000),  # for LJspeech\n",
    "        n_mels=config.model.feat_in,\n",
    "        normalize=config.dataset.get('normalize', None)\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    AddLengths(),\n",
    "    Pad()\n",
    "])\n",
    "\n",
    "# load datasets\n",
    "train_dataset = dataset_module.get_dataset(\n",
    "    config, transforms=transforms_train, part='train')\n",
    "val_dataset = dataset_module.get_dataset(\n",
    "    config, transforms=transforms_val, part='val')\n",
    "# print(\"!!!\", config.train.get('num_workers', 4))\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                                batch_size=config.train.get('batch_size', 1), collate_fn=no_pad_collate)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=1, collate_fn=no_pad_collate)\n",
    "\n",
    "model = QuartzNet(\n",
    "    model_config=getattr(\n",
    "        quartznet_configs, config.model.name, '_quartznet5x5_config'),\n",
    "    **remove_from_dict(config.model, ['name'])\n",
    ")\n",
    "\n",
    "print(model)\n",
    "write_to_file(model,'model_structure.txt')\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), **config.train.get('optimizer', {}))\n",
    "num_steps = len(train_dataloader) * config.train.get('epochs', 10)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=num_steps)\n",
    "# warmup_scheduler = warmup.UntunedLinearWarmup(optimizer)\n",
    "\n",
    "if config.train.get('from_checkpoint', None) is not None:\n",
    "    model.load_weights(config.train.from_checkpoint)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "# criterion = nn.CTCLoss(blank=config.model.vocab_size)\n",
    "decoder = GreedyDecoder(bpe=bpe)\n",
    "\n",
    "prev_wer = 1000\n",
    "wandb.init(project=config.wandb.project, config=config)\n",
    "wandb.watch(model, log=\"all\", log_freq=config.wandb.get(\n",
    "    'log_interval', 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a692be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "getattr(quartznet_configs, config.model.name, '_quartznet5x5_config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20c92c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import argparse\n",
    "import importlib\n",
    "\n",
    "# torchim:\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "# from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import pytorch_warmup as warmup\n",
    "\n",
    "# data:\n",
    "import data\n",
    "from data.collate import collate_fn, gpu_collate, no_pad_collate\n",
    "from data.transforms import (\n",
    "    Compose, AddLengths, AudioSqueeze, TextPreprocess,\n",
    "    MaskSpectrogram, ToNumpy, BPEtexts, MelSpectrogram,\n",
    "    ToGpu, Pad, NormalizedMelSpectrogram\n",
    ")\n",
    "import youtokentome as yttm\n",
    "\n",
    "import torchaudio\n",
    "from audiomentations import (\n",
    "    TimeStretch, PitchShift, AddGaussianNoise\n",
    ")\n",
    "from functools import partial\n",
    "\n",
    "# model:\n",
    "from model import configs as quartznet_configs\n",
    "from model.quartznet import QuartzNet\n",
    "\n",
    "# utils:\n",
    "import yaml\n",
    "from easydict import EasyDict as edict\n",
    "from utils import fix_seeds, remove_from_dict, prepare_bpe\n",
    "import wandb\n",
    "from decoder import GreedyDecoder, BeamCTCDecoder\n",
    "\n",
    "import youtokentome as yttm\n",
    "\n",
    "\n",
    "# TODO: wrap to trainer class\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Training model.')\n",
    "parser.add_argument('--config', default='configs/train_LJSpeech.yaml',\n",
    "                    help='path to config file')\n",
    "args = parser.parse_args(\"\")\n",
    "with open(args.config, 'r') as f:\n",
    "    config = edict(yaml.safe_load(f))\n",
    "\n",
    "def write_to_file(str_w, file_name = 'sth.txt', mode = 'w'):\n",
    "    with open(file_name,mode) as f:\n",
    "        f.write(str(str_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aff8048e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'root': 'DB/LJspeech',\n",
      "  'train_part': 0.95,\n",
      "  'name': 'ljspeech',\n",
      "  'sample_rate': 22050},\n",
      " 'bpe': {'train': True, 'model_path': 'yttm.bpe'},\n",
      " 'train': {'seed': 42,\n",
      "  'num_workers': 1,\n",
      "  'batch_size': 32,\n",
      "  'clip_grad_norm': 15,\n",
      "  'epochs': 42,\n",
      "  'optimizer': {'lr': 0.0005, 'weight_decay': 0.0001}},\n",
      " 'wandb': {'project': 'quartznet_ljspeech', 'log_interval': 20},\n",
      " 'model': {'name': '_quartznet5x5_config', 'vocab_size': 120, 'feat_in': 64}}"
     ]
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98c7a14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:pqzr4zzb) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13cc2d4fd010482a8687ffd4606f97e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.042 MB of 0.042 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">worldly-butterfly-24</strong> at: <a href='https://wandb.ai/monash-deep-neuron/quartznet_ljspeech/runs/pqzr4zzb' target=\"_blank\">https://wandb.ai/monash-deep-neuron/quartznet_ljspeech/runs/pqzr4zzb</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230322_124221-pqzr4zzb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:pqzr4zzb). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324eb06540294ccdaaa2216854070864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\nntin\\uni\\fyp\\quartznet-pytorch\\wandb\\run-20230322_124344-o15p5dpm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/monash-deep-neuron/quartznet_ljspeech/runs/o15p5dpm' target=\"_blank\">treasured-star-25</a></strong> to <a href='https://wandb.ai/monash-deep-neuron/quartznet_ljspeech' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/monash-deep-neuron/quartznet_ljspeech' target=\"_blank\">https://wandb.ai/monash-deep-neuron/quartznet_ljspeech</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/monash-deep-neuron/quartznet_ljspeech/runs/o15p5dpm' target=\"_blank\">https://wandb.ai/monash-deep-neuron/quartznet_ljspeech/runs/o15p5dpm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fix_seeds(seed=config.train.get('seed', 42))\n",
    "dataset_module = importlib.import_module(\n",
    "    f'.{config.dataset.name}', data.__name__)\n",
    "bpe = prepare_bpe(config)\n",
    "\n",
    "transforms_train = Compose([\n",
    "    TextPreprocess(),\n",
    "    ToNumpy(),\n",
    "    BPEtexts(bpe=bpe, dropout_prob=config.bpe.get('dropout_prob', 0.05)),\n",
    "    AudioSqueeze(),\n",
    "    AddGaussianNoise(\n",
    "        min_amplitude=0.001,\n",
    "        max_amplitude=0.015,\n",
    "        p=0.5\n",
    "    ),\n",
    "    TimeStretch(\n",
    "        min_rate=0.8,\n",
    "        max_rate=1.25,\n",
    "        p=0.5\n",
    "    ),\n",
    "    PitchShift(\n",
    "        min_semitones=-4,\n",
    "        max_semitones=4,\n",
    "        p=0.5\n",
    "    )\n",
    "    # AddLengths()\n",
    "])\n",
    "\n",
    "batch_transforms_train = Compose([\n",
    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    NormalizedMelSpectrogram(\n",
    "        sample_rate=config.dataset.get('sample_rate', 16000),\n",
    "        n_mels=config.model.feat_in,\n",
    "        normalize=config.dataset.get('normalize', None)\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    MaskSpectrogram(\n",
    "        probability=0.5,\n",
    "        time_mask_max_percentage=0.05,\n",
    "        frequency_mask_max_percentage=0.15\n",
    "    ),\n",
    "    AddLengths(),\n",
    "    Pad()\n",
    "])\n",
    "\n",
    "transforms_val = Compose([\n",
    "    TextPreprocess(),\n",
    "    ToNumpy(),\n",
    "    BPEtexts(bpe=bpe),\n",
    "    AudioSqueeze()\n",
    "])\n",
    "\n",
    "batch_transforms_val = Compose([\n",
    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    NormalizedMelSpectrogram(\n",
    "        sample_rate=config.dataset.get(\n",
    "            'sample_rate', 16000),  # for LJspeech\n",
    "        n_mels=config.model.feat_in,\n",
    "        normalize=config.dataset.get('normalize', None)\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    AddLengths(),\n",
    "    Pad()\n",
    "])\n",
    "\n",
    "# load datasets\n",
    "train_dataset = dataset_module.get_dataset(\n",
    "    config, transforms=transforms_train, part='train')\n",
    "val_dataset = dataset_module.get_dataset(\n",
    "    config, transforms=transforms_val, part='val')\n",
    "# print(\"!!!\", config.train.get('num_workers', 4))\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                                batch_size=config.train.get('batch_size', 1), collate_fn=no_pad_collate)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=1, collate_fn=no_pad_collate)\n",
    "\n",
    "print(getattr(quartznet_configs, config.model.name, '_quartznet5x5_config'))\n",
    "\n",
    "model = QuartzNet(\n",
    "    model_config=getattr(\n",
    "        quartznet_configs, config.model.name, '_quartznet5x5_config'),\n",
    "    **remove_from_dict(config.model, ['name'])\n",
    ")\n",
    "\n",
    "# print(model)\n",
    "write_to_file(model,'model_structure.txt')\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), **config.train.get('optimizer', {}))\n",
    "num_steps = len(train_dataloader) * config.train.get('epochs', 10)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=num_steps)\n",
    "# warmup_scheduler = warmup.UntunedLinearWarmup(optimizer)\n",
    "\n",
    "if config.train.get('from_checkpoint', None) is not None:\n",
    "    model.load_weights(config.train.from_checkpoint)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "# criterion = nn.CTCLoss(blank=config.model.vocab_size)\n",
    "decoder = GreedyDecoder(bpe=bpe)\n",
    "\n",
    "prev_wer = 1000\n",
    "wandb.init(project=config.wandb.project, config=config)\n",
    "wandb.watch(model, log=\"all\", log_freq=config.wandb.get(\n",
    "    'log_interval', 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9ce3738",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_idx in tqdm(range(config.train.get('epochs', 10))):\n",
    "    # train:\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        batch1=batch\n",
    "        batch = batch_transforms_train(batch)\n",
    "        batch2=batch\n",
    "        break\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch['audio'])\n",
    "        output_length = torch.ceil(\n",
    "            batch['input_lengths'].float() / model.stride).int()\n",
    "        loss = criterion(logits.permute(2, 0, 1).log_softmax(\n",
    "            dim=2), batch['text'], output_length, batch['target_lengths'])\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), config.train.get('clip_grad_norm', 15))\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        # warmup_scheduler.dampen()\n",
    "\n",
    "        if batch_idx % config.wandb.get('log_interval', 5000) == 0:\n",
    "            target_strings = decoder.convert_to_strings(batch['text'])\n",
    "            decoded_output = decoder.decode(\n",
    "                logits.permute(0, 2, 1).softmax(dim=2))\n",
    "            wer = np.mean([decoder.wer(true, pred)\n",
    "                            for true, pred in zip(target_strings, decoded_output)])\n",
    "            cer = np.mean([decoder.cer(true, pred)\n",
    "                            for true, pred in zip(target_strings, decoded_output)])\n",
    "            step = epoch_idx * \\\n",
    "                len(train_dataloader) * train_dataloader.batch_size + \\\n",
    "                batch_idx * train_dataloader.batch_size\n",
    "            wandb.log({\n",
    "                \"train_loss\": loss.item(),\n",
    "                \"train_wer\": wer,\n",
    "                \"train_cer\": cer,\n",
    "                \"train_samples\": wandb.Table(\n",
    "                    columns=['gt_text', 'pred_text'],\n",
    "                    data=zip(target_strings, decoded_output)\n",
    "                )\n",
    "            }, step=step)\n",
    "    # #!\n",
    "    # # validate:\n",
    "    # model.eval()\n",
    "    # val_stats = defaultdict(list)\n",
    "    # for batch_idx, batch in enumerate(val_dataloader):\n",
    "    #     batch = batch_transforms_val(batch)\n",
    "    #     with torch.no_grad():\n",
    "    #         logits = model(batch['audio'])\n",
    "    #         output_length = torch.ceil(\n",
    "    #             batch['input_lengths'].float() / model.stride).int()\n",
    "    #         loss = criterion(logits.permute(2, 0, 1).log_softmax(\n",
    "    #             dim=2), batch['text'], output_length, batch['target_lengths'])\n",
    "\n",
    "    #     target_strings = decoder.convert_to_strings(batch['text'])\n",
    "    #     decoded_output = decoder.decode(\n",
    "    #         logits.permute(0, 2, 1).softmax(dim=2))\n",
    "    #     wer = np.mean([decoder.wer(true, pred)\n",
    "    #                     for true, pred in zip(target_strings, decoded_output)])\n",
    "    #     cer = np.mean([decoder.cer(true, pred)\n",
    "    #                     for true, pred in zip(target_strings, decoded_output)])\n",
    "    #     val_stats['val_loss'].append(loss.item())\n",
    "    #     val_stats['wer'].append(wer)\n",
    "    #     val_stats['cer'].append(cer)\n",
    "    # for k, v in val_stats.items():\n",
    "    #     val_stats[k] = np.mean(v)\n",
    "    # val_stats['val_samples'] = wandb.Table(\n",
    "    #     columns=['gt_text', 'pred_text'], data=zip(target_strings, decoded_output))\n",
    "    # wandb.log(val_stats, step=step)\n",
    "\n",
    "    # # save model, TODO: save optimizer:\n",
    "    # if val_stats['wer'] < prev_wer:\n",
    "    #     os.makedirs(config.train.get(\n",
    "    #         'checkpoint_path', 'checkpoints'), exist_ok=True)\n",
    "    #     prev_wer = val_stats['wer']\n",
    "    #     torch.save(\n",
    "    #         model.state_dict(),\n",
    "    #         os.path.join(config.train.get(\n",
    "    #             'checkpoint_path', 'checkpoints'), f'model_{epoch_idx}_{prev_wer}.pth')\n",
    "    #     )\n",
    "    #     wandb.save(os.path.join(config.train.get(\n",
    "    #         'checkpoint_path', 'checkpoints'), f'model_{epoch_idx}_{prev_wer}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43825ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_idx in tqdm(range(config.train.get('epochs', 10))):\n",
    "    # train:\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        batch1=batch\n",
    "        batch = batch_transforms_train(batch)\n",
    "        batch2=batch\n",
    "        break\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch['audio'])\n",
    "        output_length = torch.ceil(\n",
    "            batch['input_lengths'].float() / model.stride).int()\n",
    "        loss = criterion(logits.permute(2, 0, 1).log_softmax(\n",
    "            dim=2), batch['text'], output_length, batch['target_lengths'])\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), config.train.get('clip_grad_norm', 15))\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        # warmup_scheduler.dampen()\n",
    "\n",
    "        if batch_idx % config.wandb.get('log_interval', 5000) == 0:\n",
    "            target_strings = decoder.convert_to_strings(batch['text'])\n",
    "            decoded_output = decoder.decode(\n",
    "                logits.permute(0, 2, 1).softmax(dim=2))\n",
    "            wer = np.mean([decoder.wer(true, pred)\n",
    "                            for true, pred in zip(target_strings, decoded_output)])\n",
    "            cer = np.mean([decoder.cer(true, pred)\n",
    "                            for true, pred in zip(target_strings, decoded_output)])\n",
    "            step = epoch_idx * \\\n",
    "                len(train_dataloader) * train_dataloader.batch_size + \\\n",
    "                batch_idx * train_dataloader.batch_size\n",
    "            wandb.log({\n",
    "                \"train_loss\": loss.item(),\n",
    "                \"train_wer\": wer,\n",
    "                \"train_cer\": cer,\n",
    "                \"train_samples\": wandb.Table(\n",
    "                    columns=['gt_text', 'pred_text'],\n",
    "                    data=zip(target_strings, decoded_output)\n",
    "                )\n",
    "            }, step=step)\n",
    "    # #!\n",
    "    # # validate:\n",
    "    # model.eval()\n",
    "    # val_stats = defaultdict(list)\n",
    "    # for batch_idx, batch in enumerate(val_dataloader):\n",
    "    #     batch = batch_transforms_val(batch)\n",
    "    #     with torch.no_grad():\n",
    "    #         logits = model(batch['audio'])\n",
    "    #         output_length = torch.ceil(\n",
    "    #             batch['input_lengths'].float() / model.stride).int()\n",
    "    #         loss = criterion(logits.permute(2, 0, 1).log_softmax(\n",
    "    #             dim=2), batch['text'], output_length, batch['target_lengths'])\n",
    "\n",
    "    #     target_strings = decoder.convert_to_strings(batch['text'])\n",
    "    #     decoded_output = decoder.decode(\n",
    "    #         logits.permute(0, 2, 1).softmax(dim=2))\n",
    "    #     wer = np.mean([decoder.wer(true, pred)\n",
    "    #                     for true, pred in zip(target_strings, decoded_output)])\n",
    "    #     cer = np.mean([decoder.cer(true, pred)\n",
    "    #                     for true, pred in zip(target_strings, decoded_output)])\n",
    "    #     val_stats['val_loss'].append(loss.item())\n",
    "    #     val_stats['wer'].append(wer)\n",
    "    #     val_stats['cer'].append(cer)\n",
    "    # for k, v in val_stats.items():\n",
    "    #     val_stats[k] = np.mean(v)\n",
    "    # val_stats['val_samples'] = wandb.Table(\n",
    "    #     columns=['gt_text', 'pred_text'], data=zip(target_strings, decoded_output))\n",
    "    # wandb.log(val_stats, step=step)\n",
    "\n",
    "    # # save model, TODO: save optimizer:\n",
    "    # if val_stats['wer'] < prev_wer:\n",
    "    #     os.makedirs(config.train.get(\n",
    "    #         'checkpoint_path', 'checkpoints'), exist_ok=True)\n",
    "    #     prev_wer = val_stats['wer']\n",
    "    #     torch.save(\n",
    "    #         model.state_dict(),\n",
    "    #         os.path.join(config.train.get(\n",
    "    #             'checkpoint_path', 'checkpoints'), f'model_{epoch_idx}_{prev_wer}.pth')\n",
    "    #     )\n",
    "    #     wandb.save(os.path.join(config.train.get(\n",
    "    #         'checkpoint_path', 'checkpoints'), f'model_{epoch_idx}_{prev_wer}.pth'))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ceb6b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': [array([ 0.00064805, -0.00185332, -0.00244806, ..., -0.00170346,\n",
      "          0.00287292,  0.00232188], dtype=float32),\n",
      "  array([-0.00314305,  0.00103363,  0.00466579, ...,  0.03743864,\n",
      "          0.04169904,  0.        ], dtype=float32),\n",
      "  array([-2.7460288e-03, -4.2517125e-03, -6.8397014e-05, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00], dtype=float32),\n",
      "  array([ 1.6830985e-04, -2.7138460e-05, -2.2539313e-04, ...,\n",
      "          2.5602366e-04,  1.0118349e-03,  0.0000000e+00], dtype=float32),\n",
      "  array([3.5219718e-04, 3.9384593e-04, 3.4842314e-04, ..., 1.4193331e-04,\n",
      "         1.3546018e-04, 8.1811209e-05], dtype=float32),\n",
      "  array([ 7.7085420e-03, -2.8882974e-03, -2.8774177e-05, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00], dtype=float32),\n",
      "  array([ 3.8510363e-05,  2.8201991e-03,  5.8321138e-03, ...,\n",
      "          4.3147802e-03,  1.1419075e-02, -6.4695268e-03], dtype=float32),\n",
      "  array([ 2.2583008e-03,  1.7700195e-03, -9.1552734e-05, ...,\n",
      "          1.8310547e-04, -1.8310547e-04, -3.3569336e-04], dtype=float32),\n",
      "  array([-8.3596539e-04, -1.0172019e-03, -7.7972747e-04, ...,\n",
      "         -9.2914495e-05, -3.4379747e-04,  1.0163203e-04], dtype=float32),\n",
      "  array([-0.00055806, -0.00051591, -0.00054907, ...,  0.        ,\n",
      "          0.        ,  0.        ], dtype=float32),\n",
      "  array([-0.0029076 , -0.00215218, -0.00059656, ...,  0.        ,\n",
      "          0.        ,  0.        ], dtype=float32),\n",
      "  array([-0.00168355, -0.00218567, -0.00184146, ...,  0.04594053,\n",
      "          0.05033867,  0.        ], dtype=float32),\n",
      "  array([0.00054366, 0.00054058, 0.0007952 , ..., 0.        , 0.        ,\n",
      "         0.        ], dtype=float32),\n",
      "  array([ 0.02272906,  0.0186081 , -0.00734079, ...,  0.05346785,\n",
      "          0.01619746, -0.01303736], dtype=float32),\n",
      "  array([-0.00841909, -0.01477104, -0.00033289, ..., -0.01118742,\n",
      "         -0.00514902, -0.00349659], dtype=float32),\n",
      "  array([-0.0003959 , -0.00292177,  0.00145343, ...,  0.01820027,\n",
      "          0.01412978,  0.01580742], dtype=float32),\n",
      "  array([-0.00232852, -0.00317211, -0.00290751, ...,  0.        ,\n",
      "          0.        ,  0.        ], dtype=float32),\n",
      "  array([-0.01470247, -0.00661179, -0.00250927, ...,  0.00378074,\n",
      "          0.00606666,  0.        ], dtype=float32),\n",
      "  array([-0.00082397, -0.00231934, -0.00234985, ..., -0.00177002,\n",
      "         -0.00119019,  0.00064087], dtype=float32),\n",
      "  array([ 0.00030518,  0.00027466,  0.00021362, ..., -0.00064087,\n",
      "         -0.00079346, -0.00082397], dtype=float32),\n",
      "  array([ 0.00084496,  0.00249144,  0.00144098, ..., -0.00069851,\n",
      "         -0.00108514, -0.00084857], dtype=float32),\n",
      "  array([-0.00063006,  0.00100754,  0.00508791, ..., -0.00277576,\n",
      "         -0.00245809,  0.00152176], dtype=float32),\n",
      "  array([-0.00081956, -0.00292294, -0.00256488, ...,  0.        ,\n",
      "          0.        ,  0.        ], dtype=float32),\n",
      "  array([-0.01048647,  0.0056416 , -0.00289302, ...,  0.        ,\n",
      "          0.        ,  0.        ], dtype=float32),\n",
      "  array([ 0.00680624,  0.00101481,  0.01428293, ..., -0.00096096,\n",
      "         -0.00834265, -0.00914024], dtype=float32),\n",
      "  array([-0.00057902, -0.00067707, -0.00095624, ..., -0.0004703 ,\n",
      "         -0.00043248, -0.00041621], dtype=float32),\n",
      "  array([ 0.0185535 , -0.00629476,  0.00299063, ..., -0.00245228,\n",
      "         -0.00088249,  0.00733585], dtype=float32),\n",
      "  array([-3.0517578e-05, -3.6621094e-04, -4.8828125e-04, ...,\n",
      "          9.1552734e-05,  5.4931641e-04,  3.0517578e-04], dtype=float32),\n",
      "  array([ 0.0015405 , -0.00042733, -0.002726  , ...,  0.00274575,\n",
      "         -0.00090818,  0.        ], dtype=float32),\n",
      "  array([-0.00360571, -0.00190096,  0.00153467, ...,  0.        ,\n",
      "          0.        ,  0.        ], dtype=float32),\n",
      "  array([-0.00581988, -0.00602441,  0.00972501, ...,  0.        ,\n",
      "          0.        ,  0.        ], dtype=float32),\n",
      "  array([ 0.00164117,  0.00101758, -0.0005462 , ...,  0.        ,\n",
      "          0.        ,  0.        ], dtype=float32)],\n",
      " 'text': [array([ 56,  12,  44,   6,  75,  65,  43,  93,  88,  48,  49, 103,  46,\n",
      "          62,  13,  99,  78,  13,  46,   5,  42,  47, 119,  56,  47,  11,\n",
      "          79,  53,  50,  16,  51,   9,  52,  73,  10,  17,  17,  51,  11,\n",
      "          57,  82,  19,  63,   8,  96,   4,  10,  17,   4,   9,  86,  57,\n",
      "          82,  19,  42,  98,  43,  42,  12,   6,  11,  72,  53,  12,   7,\n",
      "          17,   6,  11,  87,  21,  47,  11,  79,  52,  65,  43,  83,  27,\n",
      "          13,  10,  24,  62,  80]),\n",
      "  array([65, 90, 75, 53, 97, 21, 69, 54, 10, 95, 88, 63,  8, 14, 51,  9]),\n",
      "  array([106,  42,  15,   6,  13,  70, 107,  43,  53,  13,  44,  71,   5,\n",
      "          40,   8,   8,  26,   4,  94,  21,  47,  11,  11,  80,  11,  57,\n",
      "          82,  19,  46,   8,   8,  14,  55,  15,   8,  16,  26,  11,   4,\n",
      "          49,  22,  12,   7,  25,  52,  65,   4,  47,  15,  10,   5,  17,\n",
      "         106,  53,  79,  18, 116,  71,  90,  17,   8,  47,  43,  46,   8,\n",
      "           8,  14,  16, 104,   6,  51,  11,  58,  43,  77,   5,   6,  41,\n",
      "          12,  15,   7,  59,  11,  55,  23,  42,  48,  94,  10,  15,  69,\n",
      "          56,  82,  16,  71,  11]),\n",
      "  array([ 56,  82,  14,  18,  16,  52,  43,  55,  15,   8,  16,  26,  55,\n",
      "           8,   8,  26,  11,  99,  78,  13,  46, 114,   4,   6,  41,   4,\n",
      "          94,  19,  52,  10,  54,   5,  56,  47,  14,   5,  16,  71,  11,\n",
      "          64,  11,  58,  43,  40,  12,  18,   5,  56,  12,  44,   6,  52,\n",
      "          55,   8,   8,  26]),\n",
      "  array([ 43,  65,  25,  79,  80,  58,  63,   8,  25,   7,  24,  89,  63,\n",
      "         105,  68,  81, 105,   6,  51,  11,  65,  43,  63,  92,  14,  89,\n",
      "          58,  43,  57,  10,  17,   6,   5,  79,  13,  53,  79, 117,  23,\n",
      "          63,   7,  23,   4,  29,  18,  96,  88,   4,  24,   5,  53,  50,\n",
      "          11,  92, 114,  14, 109,  43,  65,  25,  79,  80,  58,  43,  42,\n",
      "          12,   6,  58,  56,  12,  44,   6,  75]),\n",
      "  array([ 72,   4,  62,   4,  61,  46,  64,   6,  13,  63,  79,  80,   4,\n",
      "          44,  56,  66,  11,  75, 100, 109,  42,   9,  83,  27,   7,  19,\n",
      "          21,  89,  58,  57,  44,   5,  40,  23,  21,   8,  22,  12,   7,\n",
      "          21,  13,  23]),\n",
      "  array([ 43,  83,  69,  15,  10,  71,   6,  55,   8,   8,  26,  56,  12,\n",
      "          44,   6,  52,  46,  62,  13,  63,   8,  25,   7,  24,  15,   5,\n",
      "          40,  23,  21,  71,  43, 101, 104,  49,  24,  51,  22,  45,  12,\n",
      "         106,   6,  23,   6,  20,   8,  81,  44,   5,  55,  10,  24,  89,\n",
      "          58,  42,  24,  70,   6,  57,   8, 117,   6,   5,  49,  57,  10,\n",
      "          17,   6,  23,  17,  10,  95]),\n",
      "  array([ 60,  66,  77,   5,  25,  51,  90,  49,  48, 117,  21,  66,  11,\n",
      "          52]),\n",
      "  array([ 56,  12,  44,   6,  75,  43,   9,  57,  64,   4,  70,  12,  56,\n",
      "         117,  21,   8, 103,  63,   7,  23,   4,  24,   5,  53,  50,  11,\n",
      "          92, 114,  14, 109,  43,  42,  12,   6,  58,  63,   7,  26,  75,\n",
      "           4,  24,   8,   8,  26,  11,  55,  23,  63,   5,  76,  11,  58,\n",
      "          63,   8,  25,   7,  24,  89,  40,  23,  21,  71]),\n",
      "  array([ 77,   8,  20, 109,  42,  98,  55,   8,   8,  26,  11,  77,   8,\n",
      "           6,  56,  12,  94,  69,  10,  88,  65,   6,  49,  14,  52, 109,\n",
      "          56,  78,   6,  18,  47,  24,   8,   8,  26,  11,  53,  50,  11,\n",
      "          61,   6,  56,  12,  44,  16,  10,  21,  68,  88,  58,  40,  23,\n",
      "          21,  71,  53,  97,  21,   8,  11,  52,  67, 106,  19,  81, 105,\n",
      "           6,  51,  21,  47,  11,  11]),\n",
      "  array([  4,  62,   4,  61,  58,  43,  57, 112,  96,   4,  94,  21,  64,\n",
      "           6,  76, 108, 100,  40,  13,   5,  81, 105,   6,  51, 110,  11,\n",
      "          52,  48,  13,  70, 113,  90,  57,  44,   5,  65,  57,   8,  12,\n",
      "          19]),\n",
      "  array([  4,  71,  21,   5,  16,  10,  68,  88, 109,  77,   8,  63,   8,\n",
      "          47,  40,  94,   5,   4,  61,  45,  16,  16,  18,  21,  10,  52,\n",
      "          45,  12,  53,   8,  96,  65,  16, 117,  47,  14,  65,  53,  66,\n",
      "           6,  75,  48, 105,   6,  75,  45,  12,  56,  12,  44,   6,  75,\n",
      "          90,   7, 104,  10,  17,  18,  15,  81, 105,   6,  51,  11]),\n",
      "  array([ 74,  76,  65,  43,  48,   7,  19,   5,  45,  21,  51,  54,  80,\n",
      "          11,  46,  62,  13, 110,  22,  88,   4,  50,  71]),\n",
      "  array([ 72,   4,  62,  84,  42,  63,  54,   6,  51,  58,  53,  70,  12,\n",
      "         103, 100,  65,  43,  63,  92,  14,  89,  42,  22,  71,  46,  41,\n",
      "           9,  43,  53,  12,   7,  17,   6,  11,  19,  49,  67,   8,  26,\n",
      "          53,   7,  47, 100,  90,   7, 104,  10,  17,  18,  15, 106,  19,\n",
      "          48,  13,  70, 113,  42,  15,  20,   7,  23,  11,  90,  42,  56,\n",
      "          69,   6,  58,  43, 112,  56,  82,  14,  18, 102,  80,  11,  99,\n",
      "          54,   5,  25,  51,  43,  23,  46, 114]),\n",
      "  array([ 43, 106,  19,  11,  58,  56,  12,  44,   6,  52,  81, 105,   6,\n",
      "          51,  11,  48,  13,  70, 113,  90,  90,   7, 104,  10,  17,  18,\n",
      "          15,  72, 100,  43, 112,  42,  12,  12,  76,  22,   5,  19,  79,\n",
      "          93,  43,  56,   7,  22,   5,  48,  13,  70, 113,  90,  87,  66,\n",
      "          50,   7,  24,  89,  72,  42,   4,  41,  15,  21,  67,  43,  48,\n",
      "          13,   7,  21,   5,  15,  44,  71,  11,  58,  43,  81, 105,   6,\n",
      "          51,  11,  43,  19, 103,  15,  25,  71]),\n",
      "  array([  4,   6,  41,  63,  92,  14,  89,  42,  22,  71,  55,  12,  70,\n",
      "         107,   6,  53,  68,  15,  10,  22,  12,   7,  21,  13,  23,  67,\n",
      "          56,  51,  17,   5, 102,  80,  72,   4,  62,  84,  77,  54, 117,\n",
      "          68,  43,  47,  17,   8,  47]),\n",
      "  array([100,  43, 106,  19,  11,  58,  56,  12,  44,   6,  52,  81, 105,\n",
      "           6,  51,  11,  48,  13,  70, 113,  57,   8,  98,   8,  20,  63,\n",
      "           8,  47,  45,  12,  81,  71,  11,  53,  15,   8, 103,  88,  74,\n",
      "           8, 103,  58,  43,  46,  12,  62,   6,  49,  53,  13,  69,   7,\n",
      "         102,  51,  72,  43,  23,  57,   8,  98,   8,  20,  52,  43,  19,\n",
      "           4,  25,  51,  23,  53,  15,   8,  11,   5,  88]),\n",
      "  array([ 43,  57, 112,  96,  55,   8,   8,  26,  11,  46, 114,  56,  12,\n",
      "          44,   6,  52,   4,  10,   9,  55,  15,   7,  16,  26,  81, 105,\n",
      "           6,  51,   4,  10,   5,  43,  81, 105,   6,  51,  99,  78,  13,\n",
      "          84,  42, 101,  86,  13,  78,  73,   5,  95,  15,   8,  21,  19,\n",
      "          79,  58,  43,  42,   9,  16,  10,  49,   6,   4,  82,  19,  76,\n",
      "          53,  13,  69,   7, 102,  51]),\n",
      "  array([ 72,  99,  78,  13,  73,   5,  95,  15,   8,  21,  52,  63,   8,\n",
      "          47,  53,  97,  21,  89,   6,   5,  88,  72,  48,  54,  61,  17,\n",
      "           7, 102,  64,  10,  88,  93,  43,  48,  92,   5,  58,  43,  81,\n",
      "           8,  20,  51,  16,  66,   5,  74,  76,  43,  53,   7,  21,  62,\n",
      "          68,  81, 105,   6,  51,  11]),\n",
      "  array([ 43,  81,   8,  20,  51,  16,  66,   5,  90,  75,  65,  57,   7,\n",
      "         102,  65,  25,  79,  52,  65,  43,  83,  69,  88,  63,  92,  14,\n",
      "          89,  42,  22,  71]),\n",
      "  array([ 43,  83,  69,  15,  10,  71,   6,  55,   8,   8,  26,  56,  12,\n",
      "          44,   6,  52,  46,  62,  13,  63,   8,  25,   7,  24,  89,  40,\n",
      "          23,  21,   5,  43,  42,  17,   8,  47,  11,   7,  92, 101, 104,\n",
      "          49,  24,  51,  22,  55,  10,  24,  89,   4,  61,  56,  12,  44,\n",
      "           6,  52,  65,  81, 105,   6,  51,  11,  99,  78,  13,  42,  47,\n",
      "          42,   9,  83,  27,   7, 102,   4,  94,  62,  54,  80]),\n",
      "  array([  4,   8,  17,  43,  63,   8,  47, 106,  19,  68,  83,  16,  16,\n",
      "          15,  71,  10,  66,   6,  78,  68,  46,  12,  62,  75,  99,  78,\n",
      "          13,  45,  24,   6,   7,  44,  52, 119, 100,  40,  94,   5,  74,\n",
      "          61,  60,   7,  11,  48,  44, 108,  90,  49,  53,  68,  15,  52,\n",
      "          63,  61,  11,  68,  40,  23,  21,   5]),\n",
      "  array([ 72,  84,  65,  57,   7, 102,  43,   4,  26,  44,  14,  58,  81,\n",
      "         105,   6,  51, 110,  11,  52,  65,  43,  63,  76,  23,  48,  21,\n",
      "          15,  49,  14,  92,  63,  61,  11,  68,  11,  56,  11,  68,   6,\n",
      "          51,  11,  83,   6,  16,  56,  82,  14,  18,  16,  52,  55,  23,\n",
      "          56,  12,  44,   6,  75,  65,  43,  57,  10,  17,   6,   5,  79,\n",
      "          13,  53,  79, 117,  23]),\n",
      "  array([ 55, 104,  43,  57, 112,  96,  55,  10,  24,  89,  42, 102,  18,\n",
      "          68,  88,  73,  54,  52,  99,  78,  13,  42,  15,  11,   8,  84,\n",
      "          56,  12,  44,   6,  52, 119,  63,   7,  44,   6,  30,  55,  23,\n",
      "          56, 105,  51,  48,  16,  13,   8,   5,  17,  17,  51,  65,  43,\n",
      "           4,  23,   5,  69,  57,  70,  12,   6,   5,  49,  48,  10,  27,\n",
      "           6,  23,   6,  20,   8]),\n",
      "  array([ 4, 94, 62, 54, 71, 42, 63, 18, 16, 13, 57, 47, 51, 60,  7, 59, 48,\n",
      "         94, 21, 15, 51,  4, 12, 70, 59, 51, 72, 81, 71, 11, 48, 21, 10, 26,\n",
      "         23, 72, 43, 47, 17,  8, 47, 57, 69, 56, 89, 66, 76,  6, 51, 72, 83,\n",
      "         66, 10, 51, 67, 87, 91]),\n",
      "  array([ 93,  43,  99,   8,  89,  43,  40,  23,  21,   5,  58,  74,  61,\n",
      "          55,   8,   8,  26,  63,   7,  23,  90,  53,  50,  11,  92, 114,\n",
      "          14,  74,   5,  77,   5,  21,  15,  18,  11,  18,  15,   6,  12,\n",
      "           7,  58, 101,  86,  13,  78,  40,  23,  21,   5]),\n",
      "  array([ 83,  11,  21,   5,  16,  10,  68,  88, 109,  87,  22,  69,  14,\n",
      "          11,  43,  81,   8,  20,  51,  16,  66,   5,  81, 105,   6,  51,\n",
      "          11,  72,  40,  23,  21,   5,   4,  25,  51,  23,  48,  94,  10,\n",
      "          15,  69,  84,   4,  18,  11,  52,  73, 117,  75,  43,  77,   5,\n",
      "          27,   6,  57,  10,  17,   6,   5,  49,  45,  12,  40,  20,  79,\n",
      "          23,   4,  23,   5,  69,  11,  77,  86,  93,  88,  55,  23,  48,\n",
      "          16,  13,   8,   5,  17,  17,  51]),\n",
      "  array([ 55, 104,  55,  23,  56,  12,  44,   6,  51,  11,  65, 111,  12,\n",
      "          66,  24, 117,  22,  55,  66,  15,   5,  56,  69,  61,  81,  18,\n",
      "          24,   5,  16,  26,  72,  45,   6,  41,  12,  53,  62,  10,  71]),\n",
      "  array([ 55, 104,  74,  70, 107,  93,  43,  99,   8,  89,  83,  27, 108,\n",
      "          21,   6,  65,   4,  62,  68,  23, 101,  86,  13,  78,  81, 105,\n",
      "           6,  51,  84,  63,   8,  96,  58,   6,  49, 110,  11,  52]),\n",
      "  array([ 42,   4,  25,  51,  23,  57,   5,  20,   4,  23,   5,  69,  11,\n",
      "          48,   7,  20,  43,  55, 112,   6,  13,  58,   4,  82,  19,  76,\n",
      "          53,  13,  69,   7, 102,  51,  77,  86,  93,  88,  65,   4,  62,\n",
      "          68,  23,  55, 104,  65, 101,  51,  19,  76,  23,  72,  57,  12,\n",
      "          76,  16,   5]),\n",
      "  array([65, 57, 70, 12,  6,  5, 49, 48, 10, 27,  6, 23, 17, 10, 95, 48, 20,\n",
      "          5, 23,  9, 41, 94, 72, 56, 76,  9, 69,  6, 30, 90, 22, 76, 56, 12,\n",
      "         44,  6, 75,  4, 44, 43, 63, 50, 66,  6, 51, 23, 58,  4, 11, 18, 24,\n",
      "         10,  7, 16,  8, 77,  5, 69,  4, 82, 19,  5]),\n",
      "  array([ 72, 110,  11,  52,  42,   9,  83,  27, 108,  52,  75,  88,  90,\n",
      "           7, 104,  10,  17,  18,  15,  40,  23,  21,   5,  99,  78,  13,\n",
      "           4,  61,  65,  14,   5,  52,  67,  81,   8,   8,  26, 119,  42,\n",
      "          40,  12,  76,  11,  62,  80,  90,   6,  20,   5,  49, 101,  86,\n",
      "          13,  78,  72,   4,  82,  19,  76])],\n",
      " 'sample_rate': [22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050,\n",
      "  22050]}"
     ]
    }
   ],
   "source": [
    "batch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc3545f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['audio', 'text', 'sample_rate'])"
     ]
    }
   ],
   "source": [
    "batch1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "170bbf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['audio', 'text', 'sample_rate', 'input_lengths', 'target_lengths'])"
     ]
    }
   ],
   "source": [
    "batch2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b029853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch2.input_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4806b137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1065,  210, 1066,  567,  895,  627,  925,  197,  833,  973,  498,  909,\n",
      "         285, 1097, 1019,  581,  774,  826,  708,  516,  950,  778,  932,  867,\n",
      "         978,  672, 1064,  654,  588,  763,  867,  781], device='cuda:0')"
     ]
    }
   ],
   "source": [
    "batch2['input_lengths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef225d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56,  12,  44,  ...,   0,   0,   0],\n",
      "        [ 65,  90,  75,  ...,   0,   0,   0],\n",
      "        [106,  42,  15,  ...,  16,  71,  11],\n",
      "        ...,\n",
      "        [ 42,   4,  25,  ...,   0,   0,   0],\n",
      "        [ 65,  57,  70,  ...,   0,   0,   0],\n",
      "        [ 72, 110,  11,  ...,   0,   0,   0]], device='cuda:0',\n",
      "       dtype=torch.int32)"
     ]
    }
   ],
   "source": [
    "batch2['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a6093ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 96])"
     ]
    }
   ],
   "source": [
    "batch2['text'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c407598",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1['text'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c80ea972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 56,  12,  44,   6,  75,  65,  43,  93,  88,  48,  49, 103,  46,\n",
      "         62,  13,  99,  78,  13,  46,   5,  42,  47, 119,  56,  47,  11,\n",
      "         79,  53,  50,  16,  51,   9,  52,  73,  10,  17,  17,  51,  11,\n",
      "         57,  82,  19,  63,   8,  96,   4,  10,  17,   4,   9,  86,  57,\n",
      "         82,  19,  42,  98,  43,  42,  12,   6,  11,  72,  53,  12,   7,\n",
      "         17,   6,  11,  87,  21,  47,  11,  79,  52,  65,  43,  83,  27,\n",
      "         13,  10,  24,  62,  80]),\n",
      " array([65, 90, 75, 53, 97, 21, 69, 54, 10, 95, 88, 63,  8, 14, 51,  9]),\n",
      " array([106,  42,  15,   6,  13,  70, 107,  43,  53,  13,  44,  71,   5,\n",
      "         40,   8,   8,  26,   4,  94,  21,  47,  11,  11,  80,  11,  57,\n",
      "         82,  19,  46,   8,   8,  14,  55,  15,   8,  16,  26,  11,   4,\n",
      "         49,  22,  12,   7,  25,  52,  65,   4,  47,  15,  10,   5,  17,\n",
      "        106,  53,  79,  18, 116,  71,  90,  17,   8,  47,  43,  46,   8,\n",
      "          8,  14,  16, 104,   6,  51,  11,  58,  43,  77,   5,   6,  41,\n",
      "         12,  15,   7,  59,  11,  55,  23,  42,  48,  94,  10,  15,  69,\n",
      "         56,  82,  16,  71,  11]),\n",
      " array([ 56,  82,  14,  18,  16,  52,  43,  55,  15,   8,  16,  26,  55,\n",
      "          8,   8,  26,  11,  99,  78,  13,  46, 114,   4,   6,  41,   4,\n",
      "         94,  19,  52,  10,  54,   5,  56,  47,  14,   5,  16,  71,  11,\n",
      "         64,  11,  58,  43,  40,  12,  18,   5,  56,  12,  44,   6,  52,\n",
      "         55,   8,   8,  26]),\n",
      " array([ 43,  65,  25,  79,  80,  58,  63,   8,  25,   7,  24,  89,  63,\n",
      "        105,  68,  81, 105,   6,  51,  11,  65,  43,  63,  92,  14,  89,\n",
      "         58,  43,  57,  10,  17,   6,   5,  79,  13,  53,  79, 117,  23,\n",
      "         63,   7,  23,   4,  29,  18,  96,  88,   4,  24,   5,  53,  50,\n",
      "         11,  92, 114,  14, 109,  43,  65,  25,  79,  80,  58,  43,  42,\n",
      "         12,   6,  58,  56,  12,  44,   6,  75]),\n",
      " array([ 72,   4,  62,   4,  61,  46,  64,   6,  13,  63,  79,  80,   4,\n",
      "         44,  56,  66,  11,  75, 100, 109,  42,   9,  83,  27,   7,  19,\n",
      "         21,  89,  58,  57,  44,   5,  40,  23,  21,   8,  22,  12,   7,\n",
      "         21,  13,  23]),\n",
      " array([ 43,  83,  69,  15,  10,  71,   6,  55,   8,   8,  26,  56,  12,\n",
      "         44,   6,  52,  46,  62,  13,  63,   8,  25,   7,  24,  15,   5,\n",
      "         40,  23,  21,  71,  43, 101, 104,  49,  24,  51,  22,  45,  12,\n",
      "        106,   6,  23,   6,  20,   8,  81,  44,   5,  55,  10,  24,  89,\n",
      "         58,  42,  24,  70,   6,  57,   8, 117,   6,   5,  49,  57,  10,\n",
      "         17,   6,  23,  17,  10,  95]),\n",
      " array([ 60,  66,  77,   5,  25,  51,  90,  49,  48, 117,  21,  66,  11,\n",
      "         52]),\n",
      " array([ 56,  12,  44,   6,  75,  43,   9,  57,  64,   4,  70,  12,  56,\n",
      "        117,  21,   8, 103,  63,   7,  23,   4,  24,   5,  53,  50,  11,\n",
      "         92, 114,  14, 109,  43,  42,  12,   6,  58,  63,   7,  26,  75,\n",
      "          4,  24,   8,   8,  26,  11,  55,  23,  63,   5,  76,  11,  58,\n",
      "         63,   8,  25,   7,  24,  89,  40,  23,  21,  71]),\n",
      " array([ 77,   8,  20, 109,  42,  98,  55,   8,   8,  26,  11,  77,   8,\n",
      "          6,  56,  12,  94,  69,  10,  88,  65,   6,  49,  14,  52, 109,\n",
      "         56,  78,   6,  18,  47,  24,   8,   8,  26,  11,  53,  50,  11,\n",
      "         61,   6,  56,  12,  44,  16,  10,  21,  68,  88,  58,  40,  23,\n",
      "         21,  71,  53,  97,  21,   8,  11,  52,  67, 106,  19,  81, 105,\n",
      "          6,  51,  21,  47,  11,  11]),\n",
      " array([  4,  62,   4,  61,  58,  43,  57, 112,  96,   4,  94,  21,  64,\n",
      "          6,  76, 108, 100,  40,  13,   5,  81, 105,   6,  51, 110,  11,\n",
      "         52,  48,  13,  70, 113,  90,  57,  44,   5,  65,  57,   8,  12,\n",
      "         19]),\n",
      " array([  4,  71,  21,   5,  16,  10,  68,  88, 109,  77,   8,  63,   8,\n",
      "         47,  40,  94,   5,   4,  61,  45,  16,  16,  18,  21,  10,  52,\n",
      "         45,  12,  53,   8,  96,  65,  16, 117,  47,  14,  65,  53,  66,\n",
      "          6,  75,  48, 105,   6,  75,  45,  12,  56,  12,  44,   6,  75,\n",
      "         90,   7, 104,  10,  17,  18,  15,  81, 105,   6,  51,  11]),\n",
      " array([ 74,  76,  65,  43,  48,   7,  19,   5,  45,  21,  51,  54,  80,\n",
      "         11,  46,  62,  13, 110,  22,  88,   4,  50,  71]),\n",
      " array([ 72,   4,  62,  84,  42,  63,  54,   6,  51,  58,  53,  70,  12,\n",
      "        103, 100,  65,  43,  63,  92,  14,  89,  42,  22,  71,  46,  41,\n",
      "          9,  43,  53,  12,   7,  17,   6,  11,  19,  49,  67,   8,  26,\n",
      "         53,   7,  47, 100,  90,   7, 104,  10,  17,  18,  15, 106,  19,\n",
      "         48,  13,  70, 113,  42,  15,  20,   7,  23,  11,  90,  42,  56,\n",
      "         69,   6,  58,  43, 112,  56,  82,  14,  18, 102,  80,  11,  99,\n",
      "         54,   5,  25,  51,  43,  23,  46, 114]),\n",
      " array([ 43, 106,  19,  11,  58,  56,  12,  44,   6,  52,  81, 105,   6,\n",
      "         51,  11,  48,  13,  70, 113,  90,  90,   7, 104,  10,  17,  18,\n",
      "         15,  72, 100,  43, 112,  42,  12,  12,  76,  22,   5,  19,  79,\n",
      "         93,  43,  56,   7,  22,   5,  48,  13,  70, 113,  90,  87,  66,\n",
      "         50,   7,  24,  89,  72,  42,   4,  41,  15,  21,  67,  43,  48,\n",
      "         13,   7,  21,   5,  15,  44,  71,  11,  58,  43,  81, 105,   6,\n",
      "         51,  11,  43,  19, 103,  15,  25,  71]),\n",
      " array([  4,   6,  41,  63,  92,  14,  89,  42,  22,  71,  55,  12,  70,\n",
      "        107,   6,  53,  68,  15,  10,  22,  12,   7,  21,  13,  23,  67,\n",
      "         56,  51,  17,   5, 102,  80,  72,   4,  62,  84,  77,  54, 117,\n",
      "         68,  43,  47,  17,   8,  47]),\n",
      " array([100,  43, 106,  19,  11,  58,  56,  12,  44,   6,  52,  81, 105,\n",
      "          6,  51,  11,  48,  13,  70, 113,  57,   8,  98,   8,  20,  63,\n",
      "          8,  47,  45,  12,  81,  71,  11,  53,  15,   8, 103,  88,  74,\n",
      "          8, 103,  58,  43,  46,  12,  62,   6,  49,  53,  13,  69,   7,\n",
      "        102,  51,  72,  43,  23,  57,   8,  98,   8,  20,  52,  43,  19,\n",
      "          4,  25,  51,  23,  53,  15,   8,  11,   5,  88]),\n",
      " array([ 43,  57, 112,  96,  55,   8,   8,  26,  11,  46, 114,  56,  12,\n",
      "         44,   6,  52,   4,  10,   9,  55,  15,   7,  16,  26,  81, 105,\n",
      "          6,  51,   4,  10,   5,  43,  81, 105,   6,  51,  99,  78,  13,\n",
      "         84,  42, 101,  86,  13,  78,  73,   5,  95,  15,   8,  21,  19,\n",
      "         79,  58,  43,  42,   9,  16,  10,  49,   6,   4,  82,  19,  76,\n",
      "         53,  13,  69,   7, 102,  51]),\n",
      " array([ 72,  99,  78,  13,  73,   5,  95,  15,   8,  21,  52,  63,   8,\n",
      "         47,  53,  97,  21,  89,   6,   5,  88,  72,  48,  54,  61,  17,\n",
      "          7, 102,  64,  10,  88,  93,  43,  48,  92,   5,  58,  43,  81,\n",
      "          8,  20,  51,  16,  66,   5,  74,  76,  43,  53,   7,  21,  62,\n",
      "         68,  81, 105,   6,  51,  11]),\n",
      " array([ 43,  81,   8,  20,  51,  16,  66,   5,  90,  75,  65,  57,   7,\n",
      "        102,  65,  25,  79,  52,  65,  43,  83,  69,  88,  63,  92,  14,\n",
      "         89,  42,  22,  71]),\n",
      " array([ 43,  83,  69,  15,  10,  71,   6,  55,   8,   8,  26,  56,  12,\n",
      "         44,   6,  52,  46,  62,  13,  63,   8,  25,   7,  24,  89,  40,\n",
      "         23,  21,   5,  43,  42,  17,   8,  47,  11,   7,  92, 101, 104,\n",
      "         49,  24,  51,  22,  55,  10,  24,  89,   4,  61,  56,  12,  44,\n",
      "          6,  52,  65,  81, 105,   6,  51,  11,  99,  78,  13,  42,  47,\n",
      "         42,   9,  83,  27,   7, 102,   4,  94,  62,  54,  80]),\n",
      " array([  4,   8,  17,  43,  63,   8,  47, 106,  19,  68,  83,  16,  16,\n",
      "         15,  71,  10,  66,   6,  78,  68,  46,  12,  62,  75,  99,  78,\n",
      "         13,  45,  24,   6,   7,  44,  52, 119, 100,  40,  94,   5,  74,\n",
      "         61,  60,   7,  11,  48,  44, 108,  90,  49,  53,  68,  15,  52,\n",
      "         63,  61,  11,  68,  40,  23,  21,   5]),\n",
      " array([ 72,  84,  65,  57,   7, 102,  43,   4,  26,  44,  14,  58,  81,\n",
      "        105,   6,  51, 110,  11,  52,  65,  43,  63,  76,  23,  48,  21,\n",
      "         15,  49,  14,  92,  63,  61,  11,  68,  11,  56,  11,  68,   6,\n",
      "         51,  11,  83,   6,  16,  56,  82,  14,  18,  16,  52,  55,  23,\n",
      "         56,  12,  44,   6,  75,  65,  43,  57,  10,  17,   6,   5,  79,\n",
      "         13,  53,  79, 117,  23]),\n",
      " array([ 55, 104,  43,  57, 112,  96,  55,  10,  24,  89,  42, 102,  18,\n",
      "         68,  88,  73,  54,  52,  99,  78,  13,  42,  15,  11,   8,  84,\n",
      "         56,  12,  44,   6,  52, 119,  63,   7,  44,   6,  30,  55,  23,\n",
      "         56, 105,  51,  48,  16,  13,   8,   5,  17,  17,  51,  65,  43,\n",
      "          4,  23,   5,  69,  57,  70,  12,   6,   5,  49,  48,  10,  27,\n",
      "          6,  23,   6,  20,   8]),\n",
      " array([ 4, 94, 62, 54, 71, 42, 63, 18, 16, 13, 57, 47, 51, 60,  7, 59, 48,\n",
      "        94, 21, 15, 51,  4, 12, 70, 59, 51, 72, 81, 71, 11, 48, 21, 10, 26,\n",
      "        23, 72, 43, 47, 17,  8, 47, 57, 69, 56, 89, 66, 76,  6, 51, 72, 83,\n",
      "        66, 10, 51, 67, 87, 91]),\n",
      " array([ 93,  43,  99,   8,  89,  43,  40,  23,  21,   5,  58,  74,  61,\n",
      "         55,   8,   8,  26,  63,   7,  23,  90,  53,  50,  11,  92, 114,\n",
      "         14,  74,   5,  77,   5,  21,  15,  18,  11,  18,  15,   6,  12,\n",
      "          7,  58, 101,  86,  13,  78,  40,  23,  21,   5]),\n",
      " array([ 83,  11,  21,   5,  16,  10,  68,  88, 109,  87,  22,  69,  14,\n",
      "         11,  43,  81,   8,  20,  51,  16,  66,   5,  81, 105,   6,  51,\n",
      "         11,  72,  40,  23,  21,   5,   4,  25,  51,  23,  48,  94,  10,\n",
      "         15,  69,  84,   4,  18,  11,  52,  73, 117,  75,  43,  77,   5,\n",
      "         27,   6,  57,  10,  17,   6,   5,  49,  45,  12,  40,  20,  79,\n",
      "         23,   4,  23,   5,  69,  11,  77,  86,  93,  88,  55,  23,  48,\n",
      "         16,  13,   8,   5,  17,  17,  51]),\n",
      " array([ 55, 104,  55,  23,  56,  12,  44,   6,  51,  11,  65, 111,  12,\n",
      "         66,  24, 117,  22,  55,  66,  15,   5,  56,  69,  61,  81,  18,\n",
      "         24,   5,  16,  26,  72,  45,   6,  41,  12,  53,  62,  10,  71]),\n",
      " array([ 55, 104,  74,  70, 107,  93,  43,  99,   8,  89,  83,  27, 108,\n",
      "         21,   6,  65,   4,  62,  68,  23, 101,  86,  13,  78,  81, 105,\n",
      "          6,  51,  84,  63,   8,  96,  58,   6,  49, 110,  11,  52]),\n",
      " array([ 42,   4,  25,  51,  23,  57,   5,  20,   4,  23,   5,  69,  11,\n",
      "         48,   7,  20,  43,  55, 112,   6,  13,  58,   4,  82,  19,  76,\n",
      "         53,  13,  69,   7, 102,  51,  77,  86,  93,  88,  65,   4,  62,\n",
      "         68,  23,  55, 104,  65, 101,  51,  19,  76,  23,  72,  57,  12,\n",
      "         76,  16,   5]),\n",
      " array([65, 57, 70, 12,  6,  5, 49, 48, 10, 27,  6, 23, 17, 10, 95, 48, 20,\n",
      "         5, 23,  9, 41, 94, 72, 56, 76,  9, 69,  6, 30, 90, 22, 76, 56, 12,\n",
      "        44,  6, 75,  4, 44, 43, 63, 50, 66,  6, 51, 23, 58,  4, 11, 18, 24,\n",
      "        10,  7, 16,  8, 77,  5, 69,  4, 82, 19,  5]),\n",
      " array([ 72, 110,  11,  52,  42,   9,  83,  27, 108,  52,  75,  88,  90,\n",
      "          7, 104,  10,  17,  18,  15,  40,  23,  21,   5,  99,  78,  13,\n",
      "          4,  61,  65,  14,   5,  52,  67,  81,   8,   8,  26, 119,  42,\n",
      "         40,  12,  76,  11,  62,  80,  90,   6,  20,   5,  49, 101,  86,\n",
      "         13,  78,  72,   4,  82,  19,  76])]"
     ]
    }
   ],
   "source": [
    "batch1['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "75e0a44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 56,  12,  44,   6,  75,  65,  43,  93,  88,  48,  49, 103,  46,\n",
      "        62,  13,  99,  78,  13,  46,   5,  42,  47, 119,  56,  47,  11,\n",
      "        79,  53,  50,  16,  51,   9,  52,  73,  10,  17,  17,  51,  11,\n",
      "        57,  82,  19,  63,   8,  96,   4,  10,  17,   4,   9,  86,  57,\n",
      "        82,  19,  42,  98,  43,  42,  12,   6,  11,  72,  53,  12,   7,\n",
      "        17,   6,  11,  87,  21,  47,  11,  79,  52,  65,  43,  83,  27,\n",
      "        13,  10,  24,  62,  80])"
     ]
    }
   ],
   "source": [
    "batch1['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b5084c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1['text'][0].len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b457dff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83"
     ]
    }
   ],
   "source": [
    "len(batch1['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a72e47ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32"
     ]
    }
   ],
   "source": [
    "len(batch1['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "669813a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 56,  12,  44,   6,  75,  65,  43,  93,  88,  48,  49, 103,  46,\n",
      "        62,  13,  99,  78,  13,  46,   5,  42,  47, 119,  56,  47,  11,\n",
      "        79,  53,  50,  16,  51,   9,  52,  73,  10,  17,  17,  51,  11,\n",
      "        57,  82,  19,  63,   8,  96,   4,  10,  17,   4,   9,  86,  57,\n",
      "        82,  19,  42,  98,  43,  42,  12,   6,  11,  72,  53,  12,   7,\n",
      "        17,   6,  11,  87,  21,  47,  11,  79,  52,  65,  43,  83,  27,\n",
      "        13,  10,  24,  62,  80])"
     ]
    }
   ],
   "source": [
    "batch1['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "04787188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset at 0x1ee6cd9f280>"
     ]
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8bb83ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__',\n",
      " '__annotations__',\n",
      " '__class__',\n",
      " '__class_getitem__',\n",
      " '__delattr__',\n",
      " '__dict__',\n",
      " '__dir__',\n",
      " '__doc__',\n",
      " '__eq__',\n",
      " '__format__',\n",
      " '__ge__',\n",
      " '__getattribute__',\n",
      " '__getitem__',\n",
      " '__gt__',\n",
      " '__hash__',\n",
      " '__init__',\n",
      " '__init_subclass__',\n",
      " '__le__',\n",
      " '__len__',\n",
      " '__lt__',\n",
      " '__module__',\n",
      " '__ne__',\n",
      " '__new__',\n",
      " '__orig_bases__',\n",
      " '__parameters__',\n",
      " '__reduce__',\n",
      " '__reduce_ex__',\n",
      " '__repr__',\n",
      " '__setattr__',\n",
      " '__sizeof__',\n",
      " '__slots__',\n",
      " '__str__',\n",
      " '__subclasshook__',\n",
      " '__weakref__',\n",
      " '_is_protocol',\n",
      " 'dataset',\n",
      " 'indices']"
     ]
    }
   ],
   "source": [
    "dir(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "442dfbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "88e7e59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<data.ljspeech.LJSpeechDataset at 0x1ee6cd5b2b0>"
     ]
    }
   ],
   "source": [
    "train_dataset.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "000a2c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__',\n",
      " '__class__',\n",
      " '__class_getitem__',\n",
      " '__delattr__',\n",
      " '__dict__',\n",
      " '__dir__',\n",
      " '__doc__',\n",
      " '__eq__',\n",
      " '__format__',\n",
      " '__ge__',\n",
      " '__getattribute__',\n",
      " '__getitem__',\n",
      " '__gt__',\n",
      " '__hash__',\n",
      " '__init__',\n",
      " '__init_subclass__',\n",
      " '__le__',\n",
      " '__len__',\n",
      " '__lt__',\n",
      " '__module__',\n",
      " '__ne__',\n",
      " '__new__',\n",
      " '__orig_bases__',\n",
      " '__parameters__',\n",
      " '__reduce__',\n",
      " '__reduce_ex__',\n",
      " '__repr__',\n",
      " '__setattr__',\n",
      " '__sizeof__',\n",
      " '__slots__',\n",
      " '__str__',\n",
      " '__subclasshook__',\n",
      " '__weakref__',\n",
      " '_flist',\n",
      " '_is_protocol',\n",
      " '_metadata_path',\n",
      " '_parse_filesystem',\n",
      " '_path',\n",
      " 'get_text',\n",
      " 'transforms']"
     ]
    }
   ],
   "source": [
    "dir(train_dataset.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "580277f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(train_dataset.dataset.getitem())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0564d213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__',\n",
      " '__class__',\n",
      " '__class_getitem__',\n",
      " '__delattr__',\n",
      " '__dict__',\n",
      " '__dir__',\n",
      " '__doc__',\n",
      " '__eq__',\n",
      " '__format__',\n",
      " '__ge__',\n",
      " '__getattribute__',\n",
      " '__getitem__',\n",
      " '__gt__',\n",
      " '__hash__',\n",
      " '__init__',\n",
      " '__init_subclass__',\n",
      " '__le__',\n",
      " '__len__',\n",
      " '__lt__',\n",
      " '__module__',\n",
      " '__ne__',\n",
      " '__new__',\n",
      " '__orig_bases__',\n",
      " '__parameters__',\n",
      " '__reduce__',\n",
      " '__reduce_ex__',\n",
      " '__repr__',\n",
      " '__setattr__',\n",
      " '__sizeof__',\n",
      " '__slots__',\n",
      " '__str__',\n",
      " '__subclasshook__',\n",
      " '__weakref__',\n",
      " '_flist',\n",
      " '_is_protocol',\n",
      " '_metadata_path',\n",
      " '_parse_filesystem',\n",
      " '_path',\n",
      " 'get_text',\n",
      " 'transforms']"
     ]
    }
   ],
   "source": [
    "dir(train_dataset.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "37bfda7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.dataset__getitem__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "01ea534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.dataset.__getitem__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "607d70a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': array([ 0.00337451, -0.00338884, -0.00205553, ...,  0.00065728,\n",
      "         0.00708335,  0.00721541], dtype=float32),\n",
      " 'text': array([ 56,  12,  44,   6,  75,  65,  43,  93,  88,  48,  49, 103,  46,\n",
      "         62,  13,  99,  78,  13,  46,   5,  42,  47, 119,  56,  47,  11,\n",
      "         79,  53,  50,  16,  51,   9,  52,  73,  10,  17,  17,  51,  11,\n",
      "         57,  82,  19,  63,   8,  96,   4,  10,  17,  77,  86,  57,  82,\n",
      "         19,  42,  98,  43,  42,  12,   6,  11,  72,  53,  12,   7,  17,\n",
      "          6,  11,  87,  21,  47,  11,  79,  52,  65,  43,  83,  27,  13,\n",
      "         10,  24,  62,  80]),\n",
      " 'sample_rate': 22050}"
     ]
    }
   ],
   "source": [
    "train_dataset.dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b909007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "train_dataset_2 = dataset_module.get_dataset(\n",
    "    config, part='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d3ecf0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': tensor([[-7.3242e-04, -7.6294e-04, -6.4087e-04,  ...,  7.3242e-04,\n",
      "           2.1362e-04,  6.1035e-05]]),\n",
      " 'text': 'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition',\n",
      " 'sample_rate': 22050}"
     ]
    }
   ],
   "source": [
    "# load datasets\n",
    "train_dataset_2 = dataset_module.get_dataset(\n",
    "    config, part='train')\n",
    "train_dataset_2.dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "055338fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': tensor([[-7.3242e-04, -7.6294e-04, -6.4087e-04,  ...,  7.3242e-04,\n",
      "           2.1362e-04,  6.1035e-05]]),\n",
      " 'text': 'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition',\n",
      " 'sample_rate': 22050}"
     ]
    }
   ],
   "source": [
    "# load datasets\n",
    "train_dataset_2 = dataset_module.get_dataset(\n",
    "    config, part='train')\n",
    "train_dataset_2.dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0207f849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': array([0.00087503, 0.00045779, 0.00011389, ..., 0.        , 0.        ,\n",
      "        0.        ], dtype=float32),\n",
      " 'text': array([ 56,  12,  44,   6,  75,  65,  43,  93,  88,  48,  49, 103,  46,\n",
      "         62,  13,  46,  13,  78,  13,  46,   5,  42,  47, 119,  56,  47,\n",
      "         11,  79,  53,  50,  16,  51,   9,  52,  73,  10,  17,  17,  51,\n",
      "         11,  57,  82,  19,  63,   8,  96,   4,  10,  17,  77,  86,  57,\n",
      "         82,  19,  42,  15,  15,  43,  42,  12,   6,  11,  72,  53,  12,\n",
      "          7,  17,   6,  11,  87,  21,  47,  11,  79,  52,  65,  43,  83,\n",
      "         27,  13,  10,  24,  62,  80]),\n",
      " 'sample_rate': 22050}"
     ]
    }
   ],
   "source": [
    "train_dataset.dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6af3dc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': tensor([[-7.3242e-04, -7.6294e-04, -6.4087e-04,  ...,  7.3242e-04,\n",
      "           2.1362e-04,  6.1035e-05]]),\n",
      " 'text': 'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition',\n",
      " 'sample_rate': 22050}"
     ]
    }
   ],
   "source": [
    "# load datasets\n",
    "train_dataset_2 = dataset_module.get_dataset(\n",
    "    config, part='train')\n",
    "train_dataset_2.dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c7256209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': tensor([[-7.3242e-04, -7.6294e-04, -6.4087e-04,  ...,  7.3242e-04,\n",
      "           2.1362e-04,  6.1035e-05]]),\n",
      " 'text': 'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition',\n",
      " 'sample_rate': 22050}"
     ]
    }
   ],
   "source": [
    "# load datasets\n",
    "train_dataset_2 = dataset_module.get_dataset(\n",
    "    config, part='train')\n",
    "train_dataset_2.dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4c1d9e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': array([-6.6338282e-04, -7.4983307e-04, -6.5849890e-04, ...,\n",
      "        -1.4586677e-04, -9.8007549e-05, -7.8858320e-05], dtype=float32),\n",
      " 'text': array([ 56,  12,  44,   6,  75,  65,  43,  93,  88,  48,  49, 103,  46,\n",
      "         62,  13,  99,  78,  13,  46,   5,  42,  47, 119,  56,  47,  11,\n",
      "         79,  53,  50,  16,  51,   9,  52,  73,  10,  17,  17,  51,  11,\n",
      "         57,  82,  19,  63,   8,  96,   4,  10,  17,  77,   8,   6,  57,\n",
      "         82,  19,  42,  98,  43,  42,  12,   6,  11,  72,  53,  12,   7,\n",
      "         17,   6,  11,  87,  21,  47,  11,  79,  52,  65,  43,  83,  27,\n",
      "         13,  10,  24,  62,  80]),\n",
      " 'sample_rate': 22050}"
     ]
    }
   ],
   "source": [
    "train_dataset.dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bc03d8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': array([-7.3242188e-04, -7.6293945e-04, -6.4086914e-04, ...,\n",
      "         7.3242188e-04,  2.1362305e-04,  6.1035156e-05], dtype=float32),\n",
      " 'text': array([ 56,  12,  44,   6,  75,  65,  43,   4,  50,  88,  48,  49, 103,\n",
      "         46,  62,  13,  99,  78,  13,  46,   5,  42,  47, 119,  56,  47,\n",
      "         11,  79,  53,  50,  16,  51,   9,  52,   4,  14,  10,  17,  17,\n",
      "         51,  11,  57,  82,  19,  63,   8,  96,   4,  10,  17,  77,  86,\n",
      "         57,  82,  19,  42,  15,  15,  43,  42,  12,   6,  11,  72,   4,\n",
      "         16,  12,   7,  17,   6,  11,  87,  21,  47,  11,  79,  52,   4,\n",
      "         10,   9,  43,  83,  27,  13,  10,  24,  62,  80]),\n",
      " 'sample_rate': 22050}"
     ]
    }
   ],
   "source": [
    "train_dataset.dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c1cc77e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': array([ 3.0744888e-04,  8.4192281e-05, -2.1004396e-04, ...,\n",
      "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00], dtype=float32),\n",
      " 'text': array([56, 12, 44,  6, 75,  4, 10,  9, 74,  5, 93, 88, 48, 49, 11,  5, 46,\n",
      "        62, 13, 99, 78, 13, 46,  5, 42, 47,  4, 54, 56, 47, 11, 79, 53, 50,\n",
      "        16, 51,  9, 52, 73, 10, 17, 17, 51, 11, 57, 82, 19, 63,  8, 96,  4,\n",
      "        10, 17, 77, 86, 57, 82, 19, 42, 98, 43, 42, 12,  6, 11, 72, 53, 12,\n",
      "         7, 17,  6, 11, 87, 21, 47, 11, 79, 52, 65, 43, 83, 27, 13, 10, 24,\n",
      "        62, 80]),\n",
      " 'sample_rate': 22050}"
     ]
    }
   ],
   "source": [
    "train_dataset.dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fab7f3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': array([7.1615778e-04, 3.3713618e-04, 3.9294842e-05, ..., 0.0000000e+00,\n",
      "        0.0000000e+00, 0.0000000e+00], dtype=float32),\n",
      " 'text': array([ 56,  12,  44,   6,  75,  65,  43,  93,  88,  48,  49, 103,  46,\n",
      "         62,  13,  99,  78,  13,  46,   5,  42,  47, 119,  56,  47,  11,\n",
      "         79,  53,  50,  16,  51,   9,  52,  73,  10,  17,  17,  51,  11,\n",
      "         57,  82,  19,  63,   8,  96,   4,  10,  17,  77,  86,  57,  82,\n",
      "         19,  42,  98,  43,  42,  12,   6,  11,  72,  53,  12,   7,  17,\n",
      "          6,  11,  87,  21,  47,  11,  79,  52,  65,  43,  83,  27,  13,\n",
      "         10,  24,  62,  80]),\n",
      " 'sample_rate': 22050}"
     ]
    }
   ],
   "source": [
    "train_dataset.dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "62f2d680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': array([0.00112545, 0.00064798, 0.00023147, ..., 0.        , 0.        ,\n",
      "        0.        ], dtype=float32),\n",
      " 'text': array([ 56,  12,  44,   6,  75,  65,  43,  93,  88,  48,  49, 103,  46,\n",
      "         62,  13,  99,  78,  13,  46,   5,  42,  47, 119,  56,  47,  11,\n",
      "         79,  53,  50,  16,  51,   9,  52,  73,  10,  17,  17,  51,  11,\n",
      "         57,  82,  19,  63,   8,  11,   6,   4,  10,  17,  77,  86,  57,\n",
      "         82,  19,  42,  98,  43,  42,  12,   6,  11,  72,  53,  12,   7,\n",
      "         17,   6,  11,  87,  21,  47,  11,  79,  52,  65,  43,  83,  27,\n",
      "         13,  10,  24,  62,  80]),\n",
      " 'sample_rate': 22050}"
     ]
    }
   ],
   "source": [
    "train_dataset.dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "03dd3250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': array([-0.00761147,  0.00097147,  0.0043958 , ...,  0.        ,\n",
      "         0.        ,  0.        ], dtype=float32),\n",
      " 'text': array([ 56,  12,  44,   6,  75,  65,  43,  93,  88,  48,  49,  11,   5,\n",
      "         46,  62,  13,  46,  13,  78,  13,  46,   5,  42,  47, 119,  56,\n",
      "         47,  11,  79,  53,  50,  16,  51,   9,  52,  73,  10,  17,  17,\n",
      "         51,  11,  57,  82,  19,  63,   8,  96,   4,  10,  17,  77,  86,\n",
      "         57,  82,  19,  42,  98,  43,  42,  12,   6,  11,  42,  59,  53,\n",
      "         12,   7,  17,   6,  11,  87,  21,  47,  11,  79,  52,  65,  43,\n",
      "         83,  27,  13,  10,  24,  62,  80]),\n",
      " 'sample_rate': 22050}"
     ]
    }
   ],
   "source": [
    "train_dataset.dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1f491b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': array([-0.00902189,  0.00607888, -0.0015608 , ..., -0.00049304,\n",
      "         0.00043237,  0.        ], dtype=float32),\n",
      " 'text': array([ 56,  12,  44,   6,  75,   4,  10,   9,  43,  93,  88,  48,  49,\n",
      "        103,  46,  62,  13,  99,  78,  13,  46,   5,  42,  47, 119,  56,\n",
      "         47,  11,  79,  53,  50,  16,  51,   9,  52,  73,  10,  17,  17,\n",
      "         51,  11,  57,  82,  19,  63,   8,  96,   4,  10,  17,  77,  86,\n",
      "         57,  82,  19,  42,  98,  43,  42,  12,   6,  11,  72,  53,  12,\n",
      "          7,  17,   6,  11,  87,  21,  47,  11,  79,  52,  65,  43,   4,\n",
      "          5,  27,  13,  10,  24,  62,  80]),\n",
      " 'sample_rate': 22050}"
     ]
    }
   ],
   "source": [
    "train_dataset.dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b64e9089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': array([ 0.00728497, -0.00395672,  0.01365469, ..., -0.00145744,\n",
      "         0.00241929,  0.        ], dtype=float32),\n",
      " 'text': array([ 56,  12,  44,   6,  75,  65,  43,  93,  88,  48,  49, 103,  46,\n",
      "         62,  13,  99,  78,  13,  46,   5,  42,  47, 119,  56,  47,  11,\n",
      "         79,  53,  50,  16,  51,   9,  52,  73,  10,  17,  17,  51,  11,\n",
      "         57,  82,  19,  63,   8,  96,   4,  10,  17,  77,  86,  57,  82,\n",
      "         19,  42,  98,  43,  42,  12,   6,  11,  72,  53,  12,   7,  17,\n",
      "          6,  11,  87,  21,  47,  11,  79,  52,  65,  43,  83,  27,  13,\n",
      "         10,  24,  62,  80]),\n",
      " 'sample_rate': 22050}"
     ]
    }
   ],
   "source": [
    "train_dataset.dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c030c2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': array([-0.00930794, -0.0199947 , -0.01293118, ..., -0.01280719,\n",
      "         0.00850524,  0.        ], dtype=float32),\n",
      " 'text': array([ 56,  12,  44,   6,  75,  65,  43,  93,  88,  48,  49, 103,  46,\n",
      "         62,  13,  99,  78,  13,  46,   5,  42,  47, 119,  56,  47,  11,\n",
      "         79,  53,  50,  16,  51,   9,  52,  73,  10,  17,  17,  51,  11,\n",
      "         57,  82,  19,  63,   8,  96,   4,  10,  17,  77,  86,  57,  82,\n",
      "         19,  42,  98,  43,  42,  12,   6,  11,  72,  53,  12,   7,  17,\n",
      "          6,  11,  87,  21,  47,  11,  79,  52,  65,  43,  83,  27,  13,\n",
      "         10,  24,  62,  80]),\n",
      " 'sample_rate': 22050}"
     ]
    }
   ],
   "source": [
    "train_dataset.dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e418ca74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83"
     ]
    }
   ],
   "source": [
    "train_dataset.dataset.__getitem__(0)\n",
    "len(train_dataset.dataset.__getitem__(0)['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "98830a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151"
     ]
    }
   ],
   "source": [
    "# load datasets\n",
    "train_dataset_2 = dataset_module.get_dataset(\n",
    "    config, part='train')\n",
    "train_dataset_2.dataset.__getitem__(0)\n",
    "len(train_dataset_2.dataset.__getitem__(0)['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a9ec1af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84"
     ]
    }
   ],
   "source": [
    "train_dataset.dataset.__getitem__(0)\n",
    "len(train_dataset.dataset.__getitem__(0)['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "23357557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151"
     ]
    }
   ],
   "source": [
    "# load datasets\n",
    "train_dataset_2 = dataset_module.get_dataset(\n",
    "    config, part='train')\n",
    "train_dataset_2.dataset.__getitem__(0)\n",
    "len(train_dataset_2.dataset.__getitem__(0)['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e47191c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83"
     ]
    }
   ],
   "source": [
    "train_dataset.dataset.__getitem__(0)\n",
    "len(train_dataset.dataset.__getitem__(0)['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "05a7384f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84"
     ]
    }
   ],
   "source": [
    "train_dataset.dataset.__getitem__(0)\n",
    "len(train_dataset.dataset.__getitem__(0)['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3ff09e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83"
     ]
    }
   ],
   "source": [
    "train_dataset.dataset.__getitem__(0)\n",
    "len(train_dataset.dataset.__getitem__(0)['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b1c4070a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86"
     ]
    }
   ],
   "source": [
    "train_dataset.dataset.__getitem__(0)\n",
    "len(train_dataset.dataset.__getitem__(0)['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ad0689bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': tensor([[-7.3242e-04, -7.6294e-04, -6.4087e-04,  ...,  7.3242e-04,\n",
      "           2.1362e-04,  6.1035e-05]]),\n",
      " 'text': 'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition',\n",
      " 'sample_rate': 22050}"
     ]
    }
   ],
   "source": [
    "# load datasets\n",
    "train_dataset_2 = dataset_module.get_dataset(\n",
    "    config, part='train')\n",
    "train_dataset_2.dataset.__getitem__(0)\n",
    "# len(train_dataset_2.dataset.__getitem__(0)['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b658503d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': array([ 3.74017225e-04,  7.72984131e-05, -1.21351484e-04, ...,\n",
      "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00], dtype=float32),\n",
      " 'text': array([ 56,  12,  44,   6,  75,  65,  43,  93,  88,  48,  49, 103,  46,\n",
      "         62,  13,  99,  78,  13,  46,   5,  42,  47, 119,  56,  47,  11,\n",
      "         79,  53,  50,  16,  51,   9,  52,   4,  14,  10,  17,  17,  51,\n",
      "         11,  57,  82,  19,  63,   8,  96,   4,  10,  17,  77,  86,  57,\n",
      "         82,  19,  42,  98,  43,   4,  69,   6,  11,  72,   4,  16,  12,\n",
      "          7,  17,   6,  11,  87,  21,  47,  11,  79,  52,  65,  43,  83,\n",
      "         27,  13,  10,  24,  62,  80]),\n",
      " 'sample_rate': 22050}"
     ]
    }
   ],
   "source": [
    "train_dataset.dataset.__getitem__(0)\n",
    "# len(train_dataset.dataset.__getitem__(0)['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "db824903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'root': 'DB/LJspeech',\n",
      " 'train_part': 0.95,\n",
      " 'name': 'ljspeech',\n",
      " 'sample_rate': 22050}"
     ]
    }
   ],
   "source": [
    "config.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "371ede7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__',\n",
      " '__class_getitem__',\n",
      " '__contains__',\n",
      " '__delattr__',\n",
      " '__delitem__',\n",
      " '__dict__',\n",
      " '__dir__',\n",
      " '__doc__',\n",
      " '__eq__',\n",
      " '__format__',\n",
      " '__ge__',\n",
      " '__getattribute__',\n",
      " '__getitem__',\n",
      " '__gt__',\n",
      " '__hash__',\n",
      " '__init__',\n",
      " '__init_subclass__',\n",
      " '__ior__',\n",
      " '__iter__',\n",
      " '__le__',\n",
      " '__len__',\n",
      " '__lt__',\n",
      " '__module__',\n",
      " '__ne__',\n",
      " '__new__',\n",
      " '__or__',\n",
      " '__reduce__',\n",
      " '__reduce_ex__',\n",
      " '__repr__',\n",
      " '__reversed__',\n",
      " '__ror__',\n",
      " '__setattr__',\n",
      " '__setitem__',\n",
      " '__sizeof__',\n",
      " '__str__',\n",
      " '__subclasshook__',\n",
      " '__weakref__',\n",
      " 'clear',\n",
      " 'copy',\n",
      " 'fromkeys',\n",
      " 'get',\n",
      " 'items',\n",
      " 'keys',\n",
      " 'name',\n",
      " 'pop',\n",
      " 'popitem',\n",
      " 'root',\n",
      " 'sample_rate',\n",
      " 'setdefault',\n",
      " 'train_part',\n",
      " 'update',\n",
      " 'values']"
     ]
    }
   ],
   "source": [
    "dir(config.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ceca1d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22050"
     ]
    }
   ],
   "source": [
    "config.dataset.get('sample_rate', 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e208d02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22050"
     ]
    }
   ],
   "source": [
    "config.dataset.get('sample_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fac93aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.dataset.__getitem__(0))\n",
    "# len(train_dataset.dataset.__getitem__(0)['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "89859591",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=train_dataset.dataset.__getitem__(0)\n",
    "print(a)\n",
    "# len(train_dataset.dataset.__getitem__(0)['text'])\n",
    "print(a['audio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e3628935",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=train_dataset.dataset.__getitem__(0)\n",
    "print(a)\n",
    "# len(train_dataset.dataset.__getitem__(0)['text'])\n",
    "print(a['audio'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "12dc66e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "train_dataset_2 = dataset_module.get_dataset(\n",
    "    config, part='train')\n",
    "b = train_dataset_2.dataset.__getitem__(0)\n",
    "# len(train_dataset_2.dataset.__getitem__(0)['text'])\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7fb237de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "train_dataset_2 = dataset_module.get_dataset(\n",
    "    config, part='train')\n",
    "b = train_dataset_2.dataset.__getitem__(0)\n",
    "# len(train_dataset_2.dataset.__getitem__(0)['text'])\n",
    "print(b)\n",
    "print(b['audio'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dc5fc33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "train_dataset_2 = dataset_module.get_dataset(\n",
    "    config, part='train')\n",
    "b = train_dataset_2.dataset.__getitem__(1)\n",
    "# len(train_dataset_2.dataset.__getitem__(0)['text'])\n",
    "print(b)\n",
    "print(b['audio'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e9e674c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "train_dataset_2 = dataset_module.get_dataset(\n",
    "    config, part='train')\n",
    "b = train_dataset_2.dataset.__getitem__(0)\n",
    "# len(train_dataset_2.dataset.__getitem__(0)['text'])\n",
    "print(b)\n",
    "print(b['audio'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "944eade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1['input_lengths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6c3e879e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 56,  12,  44,   6,  75,  65,  43,  93,  88,  48,  49, 103,  46,\n",
      "        62,  13,  99,  78,  13,  46,   5,  42,  47, 119,  56,  47,  11,\n",
      "        79,  53,  50,  16,  51,   9,  52,  73,  10,  17,  17,  51,  11,\n",
      "        57,  82,  19,  63,   8,  96,   4,  10,  17,   4,   9,  86,  57,\n",
      "        82,  19,  42,  98,  43,  42,  12,   6,  11,  72,  53,  12,   7,\n",
      "        17,   6,  11,  87,  21,  47,  11,  79,  52,  65,  43,  83,  27,\n",
      "        13,  10,  24,  62,  80])"
     ]
    }
   ],
   "source": [
    "batch1['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "faf8652f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 0.00064805, -0.00185332, -0.00244806, ..., -0.00170346,\n",
      "        0.00287292,  0.00232188], dtype=float32)"
     ]
    }
   ],
   "source": [
    "batch1['audio'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f694c009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212893"
     ]
    }
   ],
   "source": [
    "len(batch1['audio'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d561c850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212893"
     ]
    }
   ],
   "source": [
    "len(batch1['audio'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "89452445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64"
     ]
    }
   ],
   "source": [
    "len(batch1['audio'][0])\n",
    "len(batch2['audio'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d68dbbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32"
     ]
    }
   ],
   "source": [
    "len(batch1['audio'][0])\n",
    "len(batch2['audio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "96744a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64"
     ]
    }
   ],
   "source": [
    "len(batch1['audio'][0])\n",
    "len(batch2['audio'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a6ab0ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(batch1['audio'][0]))\n",
    "print(len(batch2['audio'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "10448cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(batch1['audio']))\n",
    "print(len(batch1['audio'][0]))\n",
    "print(len(batch2['audio'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "25d97341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.00064805, -0.00185332, -0.00244806, ..., -0.00170346,\n",
      "         0.00287292,  0.00232188], dtype=float32),\n",
      " array([-0.00314305,  0.00103363,  0.00466579, ...,  0.03743864,\n",
      "         0.04169904,  0.        ], dtype=float32),\n",
      " array([-2.7460288e-03, -4.2517125e-03, -6.8397014e-05, ...,\n",
      "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00], dtype=float32),\n",
      " array([ 1.6830985e-04, -2.7138460e-05, -2.2539313e-04, ...,\n",
      "         2.5602366e-04,  1.0118349e-03,  0.0000000e+00], dtype=float32),\n",
      " array([3.5219718e-04, 3.9384593e-04, 3.4842314e-04, ..., 1.4193331e-04,\n",
      "        1.3546018e-04, 8.1811209e-05], dtype=float32),\n",
      " array([ 7.7085420e-03, -2.8882974e-03, -2.8774177e-05, ...,\n",
      "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00], dtype=float32),\n",
      " array([ 3.8510363e-05,  2.8201991e-03,  5.8321138e-03, ...,\n",
      "         4.3147802e-03,  1.1419075e-02, -6.4695268e-03], dtype=float32),\n",
      " array([ 2.2583008e-03,  1.7700195e-03, -9.1552734e-05, ...,\n",
      "         1.8310547e-04, -1.8310547e-04, -3.3569336e-04], dtype=float32),\n",
      " array([-8.3596539e-04, -1.0172019e-03, -7.7972747e-04, ...,\n",
      "        -9.2914495e-05, -3.4379747e-04,  1.0163203e-04], dtype=float32),\n",
      " array([-0.00055806, -0.00051591, -0.00054907, ...,  0.        ,\n",
      "         0.        ,  0.        ], dtype=float32),\n",
      " array([-0.0029076 , -0.00215218, -0.00059656, ...,  0.        ,\n",
      "         0.        ,  0.        ], dtype=float32),\n",
      " array([-0.00168355, -0.00218567, -0.00184146, ...,  0.04594053,\n",
      "         0.05033867,  0.        ], dtype=float32),\n",
      " array([0.00054366, 0.00054058, 0.0007952 , ..., 0.        , 0.        ,\n",
      "        0.        ], dtype=float32),\n",
      " array([ 0.02272906,  0.0186081 , -0.00734079, ...,  0.05346785,\n",
      "         0.01619746, -0.01303736], dtype=float32),\n",
      " array([-0.00841909, -0.01477104, -0.00033289, ..., -0.01118742,\n",
      "        -0.00514902, -0.00349659], dtype=float32),\n",
      " array([-0.0003959 , -0.00292177,  0.00145343, ...,  0.01820027,\n",
      "         0.01412978,  0.01580742], dtype=float32),\n",
      " array([-0.00232852, -0.00317211, -0.00290751, ...,  0.        ,\n",
      "         0.        ,  0.        ], dtype=float32),\n",
      " array([-0.01470247, -0.00661179, -0.00250927, ...,  0.00378074,\n",
      "         0.00606666,  0.        ], dtype=float32),\n",
      " array([-0.00082397, -0.00231934, -0.00234985, ..., -0.00177002,\n",
      "        -0.00119019,  0.00064087], dtype=float32),\n",
      " array([ 0.00030518,  0.00027466,  0.00021362, ..., -0.00064087,\n",
      "        -0.00079346, -0.00082397], dtype=float32),\n",
      " array([ 0.00084496,  0.00249144,  0.00144098, ..., -0.00069851,\n",
      "        -0.00108514, -0.00084857], dtype=float32),\n",
      " array([-0.00063006,  0.00100754,  0.00508791, ..., -0.00277576,\n",
      "        -0.00245809,  0.00152176], dtype=float32),\n",
      " array([-0.00081956, -0.00292294, -0.00256488, ...,  0.        ,\n",
      "         0.        ,  0.        ], dtype=float32),\n",
      " array([-0.01048647,  0.0056416 , -0.00289302, ...,  0.        ,\n",
      "         0.        ,  0.        ], dtype=float32),\n",
      " array([ 0.00680624,  0.00101481,  0.01428293, ..., -0.00096096,\n",
      "        -0.00834265, -0.00914024], dtype=float32),\n",
      " array([-0.00057902, -0.00067707, -0.00095624, ..., -0.0004703 ,\n",
      "        -0.00043248, -0.00041621], dtype=float32),\n",
      " array([ 0.0185535 , -0.00629476,  0.00299063, ..., -0.00245228,\n",
      "        -0.00088249,  0.00733585], dtype=float32),\n",
      " array([-3.0517578e-05, -3.6621094e-04, -4.8828125e-04, ...,\n",
      "         9.1552734e-05,  5.4931641e-04,  3.0517578e-04], dtype=float32),\n",
      " array([ 0.0015405 , -0.00042733, -0.002726  , ...,  0.00274575,\n",
      "        -0.00090818,  0.        ], dtype=float32),\n",
      " array([-0.00360571, -0.00190096,  0.00153467, ...,  0.        ,\n",
      "         0.        ,  0.        ], dtype=float32),\n",
      " array([-0.00581988, -0.00602441,  0.00972501, ...,  0.        ,\n",
      "         0.        ,  0.        ], dtype=float32),\n",
      " array([ 0.00164117,  0.00101758, -0.0005462 , ...,  0.        ,\n",
      "         0.        ,  0.        ], dtype=float32)]"
     ]
    }
   ],
   "source": [
    "batch1['audio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "991daab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212893"
     ]
    }
   ],
   "source": [
    "len(batch1['audio'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d6c8049c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64"
     ]
    }
   ],
   "source": [
    "len(batch1['audio'][0])\n",
    "len(batch2['audio'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d05a2d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83"
     ]
    }
   ],
   "source": [
    "len(batch1['audio'][0])\n",
    "len(batch2['audio'][0])\n",
    "len(batch1['text'][0])\n",
    "# len(batch2['audio'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "016e40fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96"
     ]
    }
   ],
   "source": [
    "len(batch1['audio'][0])\n",
    "len(batch2['audio'][0])\n",
    "len(batch1['text'][0])\n",
    "len(batch2['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "caf9fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #! \n",
    "batch_transforms_val = Compose([\n",
    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    NormalizedMelSpectrogram(\n",
    "        sample_rate=config.dataset.get(\n",
    "            'sample_rate', 16000),  # for LJspeech\n",
    "        n_mels=config.model.feat_in,\n",
    "        normalize=config.dataset.get('normalize', None)\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    AddLengths(),\n",
    "    Pad()\n",
    "])\n",
    "\n",
    "for epoch_idx in tqdm(range(config.train.get('epochs', 10))):\n",
    "    # train:\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        batch1=batch\n",
    "        print(len(batch1['audio'][0]))\n",
    "        batch = batch_transforms_train(batch)\n",
    "        batch2=batch\n",
    "        print(len(batch2['audio'][0]))\n",
    "        break\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch['audio'])\n",
    "        output_length = torch.ceil(\n",
    "            batch['input_lengths'].float() / model.stride).int()\n",
    "        loss = criterion(logits.permute(2, 0, 1).log_softmax(\n",
    "            dim=2), batch['text'], output_length, batch['target_lengths'])\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), config.train.get('clip_grad_norm', 15))\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        # warmup_scheduler.dampen()\n",
    "\n",
    "        if batch_idx % config.wandb.get('log_interval', 5000) == 0:\n",
    "            target_strings = decoder.convert_to_strings(batch['text'])\n",
    "            decoded_output = decoder.decode(\n",
    "                logits.permute(0, 2, 1).softmax(dim=2))\n",
    "            wer = np.mean([decoder.wer(true, pred)\n",
    "                            for true, pred in zip(target_strings, decoded_output)])\n",
    "            cer = np.mean([decoder.cer(true, pred)\n",
    "                            for true, pred in zip(target_strings, decoded_output)])\n",
    "            step = epoch_idx * \\\n",
    "                len(train_dataloader) * train_dataloader.batch_size + \\\n",
    "                batch_idx * train_dataloader.batch_size\n",
    "            wandb.log({\n",
    "                \"train_loss\": loss.item(),\n",
    "                \"train_wer\": wer,\n",
    "                \"train_cer\": cer,\n",
    "                \"train_samples\": wandb.Table(\n",
    "                    columns=['gt_text', 'pred_text'],\n",
    "                    data=zip(target_strings, decoded_output)\n",
    "                )\n",
    "            }, step=step)\n",
    "    # #!\n",
    "    # # validate:\n",
    "    # model.eval()\n",
    "    # val_stats = defaultdict(list)\n",
    "    # for batch_idx, batch in enumerate(val_dataloader):\n",
    "    #     batch = batch_transforms_val(batch)\n",
    "    #     with torch.no_grad():\n",
    "    #         logits = model(batch['audio'])\n",
    "    #         output_length = torch.ceil(\n",
    "    #             batch['input_lengths'].float() / model.stride).int()\n",
    "    #         loss = criterion(logits.permute(2, 0, 1).log_softmax(\n",
    "    #             dim=2), batch['text'], output_length, batch['target_lengths'])\n",
    "\n",
    "    #     target_strings = decoder.convert_to_strings(batch['text'])\n",
    "    #     decoded_output = decoder.decode(\n",
    "    #         logits.permute(0, 2, 1).softmax(dim=2))\n",
    "    #     wer = np.mean([decoder.wer(true, pred)\n",
    "    #                     for true, pred in zip(target_strings, decoded_output)])\n",
    "    #     cer = np.mean([decoder.cer(true, pred)\n",
    "    #                     for true, pred in zip(target_strings, decoded_output)])\n",
    "    #     val_stats['val_loss'].append(loss.item())\n",
    "    #     val_stats['wer'].append(wer)\n",
    "    #     val_stats['cer'].append(cer)\n",
    "    # for k, v in val_stats.items():\n",
    "    #     val_stats[k] = np.mean(v)\n",
    "    # val_stats['val_samples'] = wandb.Table(\n",
    "    #     columns=['gt_text', 'pred_text'], data=zip(target_strings, decoded_output))\n",
    "    # wandb.log(val_stats, step=step)\n",
    "\n",
    "    # # save model, TODO: save optimizer:\n",
    "    # if val_stats['wer'] < prev_wer:\n",
    "    #     os.makedirs(config.train.get(\n",
    "    #         'checkpoint_path', 'checkpoints'), exist_ok=True)\n",
    "    #     prev_wer = val_stats['wer']\n",
    "    #     torch.save(\n",
    "    #         model.state_dict(),\n",
    "    #         os.path.join(config.train.get(\n",
    "    #             'checkpoint_path', 'checkpoints'), f'model_{epoch_idx}_{prev_wer}.pth')\n",
    "    #     )\n",
    "    #     wandb.save(os.path.join(config.train.get(\n",
    "    #         'checkpoint_path', 'checkpoints'), f'model_{epoch_idx}_{prev_wer}.pth'))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b4f85fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #! \n",
    "batch_transforms_val = Compose([\n",
    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    # NormalizedMelSpectrogram(\n",
    "    #     sample_rate=config.dataset.get(\n",
    "    #         'sample_rate', 16000),  # for LJspeech\n",
    "    #     n_mels=config.model.feat_in,\n",
    "    #     normalize=config.dataset.get('normalize', None)\n",
    "    # ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    AddLengths(),\n",
    "    Pad()\n",
    "])\n",
    "\n",
    "for epoch_idx in tqdm(range(config.train.get('epochs', 10))):\n",
    "    # train:\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        batch1=batch\n",
    "        print(len(batch1['audio'][0]))\n",
    "        batch = batch_transforms_train(batch)\n",
    "        batch2=batch\n",
    "        print(len(batch2['audio'][0]))\n",
    "        break\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch['audio'])\n",
    "        output_length = torch.ceil(\n",
    "            batch['input_lengths'].float() / model.stride).int()\n",
    "        loss = criterion(logits.permute(2, 0, 1).log_softmax(\n",
    "            dim=2), batch['text'], output_length, batch['target_lengths'])\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), config.train.get('clip_grad_norm', 15))\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        # warmup_scheduler.dampen()\n",
    "\n",
    "        if batch_idx % config.wandb.get('log_interval', 5000) == 0:\n",
    "            target_strings = decoder.convert_to_strings(batch['text'])\n",
    "            decoded_output = decoder.decode(\n",
    "                logits.permute(0, 2, 1).softmax(dim=2))\n",
    "            wer = np.mean([decoder.wer(true, pred)\n",
    "                            for true, pred in zip(target_strings, decoded_output)])\n",
    "            cer = np.mean([decoder.cer(true, pred)\n",
    "                            for true, pred in zip(target_strings, decoded_output)])\n",
    "            step = epoch_idx * \\\n",
    "                len(train_dataloader) * train_dataloader.batch_size + \\\n",
    "                batch_idx * train_dataloader.batch_size\n",
    "            wandb.log({\n",
    "                \"train_loss\": loss.item(),\n",
    "                \"train_wer\": wer,\n",
    "                \"train_cer\": cer,\n",
    "                \"train_samples\": wandb.Table(\n",
    "                    columns=['gt_text', 'pred_text'],\n",
    "                    data=zip(target_strings, decoded_output)\n",
    "                )\n",
    "            }, step=step)\n",
    "    # #!\n",
    "    # # validate:\n",
    "    # model.eval()\n",
    "    # val_stats = defaultdict(list)\n",
    "    # for batch_idx, batch in enumerate(val_dataloader):\n",
    "    #     batch = batch_transforms_val(batch)\n",
    "    #     with torch.no_grad():\n",
    "    #         logits = model(batch['audio'])\n",
    "    #         output_length = torch.ceil(\n",
    "    #             batch['input_lengths'].float() / model.stride).int()\n",
    "    #         loss = criterion(logits.permute(2, 0, 1).log_softmax(\n",
    "    #             dim=2), batch['text'], output_length, batch['target_lengths'])\n",
    "\n",
    "    #     target_strings = decoder.convert_to_strings(batch['text'])\n",
    "    #     decoded_output = decoder.decode(\n",
    "    #         logits.permute(0, 2, 1).softmax(dim=2))\n",
    "    #     wer = np.mean([decoder.wer(true, pred)\n",
    "    #                     for true, pred in zip(target_strings, decoded_output)])\n",
    "    #     cer = np.mean([decoder.cer(true, pred)\n",
    "    #                     for true, pred in zip(target_strings, decoded_output)])\n",
    "    #     val_stats['val_loss'].append(loss.item())\n",
    "    #     val_stats['wer'].append(wer)\n",
    "    #     val_stats['cer'].append(cer)\n",
    "    # for k, v in val_stats.items():\n",
    "    #     val_stats[k] = np.mean(v)\n",
    "    # val_stats['val_samples'] = wandb.Table(\n",
    "    #     columns=['gt_text', 'pred_text'], data=zip(target_strings, decoded_output))\n",
    "    # wandb.log(val_stats, step=step)\n",
    "\n",
    "    # # save model, TODO: save optimizer:\n",
    "    # if val_stats['wer'] < prev_wer:\n",
    "    #     os.makedirs(config.train.get(\n",
    "    #         'checkpoint_path', 'checkpoints'), exist_ok=True)\n",
    "    #     prev_wer = val_stats['wer']\n",
    "    #     torch.save(\n",
    "    #         model.state_dict(),\n",
    "    #         os.path.join(config.train.get(\n",
    "    #             'checkpoint_path', 'checkpoints'), f'model_{epoch_idx}_{prev_wer}.pth')\n",
    "    #     )\n",
    "    #     wandb.save(os.path.join(config.train.get(\n",
    "    #         'checkpoint_path', 'checkpoints'), f'model_{epoch_idx}_{prev_wer}.pth'))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5ed516b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #! \n",
    "batch_transforms_train = Compose([\n",
    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    # NormalizedMelSpectrogram(\n",
    "    #     sample_rate=config.dataset.get(\n",
    "    #         'sample_rate', 16000),  # for LJspeech\n",
    "    #     n_mels=config.model.feat_in,\n",
    "    #     normalize=config.dataset.get('normalize', None)\n",
    "    # ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    AddLengths(),\n",
    "    Pad()\n",
    "])\n",
    "\n",
    "for epoch_idx in tqdm(range(config.train.get('epochs', 10))):\n",
    "    # train:\n",
    "    model.train()\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        batch1=batch\n",
    "        print(len(batch1['audio'][0]))\n",
    "        batch = batch_transforms_train(batch)\n",
    "        batch2=batch\n",
    "        print(len(batch2['audio'][0]))\n",
    "        break\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch['audio'])\n",
    "        output_length = torch.ceil(\n",
    "            batch['input_lengths'].float() / model.stride).int()\n",
    "        loss = criterion(logits.permute(2, 0, 1).log_softmax(\n",
    "            dim=2), batch['text'], output_length, batch['target_lengths'])\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), config.train.get('clip_grad_norm', 15))\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        # warmup_scheduler.dampen()\n",
    "\n",
    "        if batch_idx % config.wandb.get('log_interval', 5000) == 0:\n",
    "            target_strings = decoder.convert_to_strings(batch['text'])\n",
    "            decoded_output = decoder.decode(\n",
    "                logits.permute(0, 2, 1).softmax(dim=2))\n",
    "            wer = np.mean([decoder.wer(true, pred)\n",
    "                            for true, pred in zip(target_strings, decoded_output)])\n",
    "            cer = np.mean([decoder.cer(true, pred)\n",
    "                            for true, pred in zip(target_strings, decoded_output)])\n",
    "            step = epoch_idx * \\\n",
    "                len(train_dataloader) * train_dataloader.batch_size + \\\n",
    "                batch_idx * train_dataloader.batch_size\n",
    "            wandb.log({\n",
    "                \"train_loss\": loss.item(),\n",
    "                \"train_wer\": wer,\n",
    "                \"train_cer\": cer,\n",
    "                \"train_samples\": wandb.Table(\n",
    "                    columns=['gt_text', 'pred_text'],\n",
    "                    data=zip(target_strings, decoded_output)\n",
    "                )\n",
    "            }, step=step)\n",
    "    # #!\n",
    "    # # validate:\n",
    "    # model.eval()\n",
    "    # val_stats = defaultdict(list)\n",
    "    # for batch_idx, batch in enumerate(val_dataloader):\n",
    "    #     batch = batch_transforms_val(batch)\n",
    "    #     with torch.no_grad():\n",
    "    #         logits = model(batch['audio'])\n",
    "    #         output_length = torch.ceil(\n",
    "    #             batch['input_lengths'].float() / model.stride).int()\n",
    "    #         loss = criterion(logits.permute(2, 0, 1).log_softmax(\n",
    "    #             dim=2), batch['text'], output_length, batch['target_lengths'])\n",
    "\n",
    "    #     target_strings = decoder.convert_to_strings(batch['text'])\n",
    "    #     decoded_output = decoder.decode(\n",
    "    #         logits.permute(0, 2, 1).softmax(dim=2))\n",
    "    #     wer = np.mean([decoder.wer(true, pred)\n",
    "    #                     for true, pred in zip(target_strings, decoded_output)])\n",
    "    #     cer = np.mean([decoder.cer(true, pred)\n",
    "    #                     for true, pred in zip(target_strings, decoded_output)])\n",
    "    #     val_stats['val_loss'].append(loss.item())\n",
    "    #     val_stats['wer'].append(wer)\n",
    "    #     val_stats['cer'].append(cer)\n",
    "    # for k, v in val_stats.items():\n",
    "    #     val_stats[k] = np.mean(v)\n",
    "    # val_stats['val_samples'] = wandb.Table(\n",
    "    #     columns=['gt_text', 'pred_text'], data=zip(target_strings, decoded_output))\n",
    "    # wandb.log(val_stats, step=step)\n",
    "\n",
    "    # # save model, TODO: save optimizer:\n",
    "    # if val_stats['wer'] < prev_wer:\n",
    "    #     os.makedirs(config.train.get(\n",
    "    #         'checkpoint_path', 'checkpoints'), exist_ok=True)\n",
    "    #     prev_wer = val_stats['wer']\n",
    "    #     torch.save(\n",
    "    #         model.state_dict(),\n",
    "    #         os.path.join(config.train.get(\n",
    "    #             'checkpoint_path', 'checkpoints'), f'model_{epoch_idx}_{prev_wer}.pth')\n",
    "    #     )\n",
    "    #     wandb.save(os.path.join(config.train.get(\n",
    "    #         'checkpoint_path', 'checkpoints'), f'model_{epoch_idx}_{prev_wer}.pth'))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fdd38d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(batch1['audio'])) # batchsize\n",
    "print(len(batch1['audio'][0]))\n",
    "print(len(batch2['audio'][0]))\n",
    "print(max([len(x) for x in batch1['audio'] ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4e4db623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import argparse\n",
    "import importlib\n",
    "\n",
    "# torchim:\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "# from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import pytorch_warmup as warmup\n",
    "\n",
    "# data:\n",
    "import data\n",
    "from data.collate import collate_fn, gpu_collate, no_pad_collate\n",
    "from data.transforms import (\n",
    "    Compose, AddLengths, AudioSqueeze, TextPreprocess,\n",
    "    MaskSpectrogram, ToNumpy, BPEtexts, MelSpectrogram,\n",
    "    ToGpu, Pad, NormalizedMelSpectrogram\n",
    ")\n",
    "import youtokentome as yttm\n",
    "\n",
    "import torchaudio\n",
    "from audiomentations import (\n",
    "    TimeStretch, PitchShift, AddGaussianNoise\n",
    ")\n",
    "from functools import partial\n",
    "\n",
    "# model:\n",
    "from model import configs as quartznet_configs\n",
    "from model.quartznet import QuartzNet\n",
    "\n",
    "# utils:\n",
    "import yaml\n",
    "from easydict import EasyDict as edict\n",
    "from utils import fix_seeds, remove_from_dict, prepare_bpe\n",
    "import wandb\n",
    "from decoder import GreedyDecoder, BeamCTCDecoder\n",
    "\n",
    "import youtokentome as yttm\n",
    "\n",
    "\n",
    "# TODO: wrap to trainer class\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Training model.')\n",
    "parser.add_argument('--config', default='configs/train_LJSpeech.yaml',\n",
    "                    help='path to config file')\n",
    "args = parser.parse_args(\"\")\n",
    "with open(args.config, 'r') as f:\n",
    "    config = edict(yaml.safe_load(f))\n",
    "\n",
    "def write_to_file(str_w, file_name = 'sth.txt', mode = 'w'):\n",
    "    with open(file_name,mode) as f:\n",
    "        f.write(str(str_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d5db4c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'root': 'DB/LJspeech',\n",
      "  'train_part': 0.95,\n",
      "  'name': 'ljspeech',\n",
      "  'sample_rate': 22050},\n",
      " 'bpe': {'train': True, 'model_path': 'yttm.bpe'},\n",
      " 'train': {'seed': 42,\n",
      "  'num_workers': 1,\n",
      "  'batch_size': 32,\n",
      "  'clip_grad_norm': 15,\n",
      "  'epochs': 42,\n",
      "  'optimizer': {'lr': 0.0005, 'weight_decay': 0.0001}},\n",
      " 'wandb': {'project': 'quartznet_ljspeech', 'log_interval': 20},\n",
      " 'model': {'name': '_quartznet5x5_config', 'vocab_size': 120, 'feat_in': 64}}"
     ]
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "789ae7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22050"
     ]
    }
   ],
   "source": [
    "config.dataset.get('sample_rate', 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "32352088",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seeds(seed=config.train.get('seed', 42))\n",
    "dataset_module = importlib.import_module(\n",
    "    f'.{config.dataset.name}', data.__name__)\n",
    "bpe = prepare_bpe(config)\n",
    "\n",
    "transforms_train = Compose([\n",
    "    TextPreprocess(),# removing punctuation in text - might not needed\n",
    "    ToNumpy(), # convert audio to numpy\n",
    "    BPEtexts(bpe=bpe, dropout_prob=config.bpe.get('dropout_prob', 0.05)),\n",
    "    AudioSqueeze(), # remove 1st dimension if it is 1 [1,...]\n",
    "    AddGaussianNoise(\n",
    "        min_amplitude=0.001,\n",
    "        max_amplitude=0.015,\n",
    "        p=0.5\n",
    "    ),\n",
    "    TimeStretch(\n",
    "        min_rate=0.8,\n",
    "        max_rate=1.25,\n",
    "        p=0.5\n",
    "    ),\n",
    "    PitchShift(\n",
    "        min_semitones=-4,\n",
    "        max_semitones=4,\n",
    "        p=0.5\n",
    "    )\n",
    "    # AddLengths()\n",
    "])\n",
    "\n",
    "batch_transforms_train = Compose([\n",
    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    NormalizedMelSpectrogram(\n",
    "        sample_rate=config.dataset.get('sample_rate', 16000),\n",
    "        n_mels=config.model.feat_in,\n",
    "        normalize=config.dataset.get('normalize', None)\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    MaskSpectrogram(\n",
    "        probability=0.5,\n",
    "        time_mask_max_percentage=0.05,\n",
    "        frequency_mask_max_percentage=0.15\n",
    "    ),\n",
    "    AddLengths(),\n",
    "    Pad()\n",
    "])\n",
    "\n",
    "transforms_val = Compose([\n",
    "    TextPreprocess(),\n",
    "    ToNumpy(),\n",
    "    BPEtexts(bpe=bpe),\n",
    "    AudioSqueeze()\n",
    "])\n",
    "\n",
    "batch_transforms_val = Compose([\n",
    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    NormalizedMelSpectrogram(\n",
    "        sample_rate=config.dataset.get(\n",
    "            'sample_rate', 16000),  # for LJspeech\n",
    "        n_mels=config.model.feat_in,\n",
    "        normalize=config.dataset.get('normalize', None)\n",
    "    ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    AddLengths(),\n",
    "    Pad()\n",
    "])\n",
    "\n",
    "# load datasets\n",
    "train_dataset = dataset_module.get_dataset(\n",
    "    config, transforms=transforms_train, part='train')\n",
    "val_dataset = dataset_module.get_dataset(\n",
    "    config, transforms=transforms_val, part='val')\n",
    "# print(\"!!!\", config.train.get('num_workers', 4))\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                                batch_size=config.train.get('batch_size', 1), collate_fn=no_pad_collate)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=1, collate_fn=no_pad_collate)\n",
    "\n",
    "print(getattr(quartznet_configs, config.model.name, '_quartznet5x5_config'))\n",
    "\n",
    "model = QuartzNet(\n",
    "    model_config=getattr(\n",
    "        quartznet_configs, config.model.name, '_quartznet5x5_config'),\n",
    "    **remove_from_dict(config.model, ['name'])\n",
    ")\n",
    "\n",
    "# print(model)\n",
    "write_to_file(model,'model_structure.txt')\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), **config.train.get('optimizer', {}))\n",
    "num_steps = len(train_dataloader) * config.train.get('epochs', 10)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=num_steps)\n",
    "# warmup_scheduler = warmup.UntunedLinearWarmup(optimizer)\n",
    "\n",
    "if config.train.get('from_checkpoint', None) is not None:\n",
    "    model.load_weights(config.train.from_checkpoint)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "# criterion = nn.CTCLoss(blank=config.model.vocab_size)\n",
    "decoder = GreedyDecoder(bpe=bpe)\n",
    "\n",
    "prev_wer = 1000\n",
    "wandb.init(project=config.wandb.project, config=config)\n",
    "wandb.watch(model, log=\"all\", log_freq=config.wandb.get(\n",
    "    'log_interval', 5000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
