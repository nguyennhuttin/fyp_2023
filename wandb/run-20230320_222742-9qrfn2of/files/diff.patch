diff --git a/configs/train_LJSpeech.yaml b/configs/train_LJSpeech.yaml
index 4c3926e..dcb7a37 100644
--- a/configs/train_LJSpeech.yaml
+++ b/configs/train_LJSpeech.yaml
@@ -8,7 +8,7 @@ bpe:
   model_path: yttm.bpe
 train:
   seed: 42
-  num_workers: 16
+  num_workers: 1
   batch_size: 32
   clip_grad_norm: 15
   epochs: 42
diff --git a/data/ljspeech.py b/data/ljspeech.py
index 5a88855..c8870f8 100644
--- a/data/ljspeech.py
+++ b/data/ljspeech.py
@@ -3,6 +3,7 @@ import torchaudio
 import os
 from torch.utils.data import Subset
 
+
 class LJSpeechDataset(torchaudio.datasets.LJSPEECH):
     def __init__(self, transforms, *args, **kwargs):
         if kwargs.get('download', False):
@@ -12,29 +13,34 @@ class LJSpeechDataset(torchaudio.datasets.LJSPEECH):
 
     def __getitem__(self, idx):
         audio, sample_rate, _, norm_text = super().__getitem__(idx)
-        return self.transforms({'audio' : audio, 'text': norm_text, 'sample_rate': sample_rate})
+        return self.transforms({'audio': audio, 'text': norm_text, 'sample_rate': sample_rate})
 
     def get_text(self, n):
-        line = self._walker[n]
+        line = self._flist[n]
         fileid, transcript, normalized_transcript = line
-        return self.transforms({'text' : normalized_transcript})['text']
-
+        return self.transforms({'text': normalized_transcript})['text']
 
 
 def get_dataset(config, transforms=lambda x: x, part='train'):
     if part == 'train':
-        dataset = LJSpeechDataset(root=config.dataset.root, download=True, transforms=transforms)
+        dataset = LJSpeechDataset(
+            root=config.dataset.root, download=True, transforms=transforms)
         indices = list(range(len(dataset)))
-        dataset = Subset(dataset, indices[:int(config.dataset.get('train_part', 0.95) * len(dataset))])
+        dataset = Subset(dataset, indices[:int(
+            config.dataset.get('train_part', 0.95) * len(dataset))])
         return dataset
     elif part == 'val':
-        dataset = LJSpeechDataset(root=config.dataset.root, download=True, transforms=transforms)
+        dataset = LJSpeechDataset(
+            root=config.dataset.root, download=True, transforms=transforms)
         indices = list(range(len(dataset)))
-        dataset = Subset(dataset, indices[int(config.dataset.get('train_part', 0.95) * len(dataset)):])
+        dataset = Subset(dataset, indices[int(
+            config.dataset.get('train_part', 0.95) * len(dataset)):])
         return dataset
     elif part == 'bpe':
-        dataset = LJSpeechDataset(root=config.dataset.root, download=True, transforms=transforms)
-        indices = list(range(len(dataset)))[:int(config.dataset.get('train_part', 0.95) * len(dataset))]
+        dataset = LJSpeechDataset(
+            root=config.dataset.root, download=True, transforms=transforms)
+        indices = list(range(len(dataset)))[:int(
+            config.dataset.get('train_part', 0.95) * len(dataset))]
         return dataset, indices
     else:
         raise ValueError('Unknown')
diff --git a/notebooks/quartznet_finetune_ljspeech.ipynb b/notebooks/quartznet_finetune_ljspeech.ipynb
index 7fd8118..c0ea15a 100644
--- a/notebooks/quartznet_finetune_ljspeech.ipynb
+++ b/notebooks/quartznet_finetune_ljspeech.ipynb
@@ -1,36 +1,19 @@
 {
-  "nbformat": 4,
-  "nbformat_minor": 0,
-  "metadata": {
-    "colab": {
-      "name": "quartznet_finetune_ljspeech.ipynb",
-      "provenance": [],
-      "collapsed_sections": [],
-      "toc_visible": true
-    },
-    "kernelspec": {
-      "name": "python3",
-      "display_name": "Python 3"
-    },
-    "accelerator": "GPU"
-  },
   "cells": [
     {
       "cell_type": "code",
+      "execution_count": 1,
       "metadata": {
-        "id": "YvnbraQcZRL6",
-        "outputId": "cd054a22-6f91-41b8-ab85-ae326aafbaf5",
         "colab": {
           "base_uri": "https://localhost:8080/",
           "height": 136
-        }
+        },
+        "id": "YvnbraQcZRL6",
+        "outputId": "cd054a22-6f91-41b8-ab85-ae326aafbaf5"
       },
-      "source": [
-        "!git clone https://<login>:<pass>@github.com/oleges1/quartznet-pytorch.git"
-      ],
-      "execution_count": 1,
       "outputs": [
         {
+          "name": "stdout",
           "output_type": "stream",
           "text": [
             "Cloning into 'quartznet-pytorch'...\n",
@@ -40,27 +23,27 @@
             "remote: Total 196 (delta 111), reused 136 (delta 64), pack-reused 0\u001b[K\n",
             "Receiving objects: 100% (196/196), 37.48 KiB | 511.00 KiB/s, done.\n",
             "Resolving deltas: 100% (111/111), done.\n"
-          ],
-          "name": "stdout"
+          ]
         }
+      ],
+      "source": [
+        "!git clone https://<login>:<pass>@github.com/oleges1/quartznet-pytorch.git"
       ]
     },
     {
       "cell_type": "code",
+      "execution_count": 2,
       "metadata": {
-        "id": "zJsi547FaY2C",
-        "outputId": "13ad5430-ccc9-4379-85bb-ca6573c11de3",
         "colab": {
           "base_uri": "https://localhost:8080/",
           "height": 1000
-        }
+        },
+        "id": "zJsi547FaY2C",
+        "outputId": "13ad5430-ccc9-4379-85bb-ca6573c11de3"
       },
-      "source": [
-        "!pip install -r quartznet-pytorch/requirments.txt"
-      ],
-      "execution_count": 2,
       "outputs": [
         {
+          "name": "stdout",
           "output_type": "stream",
           "text": [
             "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from -r quartznet-pytorch/requirments.txt (line 1)) (3.13)\n",
@@ -146,27 +129,27 @@
             "Successfully built python-Levenshtein watchdog subprocess32 pathtools\n",
             "Installing collected packages: youtokentome, pathtools, watchdog, sentry-sdk, smmap, gitdb, GitPython, docker-pycreds, shortuuid, subprocess32, configparser, wandb, torchaudio, python-Levenshtein, audiomentations, pytorch-warmup\n",
             "Successfully installed GitPython-3.1.9 audiomentations-0.12.1 configparser-5.0.1 docker-pycreds-0.4.0 gitdb-4.0.5 pathtools-0.1.2 python-Levenshtein-0.12.0 pytorch-warmup-0.0.4 sentry-sdk-0.19.0 shortuuid-1.0.1 smmap-3.0.4 subprocess32-3.5.4 torchaudio-0.6.0 wandb-0.10.5 watchdog-0.10.3 youtokentome-1.0.6\n"
-          ],
-          "name": "stdout"
+          ]
         }
+      ],
+      "source": [
+        "!pip install -r quartznet-pytorch/requirments.txt"
       ]
     },
     {
       "cell_type": "code",
+      "execution_count": 3,
       "metadata": {
-        "id": "p4I6cyozVt5a",
-        "outputId": "0596deef-a03c-431f-fc61-dd0b70f2f26d",
         "colab": {
           "base_uri": "https://localhost:8080/",
           "height": 357
-        }
+        },
+        "id": "p4I6cyozVt5a",
+        "outputId": "0596deef-a03c-431f-fc61-dd0b70f2f26d"
       },
-      "source": [
-        "!nvidia-smi"
-      ],
-      "execution_count": 3,
       "outputs": [
         {
+          "name": "stdout",
           "output_type": "stream",
           "text": [
             "Tue Oct 13 14:22:35 2020       \n",
@@ -189,28 +172,27 @@
             "|=============================================================================|\n",
             "|  No running processes found                                                 |\n",
             "+-----------------------------------------------------------------------------+\n"
-          ],
-          "name": "stdout"
+          ]
         }
+      ],
+      "source": [
+        "!nvidia-smi"
       ]
     },
     {
       "cell_type": "code",
+      "execution_count": 4,
       "metadata": {
-        "id": "fHa2U9sMsPqz",
-        "outputId": "b474c307-7761-44be-c230-10fa283e1314",
         "colab": {
           "base_uri": "https://localhost:8080/",
           "height": 530
-        }
+        },
+        "id": "fHa2U9sMsPqz",
+        "outputId": "b474c307-7761-44be-c230-10fa283e1314"
       },
-      "source": [
-        "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
-        "!cd ctcdecode && pip install ."
-      ],
-      "execution_count": 4,
       "outputs": [
         {
+          "name": "stdout",
           "output_type": "stream",
           "text": [
             "Cloning into 'ctcdecode'...\n",
@@ -242,99 +224,104 @@
             "Successfully built ctcdecode\n",
             "Installing collected packages: ctcdecode\n",
             "Successfully installed ctcdecode-1.0.2\n"
-          ],
-          "name": "stdout"
+          ]
         }
+      ],
+      "source": [
+        "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
+        "!cd ctcdecode && pip install ."
       ]
     },
     {
       "cell_type": "code",
+      "execution_count": 1,
       "metadata": {
         "id": "rb7XESIrcXyQ"
       },
+      "outputs": [],
       "source": [
         "!git config --global user.email \"ya.hef@yandex.ru\"\n",
         "!git config --global user.name \"oleges1\""
-      ],
-      "execution_count": 1,
-      "outputs": []
+      ]
     },
     {
       "cell_type": "code",
+      "execution_count": 2,
       "metadata": {
-        "id": "Ocsz5akwFRaI",
-        "outputId": "77a1c214-efe4-4da7-e957-81736daffdf1",
         "colab": {
           "base_uri": "https://localhost:8080/",
           "height": 34
-        }
+        },
+        "id": "Ocsz5akwFRaI",
+        "outputId": "77a1c214-efe4-4da7-e957-81736daffdf1"
       },
-      "source": [
-        "import torch\n",
-        "torch.cuda.is_available()"
-      ],
-      "execution_count": 2,
       "outputs": [
         {
-          "output_type": "execute_result",
           "data": {
             "text/plain": [
               "True"
             ]
           },
+          "execution_count": 2,
           "metadata": {
             "tags": []
           },
-          "execution_count": 2
+          "output_type": "execute_result"
         }
+      ],
+      "source": [
+        "import torch\n",
+        "torch.cuda.is_available()"
       ]
     },
     {
       "cell_type": "code",
+      "execution_count": 8,
       "metadata": {
         "id": "pUlHfKPPZiAg"
       },
+      "outputs": [],
       "source": [
-        "import sys"
-      ],
-      "execution_count": 3,
-      "outputs": []
+        "import sys\n",
+        "import os\n",
+        "os.chdir('C:/Users/nntin/uni/fyp/quartznet-pytorch')"
+      ]
     },
     {
       "cell_type": "code",
+      "execution_count": 9,
       "metadata": {
         "id": "CyrPTog-aC8W"
       },
+      "outputs": [],
       "source": [
         "sys.path.append('quartznet-pytorch')"
-      ],
-      "execution_count": 35,
-      "outputs": []
+      ]
     },
     {
       "cell_type": "code",
+      "execution_count": 10,
       "metadata": {
         "id": "9UyNGQSxbs0j"
       },
+      "outputs": [],
       "source": [
         "from train import train\n",
         "import yaml\n",
         "from easydict import EasyDict as edict"
-      ],
-      "execution_count": 5,
-      "outputs": []
+      ]
     },
     {
       "cell_type": "code",
+      "execution_count": 12,
       "metadata": {
         "id": "DRYQ3Ox3aIM1"
       },
+      "outputs": [],
       "source": [
         "with open(\"quartznet-pytorch/configs/finetune_LJSpeech.yaml\", 'r') as stream:\n",
         "    config = edict(yaml.safe_load(stream))"
-      ],
-      "execution_count": 12,
-      "outputs": []
+      ]
     },
     {
       "cell_type": "markdown",
@@ -347,20 +334,18 @@
     },
     {
       "cell_type": "code",
+      "execution_count": 13,
       "metadata": {
-        "id": "VMG-kRvChDOW",
-        "outputId": "5403a216-ac13-4417-eed0-aedaadbcaaaa",
         "colab": {
           "base_uri": "https://localhost:8080/",
           "height": 1000
-        }
+        },
+        "id": "VMG-kRvChDOW",
+        "outputId": "5403a216-ac13-4417-eed0-aedaadbcaaaa"
       },
-      "source": [
-        "train(config)"
-      ],
-      "execution_count": 13,
       "outputs": [
         {
+          "name": "stdout",
           "output_type": "stream",
           "text": [
             "QuartzNet(\n",
@@ -705,60 +690,28 @@
             ")\n",
             "<All keys matched successfully>\n",
             "['<PAD>', '<UNK>', '<BOS>', '<EOS>', '▁', 'e', 't', 'a', 'o', 'n', 'i', 's', 'r', 'h', 'd', 'l', 'c', 'f', 'u', 'm', 'w', 'p', 'g', 'y', 'b', 'v', 'k', 'x', 'q', 'j', 'z', 'ü', '”', '“', 'é', '’', 'ê', 'è', 'â', 'à', '▁t', 'he', '▁a', '▁the', 'in', '▁o', '▁w', 're', '▁s', 'en', 'on', 'er', 'ed', '▁c', 'at', '▁b', '▁p', '▁f', '▁of', 'nd', '▁h', 'is', 'it', '▁m', 'or', '▁in', 'as', '▁to', 'al', 'ar', 'ou', 'es', '▁and', '▁d', '▁th', 'ing', 'an', '▁n', 'ic', 'ent', 'ion', '▁l', 'ro', '▁e', '▁was', '▁he', 'ot', '▁re', 'ly', 'le', '▁be', 'ad', 'id', '▁on', 'im', 've', 'st', 'om', 'll', '▁wh', '▁that', '▁g', 'ct', 'se', 'ut', 'et', '▁for', 'gh', 'ce', '▁as', '▁u', '▁st', 'ir', 'ld', 'ere', '▁his', 'ri', 'ur', 'qu', '▁at']\n"
-          ],
-          "name": "stdout"
+          ]
         },
         {
-          "output_type": "display_data",
           "data": {
-            "application/javascript": [
-              "\n",
-              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
-              "            function loadScript(url) {\n",
-              "            return new Promise(function(resolve, reject) {\n",
-              "                let newScript = document.createElement(\"script\");\n",
-              "                newScript.onerror = reject;\n",
-              "                newScript.onload = resolve;\n",
-              "                document.body.appendChild(newScript);\n",
-              "                newScript.src = url;\n",
-              "            });\n",
-              "            }\n",
-              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
-              "            const iframe = document.createElement('iframe')\n",
-              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
-              "            document.body.appendChild(iframe)\n",
-              "            const handshake = new Postmate({\n",
-              "                container: iframe,\n",
-              "                url: 'https://wandb.ai/authorize'\n",
-              "            });\n",
-              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
-              "            handshake.then(function(child) {\n",
-              "                child.on('authorize', data => {\n",
-              "                    clearTimeout(timeout)\n",
-              "                    resolve(data)\n",
-              "                });\n",
-              "            });\n",
-              "            })\n",
-              "        });\n",
-              "    "
-            ],
+            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
             "text/plain": [
               "<IPython.core.display.Javascript object>"
             ]
           },
           "metadata": {
             "tags": []
-          }
+          },
+          "output_type": "display_data"
         },
         {
+          "name": "stderr",
           "output_type": "stream",
           "text": [
             "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
-          ],
-          "name": "stderr"
+          ]
         },
         {
-          "output_type": "display_data",
           "data": {
             "text/html": [
               "\n",
@@ -775,22 +728,28 @@
           },
           "metadata": {
             "tags": []
-          }
+          },
+          "output_type": "display_data"
         },
         {
+          "name": "stderr",
           "output_type": "stream",
           "text": [
             "100%|██████████| 20/20 [10:46:31<00:00, 1939.56s/it]\n"
-          ],
-          "name": "stderr"
+          ]
         }
+      ],
+      "source": [
+        "train(config)"
       ]
     },
     {
       "cell_type": "code",
+      "execution_count": 17,
       "metadata": {
         "id": "qYKUCRNYvylK"
       },
+      "outputs": [],
       "source": [
         "import os\n",
         "import numpy as np\n",
@@ -798,95 +757,91 @@
         "checkpoint_files = [item for item in os.listdir('checkpoints') if item.endswith('pth')]\n",
         "epochs = [int(file.split('_')[1]) for file in checkpoint_files]\n",
         "checkpoint_file = checkpoint_files[int(np.argmax(epochs))]"
-      ],
-      "execution_count": 17,
-      "outputs": []
+      ]
     },
     {
       "cell_type": "code",
+      "execution_count": 46,
       "metadata": {
         "id": "00XGQzpSdDT-"
       },
+      "outputs": [],
       "source": [
         "config.model.name = '_quartznet5x5_config'\n",
         "config.train.from_checkpoint = f'checkpoints/{checkpoint_file}'"
-      ],
-      "execution_count": 46,
-      "outputs": []
+      ]
     },
     {
       "cell_type": "code",
+      "execution_count": 21,
       "metadata": {
-        "id": "VjWpld0-x-pL",
-        "outputId": "10021aa1-a612-4fc0-9aa8-00e3e71116a7",
         "colab": {
           "base_uri": "https://localhost:8080/",
           "height": 34
-        }
+        },
+        "id": "VjWpld0-x-pL",
+        "outputId": "10021aa1-a612-4fc0-9aa8-00e3e71116a7"
       },
-      "source": [
-        "os.system(f'cp checkpoints/{checkpoint_file} .')"
-      ],
-      "execution_count": 21,
       "outputs": [
         {
-          "output_type": "execute_result",
           "data": {
             "text/plain": [
               "0"
             ]
           },
+          "execution_count": 21,
           "metadata": {
             "tags": []
           },
-          "execution_count": 21
+          "output_type": "execute_result"
         }
+      ],
+      "source": [
+        "os.system(f'cp checkpoints/{checkpoint_file} .')"
       ]
     },
     {
       "cell_type": "code",
+      "execution_count": 44,
       "metadata": {
-        "id": "keosR9WldPYf",
-        "outputId": "8ac8530b-e1df-4f4d-e46a-95699af2d6cb",
         "colab": {
           "base_uri": "https://localhost:8080/",
           "height": 51
-        }
+        },
+        "id": "keosR9WldPYf",
+        "outputId": "8ac8530b-e1df-4f4d-e46a-95699af2d6cb"
       },
-      "source": [
-        "%load_ext autoreload\n",
-        "%autoreload 2\n",
-        "\n",
-        "from evaluate import evaluate"
-      ],
-      "execution_count": 44,
       "outputs": [
         {
+          "name": "stdout",
           "output_type": "stream",
           "text": [
             "The autoreload extension is already loaded. To reload it, use:\n",
             "  %reload_ext autoreload\n"
-          ],
-          "name": "stdout"
+          ]
         }
+      ],
+      "source": [
+        "%load_ext autoreload\n",
+        "%autoreload 2\n",
+        "\n",
+        "from evaluate import evaluate"
       ]
     },
     {
       "cell_type": "code",
+      "execution_count": 47,
       "metadata": {
-        "id": "MVoihN-qdXyK",
-        "outputId": "6eec3233-50b1-4177-ba43-5a6d7924ddd8",
         "colab": {
           "base_uri": "https://localhost:8080/",
           "height": 1000
-        }
+        },
+        "id": "MVoihN-qdXyK",
+        "outputId": "6eec3233-50b1-4177-ba43-5a6d7924ddd8"
       },
-      "source": [
-        "evaluate(config)"
-      ],
-      "execution_count": 47,
       "outputs": [
         {
+          "name": "stdout",
           "output_type": "stream",
           "text": [
             "QuartzNet(\n",
@@ -1232,27 +1187,27 @@
             "<All keys matched successfully>\n",
             "['<PAD>', '<UNK>', '<BOS>', '<EOS>', '▁', 'e', 't', 'a', 'o', 'n', 'i', 's', 'r', 'h', 'd', 'l', 'c', 'f', 'u', 'm', 'w', 'p', 'g', 'y', 'b', 'v', 'k', 'x', 'q', 'j', 'z', 'ü', '”', '“', 'é', '’', 'ê', 'è', 'â', 'à', '▁t', 'he', '▁a', '▁the', 'in', '▁o', '▁w', 're', '▁s', 'en', 'on', 'er', 'ed', '▁c', 'at', '▁b', '▁p', '▁f', '▁of', 'nd', '▁h', 'is', 'it', '▁m', 'or', '▁in', 'as', '▁to', 'al', 'ar', 'ou', 'es', '▁and', '▁d', '▁th', 'ing', 'an', '▁n', 'ic', 'ent', 'ion', '▁l', 'ro', '▁e', '▁was', '▁he', 'ot', '▁re', 'ly', 'le', '▁be', 'ad', 'id', '▁on', 'im', 've', 'st', 'om', 'll', '▁wh', '▁that', '▁g', 'ct', 'se', 'ut', 'et', '▁for', 'gh', 'ce', '▁as', '▁u', '▁st', 'ir', 'ld', 'ere', '▁his', 'ri', 'ur', 'qu', '▁at']\n",
             "defaultdict(<class 'list'>, {'val_loss': 0.5264071103185415, 'wer': 0.27192559258643667, 'cer': 0.07488137021352631, 'val_samples': <wandb.data_types.Table object at 0x7f4732345668>})\n"
-          ],
-          "name": "stdout"
+          ]
         }
+      ],
+      "source": [
+        "evaluate(config)"
       ]
     },
     {
       "cell_type": "code",
+      "execution_count": null,
       "metadata": {
-        "id": "yQ0TRgtfhgAF",
-        "outputId": "acd9c6e3-83bd-48c0-cf22-b2b8a01b3623",
         "colab": {
           "base_uri": "https://localhost:8080/",
           "height": 221
-        }
+        },
+        "id": "yQ0TRgtfhgAF",
+        "outputId": "acd9c6e3-83bd-48c0-cf22-b2b8a01b3623"
       },
-      "source": [
-        "!cd quartznet-pytorch && git pull origin main"
-      ],
-      "execution_count": null,
       "outputs": [
         {
+          "name": "stdout",
           "output_type": "stream",
           "text": [
             "remote: Enumerating objects: 5, done.\u001b[K\n",
@@ -1267,64 +1222,64 @@
             "Fast-forward\n",
             " README.md | 1 \u001b[32m+\u001b[m\n",
             " 1 file changed, 1 insertion(+)\n"
-          ],
-          "name": "stdout"
+          ]
         }
+      ],
+      "source": [
+        "!cd quartznet-pytorch && git pull origin main"
       ]
     },
     {
       "cell_type": "code",
+      "execution_count": 48,
       "metadata": {
         "id": "NzxMfO8uck3s"
       },
+      "outputs": [],
       "source": [
         "!cd quartznet-pytorch && git add ."
-      ],
-      "execution_count": 48,
-      "outputs": []
+      ]
     },
     {
       "cell_type": "code",
+      "execution_count": 49,
       "metadata": {
-        "id": "JEjKZ9sEmVly",
-        "outputId": "f40e7d67-c521-4c0c-da76-64485c8533be",
         "colab": {
           "base_uri": "https://localhost:8080/",
           "height": 68
-        }
+        },
+        "id": "JEjKZ9sEmVly",
+        "outputId": "f40e7d67-c521-4c0c-da76-64485c8533be"
       },
-      "source": [
-        "!cd quartznet-pytorch && git commit -m \"eval -> evaluate, bugfixes\""
-      ],
-      "execution_count": 49,
       "outputs": [
         {
+          "name": "stdout",
           "output_type": "stream",
           "text": [
             "[main 8b2748a] eval -> evaluate, bugfixes\n",
             " 1 file changed, 7 insertions(+)\n",
             " rename eval.py => evaluate.py (94%)\n"
-          ],
-          "name": "stdout"
+          ]
         }
+      ],
+      "source": [
+        "!cd quartznet-pytorch && git commit -m \"eval -> evaluate, bugfixes\""
       ]
     },
     {
       "cell_type": "code",
+      "execution_count": 50,
       "metadata": {
-        "id": "KfisxXPQcF-u",
-        "outputId": "4905a7f3-0c21-49fb-e90f-703bebfc85c3",
         "colab": {
           "base_uri": "https://localhost:8080/",
           "height": 153
-        }
+        },
+        "id": "KfisxXPQcF-u",
+        "outputId": "4905a7f3-0c21-49fb-e90f-703bebfc85c3"
       },
-      "source": [
-        "!cd quartznet-pytorch && git push origin main"
-      ],
-      "execution_count": 50,
       "outputs": [
         {
+          "name": "stdout",
           "output_type": "stream",
           "text": [
             "Counting objects: 3, done.\n",
@@ -1335,27 +1290,27 @@
             "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
             "To https://github.com/oleges1/quartznet-pytorch.git\n",
             "   4ab1d12..8b2748a  main -> main\n"
-          ],
-          "name": "stdout"
+          ]
         }
+      ],
+      "source": [
+        "!cd quartznet-pytorch && git push origin main"
       ]
     },
     {
       "cell_type": "code",
+      "execution_count": 51,
       "metadata": {
-        "id": "5Nz0Tj_qU7sq",
-        "outputId": "d83e629a-6d34-4795-9ded-79af89d3a127",
         "colab": {
           "base_uri": "https://localhost:8080/",
           "height": 187
-        }
+        },
+        "id": "5Nz0Tj_qU7sq",
+        "outputId": "d83e629a-6d34-4795-9ded-79af89d3a127"
       },
-      "source": [
-        "!ls -a checkpoints"
-      ],
-      "execution_count": 51,
       "outputs": [
         {
+          "name": "stdout",
           "output_type": "stream",
           "text": [
             ".\t\t\t\t  model_17_0.2841321284154134.pth\n",
@@ -1368,46 +1323,73 @@
             "model_13_0.2954691471587973.pth   model_8_0.34120389584236915.pth\n",
             "model_14_0.29511776741055046.pth  model_9_0.3318550738096163.pth\n",
             "model_15_0.28474790346010614.pth\n"
-          ],
-          "name": "stdout"
+          ]
         }
+      ],
+      "source": [
+        "!ls -a checkpoints"
       ]
     },
     {
       "cell_type": "code",
+      "execution_count": null,
       "metadata": {
-        "id": "v4YDiWNX1dyW",
-        "outputId": "4826a7e0-62c2-4155-8c97-f0fb7c16d406",
         "colab": {
           "base_uri": "https://localhost:8080/",
           "height": 34
-        }
+        },
+        "id": "v4YDiWNX1dyW",
+        "outputId": "4826a7e0-62c2-4155-8c97-f0fb7c16d406"
       },
-      "source": [
-        "! rm -r wandb \n",
-        "! rm -r checkpoints"
-      ],
-      "execution_count": null,
       "outputs": [
         {
+          "name": "stdout",
           "output_type": "stream",
           "text": [
             "rm: cannot remove 'checkpoints': No such file or directory\n"
-          ],
-          "name": "stdout"
+          ]
         }
+      ],
+      "source": [
+        "! rm -r wandb \n",
+        "! rm -r checkpoints"
       ]
     },
     {
       "cell_type": "code",
+      "execution_count": null,
       "metadata": {
         "id": "QOiBhdjFVEiz"
       },
-      "source": [
-        ""
-      ],
-      "execution_count": null,
-      "outputs": []
+      "outputs": [],
+      "source": []
     }
-  ]
-}
\ No newline at end of file
+  ],
+  "metadata": {
+    "accelerator": "GPU",
+    "colab": {
+      "collapsed_sections": [],
+      "name": "quartznet_finetune_ljspeech.ipynb",
+      "provenance": [],
+      "toc_visible": true
+    },
+    "kernelspec": {
+      "display_name": "Python 3",
+      "name": "python3"
+    },
+    "language_info": {
+      "codemirror_mode": {
+        "name": "ipython",
+        "version": 3
+      },
+      "file_extension": ".py",
+      "mimetype": "text/x-python",
+      "name": "python",
+      "nbconvert_exporter": "python",
+      "pygments_lexer": "ipython3",
+      "version": "3.9.15"
+    }
+  },
+  "nbformat": 4,
+  "nbformat_minor": 0
+}
diff --git a/requirments.txt b/requirments.txt
deleted file mode 100644
index 7a315ea..0000000
--- a/requirments.txt
+++ /dev/null
@@ -1,9 +0,0 @@
-pyyaml
-easydict
-youtokentome
-wandb
-torchaudio
-librosa
-python-Levenshtein
-audiomentations
-pytorch_warmup
\ No newline at end of file
diff --git a/train.py b/train.py
index dc5b5f8..7a61d4b 100644
--- a/train.py
+++ b/train.py
@@ -22,9 +22,9 @@ import pytorch_warmup as warmup
 import data
 from data.collate import collate_fn, gpu_collate, no_pad_collate
 from data.transforms import (
-        Compose, AddLengths, AudioSqueeze, TextPreprocess,
-        MaskSpectrogram, ToNumpy, BPEtexts, MelSpectrogram,
-        ToGpu, Pad, NormalizedMelSpectrogram
+    Compose, AddLengths, AudioSqueeze, TextPreprocess,
+    MaskSpectrogram, ToNumpy, BPEtexts, MelSpectrogram,
+    ToGpu, Pad, NormalizedMelSpectrogram
 )
 import youtokentome as yttm
 
@@ -46,88 +46,96 @@ import wandb
 from decoder import GreedyDecoder, BeamCTCDecoder
 
 # TODO: wrap to trainer class
+
+
 def train(config):
     fix_seeds(seed=config.train.get('seed', 42))
-    dataset_module = importlib.import_module(f'.{config.dataset.name}', data.__name__)
+    dataset_module = importlib.import_module(
+        f'.{config.dataset.name}', data.__name__)
     bpe = prepare_bpe(config)
 
     transforms_train = Compose([
-            TextPreprocess(),
-            ToNumpy(),
-            BPEtexts(bpe=bpe, dropout_prob=config.bpe.get('dropout_prob', 0.05)),
-            AudioSqueeze(),
-            AddGaussianNoise(
-                min_amplitude=0.001,
-                max_amplitude=0.015,
-                p=0.5
-            ),
-            TimeStretch(
-                min_rate=0.8,
-                max_rate=1.25,
-                p=0.5
-            ),
-            PitchShift(
-                min_semitones=-4,
-                max_semitones=4,
-                p=0.5
-            )
-            # AddLengths()
+        TextPreprocess(),
+        ToNumpy(),
+        BPEtexts(bpe=bpe, dropout_prob=config.bpe.get('dropout_prob', 0.05)),
+        AudioSqueeze(),
+        AddGaussianNoise(
+            min_amplitude=0.001,
+            max_amplitude=0.015,
+            p=0.5
+        ),
+        TimeStretch(
+            min_rate=0.8,
+            max_rate=1.25,
+            p=0.5
+        ),
+        PitchShift(
+            min_semitones=-4,
+            max_semitones=4,
+            p=0.5
+        )
+        # AddLengths()
     ])
 
     batch_transforms_train = Compose([
-            ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),
-            NormalizedMelSpectrogram(
-                sample_rate=config.dataset.get('sample_rate', 16000),
-                n_mels=config.model.feat_in,
-                normalize=config.dataset.get('normalize', None)
-            ).to('cuda' if torch.cuda.is_available() else 'cpu'),
-            MaskSpectrogram(
-                probability=0.5,
-                time_mask_max_percentage=0.05,
-                frequency_mask_max_percentage=0.15
-            ),
-            AddLengths(),
-            Pad()
+        ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),
+        NormalizedMelSpectrogram(
+            sample_rate=config.dataset.get('sample_rate', 16000),
+            n_mels=config.model.feat_in,
+            normalize=config.dataset.get('normalize', None)
+        ).to('cuda' if torch.cuda.is_available() else 'cpu'),
+        MaskSpectrogram(
+            probability=0.5,
+            time_mask_max_percentage=0.05,
+            frequency_mask_max_percentage=0.15
+        ),
+        AddLengths(),
+        Pad()
     ])
 
     transforms_val = Compose([
-            TextPreprocess(),
-            ToNumpy(),
-            BPEtexts(bpe=bpe),
-            AudioSqueeze()
+        TextPreprocess(),
+        ToNumpy(),
+        BPEtexts(bpe=bpe),
+        AudioSqueeze()
     ])
 
     batch_transforms_val = Compose([
-            ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),
-            NormalizedMelSpectrogram(
-                sample_rate=config.dataset.get('sample_rate', 16000), # for LJspeech
-                n_mels=config.model.feat_in,
-                normalize=config.dataset.get('normalize', None)
-            ).to('cuda' if torch.cuda.is_available() else 'cpu'),
-            AddLengths(),
-            Pad()
+        ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),
+        NormalizedMelSpectrogram(
+            sample_rate=config.dataset.get(
+                'sample_rate', 16000),  # for LJspeech
+            n_mels=config.model.feat_in,
+            normalize=config.dataset.get('normalize', None)
+        ).to('cuda' if torch.cuda.is_available() else 'cpu'),
+        AddLengths(),
+        Pad()
     ])
 
     # load datasets
-    train_dataset = dataset_module.get_dataset(config, transforms=transforms_train, part='train')
-    val_dataset = dataset_module.get_dataset(config, transforms=transforms_val, part='val')
-
-
+    train_dataset = dataset_module.get_dataset(
+        config, transforms=transforms_train, part='train')
+    val_dataset = dataset_module.get_dataset(
+        config, transforms=transforms_val, part='val')
+    print("!!!", config.train.get('num_workers', 4))
     train_dataloader = DataLoader(train_dataset, num_workers=config.train.get('num_workers', 4),
-                batch_size=config.train.get('batch_size', 1), collate_fn=no_pad_collate)
+                                  batch_size=config.train.get('batch_size', 1), collate_fn=no_pad_collate)
 
     val_dataloader = DataLoader(val_dataset, num_workers=config.train.get('num_workers', 4),
-                batch_size=1, collate_fn=no_pad_collate)
+                                batch_size=1, collate_fn=no_pad_collate)
 
     model = QuartzNet(
-        model_config=getattr(quartznet_configs, config.model.name, '_quartznet5x5_config'),
+        model_config=getattr(
+            quartznet_configs, config.model.name, '_quartznet5x5_config'),
         **remove_from_dict(config.model, ['name'])
     )
 
     print(model)
-    optimizer = torch.optim.Adam(model.parameters(), **config.train.get('optimizer', {}))
+    optimizer = torch.optim.Adam(
+        model.parameters(), **config.train.get('optimizer', {}))
     num_steps = len(train_dataloader) * config.train.get('epochs', 10)
-    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_steps)
+    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
+        optimizer, T_max=num_steps)
     # warmup_scheduler = warmup.UntunedLinearWarmup(optimizer)
 
     if config.train.get('from_checkpoint', None) is not None:
@@ -142,28 +150,41 @@ def train(config):
 
     prev_wer = 1000
     wandb.init(project=config.wandb.project, config=config)
-    wandb.watch(model, log="all", log_freq=config.wandb.get('log_interval', 5000))
+    wandb.watch(model, log="all", log_freq=config.wandb.get(
+        'log_interval', 5000))
     for epoch_idx in tqdm(range(config.train.get('epochs', 10))):
         # train:
         model.train()
         for batch_idx, batch in enumerate(train_dataloader):
+            print(batch)
             batch = batch_transforms_train(batch)
+            print(batch)
+            return
+
             optimizer.zero_grad()
             logits = model(batch['audio'])
-            output_length = torch.ceil(batch['input_lengths'].float() / model.stride).int()
-            loss = criterion(logits.permute(2, 0, 1).log_softmax(dim=2), batch['text'], output_length, batch['target_lengths'])
+            output_length = torch.ceil(
+                batch['input_lengths'].float() / model.stride).int()
+            loss = criterion(logits.permute(2, 0, 1).log_softmax(
+                dim=2), batch['text'], output_length, batch['target_lengths'])
             loss.backward()
-            torch.nn.utils.clip_grad_norm_(model.parameters(), config.train.get('clip_grad_norm', 15))
+            torch.nn.utils.clip_grad_norm_(
+                model.parameters(), config.train.get('clip_grad_norm', 15))
             optimizer.step()
             lr_scheduler.step()
             # warmup_scheduler.dampen()
 
             if batch_idx % config.wandb.get('log_interval', 5000) == 0:
                 target_strings = decoder.convert_to_strings(batch['text'])
-                decoded_output = decoder.decode(logits.permute(0, 2, 1).softmax(dim=2))
-                wer = np.mean([decoder.wer(true, pred) for true, pred in zip(target_strings, decoded_output)])
-                cer = np.mean([decoder.cer(true, pred) for true, pred in zip(target_strings, decoded_output)])
-                step = epoch_idx * len(train_dataloader) * train_dataloader.batch_size + batch_idx * train_dataloader.batch_size
+                decoded_output = decoder.decode(
+                    logits.permute(0, 2, 1).softmax(dim=2))
+                wer = np.mean([decoder.wer(true, pred)
+                              for true, pred in zip(target_strings, decoded_output)])
+                cer = np.mean([decoder.cer(true, pred)
+                              for true, pred in zip(target_strings, decoded_output)])
+                step = epoch_idx * \
+                    len(train_dataloader) * train_dataloader.batch_size + \
+                    batch_idx * train_dataloader.batch_size
                 wandb.log({
                     "train_loss": loss.item(),
                     "train_wer": wer,
@@ -181,36 +202,44 @@ def train(config):
             batch = batch_transforms_val(batch)
             with torch.no_grad():
                 logits = model(batch['audio'])
-                output_length = torch.ceil(batch['input_lengths'].float() / model.stride).int()
-                loss = criterion(logits.permute(2, 0, 1).log_softmax(dim=2), batch['text'], output_length, batch['target_lengths'])
+                output_length = torch.ceil(
+                    batch['input_lengths'].float() / model.stride).int()
+                loss = criterion(logits.permute(2, 0, 1).log_softmax(
+                    dim=2), batch['text'], output_length, batch['target_lengths'])
 
             target_strings = decoder.convert_to_strings(batch['text'])
-            decoded_output = decoder.decode(logits.permute(0, 2, 1).softmax(dim=2))
-            wer = np.mean([decoder.wer(true, pred) for true, pred in zip(target_strings, decoded_output)])
-            cer = np.mean([decoder.cer(true, pred) for true, pred in zip(target_strings, decoded_output)])
+            decoded_output = decoder.decode(
+                logits.permute(0, 2, 1).softmax(dim=2))
+            wer = np.mean([decoder.wer(true, pred)
+                          for true, pred in zip(target_strings, decoded_output)])
+            cer = np.mean([decoder.cer(true, pred)
+                          for true, pred in zip(target_strings, decoded_output)])
             val_stats['val_loss'].append(loss.item())
             val_stats['wer'].append(wer)
             val_stats['cer'].append(cer)
         for k, v in val_stats.items():
             val_stats[k] = np.mean(v)
-        val_stats['val_samples'] = wandb.Table(columns=['gt_text', 'pred_text'], data=zip(target_strings, decoded_output))
+        val_stats['val_samples'] = wandb.Table(
+            columns=['gt_text', 'pred_text'], data=zip(target_strings, decoded_output))
         wandb.log(val_stats, step=step)
 
         # save model, TODO: save optimizer:
         if val_stats['wer'] < prev_wer:
-            os.makedirs(config.train.get('checkpoint_path', 'checkpoints'), exist_ok=True)
+            os.makedirs(config.train.get(
+                'checkpoint_path', 'checkpoints'), exist_ok=True)
             prev_wer = val_stats['wer']
             torch.save(
                 model.state_dict(),
-                os.path.join(config.train.get('checkpoint_path', 'checkpoints'), f'model_{epoch_idx}_{prev_wer}.pth')
+                os.path.join(config.train.get(
+                    'checkpoint_path', 'checkpoints'), f'model_{epoch_idx}_{prev_wer}.pth')
             )
-            wandb.save(os.path.join(config.train.get('checkpoint_path', 'checkpoints'), f'model_{epoch_idx}_{prev_wer}.pth'))
-
+            wandb.save(os.path.join(config.train.get(
+                'checkpoint_path', 'checkpoints'), f'model_{epoch_idx}_{prev_wer}.pth'))
 
 
 if __name__ == '__main__':
     parser = argparse.ArgumentParser(description='Training model.')
-    parser.add_argument('--config', default='configs/train_LJSpeech.yml',
+    parser.add_argument('--config', default='configs/train_LJSpeech.yaml',
                         help='path to config file')
     args = parser.parse_args()
     with open(args.config, 'r') as f:
diff --git a/utils.py b/utils.py
index b5b60c4..a3451fe 100644
--- a/utils.py
+++ b/utils.py
@@ -16,23 +16,28 @@ def fix_seeds(seed=42):
     torch.backends.cudnn.enabled = False
     torch.backends.cudnn.deterministic = True
 
+
 def remove_from_dict(the_dict, keys):
     for key in keys:
         the_dict.pop(key, None)
     return the_dict
 
+
 def prepare_bpe(config):
-    dataset_module = importlib.import_module(f'.{config.dataset.name}', data.__name__)
+    dataset_module = importlib.import_module(
+        f'.{config.dataset.name}', data.__name__)
     # train BPE
     if config.bpe.get('train', False):
-        dataset, ids = dataset_module.get_dataset(config, part='bpe', transforms=TextPreprocess())
+        dataset, ids = dataset_module.get_dataset(
+            config, part='bpe', transforms=TextPreprocess())
         train_data_path = 'bpe_texts.txt'
-        with open(train_data_path, "w") as f:
+        with open(train_data_path, "w", encoding='utf-8') as f:
             # run ovefr only train part
             for i in ids:
                 text = dataset.get_text(i)
                 f.write(f"{text}\n")
-        yttm.BPE.train(data=train_data_path, vocab_size=config.model.vocab_size, model=config.bpe.model_path)
+        yttm.BPE.train(data=train_data_path,
+                       vocab_size=config.model.vocab_size, model=config.bpe.model_path)
         os.system(f'rm {train_data_path}')
 
     bpe = yttm.BPE(model=config.bpe.model_path)
