diff --git a/configs/train_librispeech.yaml b/configs/train_librispeech.yaml
index 261ea37..896ec38 100644
--- a/configs/train_librispeech.yaml
+++ b/configs/train_librispeech.yaml
@@ -8,8 +8,8 @@ bpe:
   model_path: yttm.bpe
 train:
   seed: 42
-  num_workers: 16
-  batch_size: 32
+  num_workers: 1
+  batch_size: 8
   clip_grad_norm: 15
   epochs: 42
   optimizer:
diff --git a/train.py b/train.py
index b9a1b1f..82afd2c 100644
--- a/train.py
+++ b/train.py
@@ -121,10 +121,10 @@ def train(config):
     val_dataset = dataset_module.get_dataset(
         config, transforms=transforms_val, part='val')
     print("!!!", config.train.get('num_workers', 4))
-    train_dataloader = DataLoader(train_dataset, 
+    train_dataloader = DataLoader(train_dataset,
                                   batch_size=config.train.get('batch_size', 1), collate_fn=no_pad_collate)
 
-    val_dataloader = DataLoader(val_dataset, 
+    val_dataloader = DataLoader(val_dataset,
                                 batch_size=1, collate_fn=no_pad_collate)
 
     model = QuartzNet(
@@ -159,10 +159,9 @@ def train(config):
         # train:
         model.train()
         for batch_idx, batch in enumerate(train_dataloader):
-            print(batch)
+            # print(batch)
             batch = batch_transforms_train(batch)
-            print(batch)
-            return
+            # print(batch)
 
             optimizer.zero_grad()
             logits = model(batch['audio'])
diff --git a/train_1.ipynb b/train_1.ipynb
index 4935767..d98427f 100644
--- a/train_1.ipynb
+++ b/train_1.ipynb
@@ -2,9 +2,20 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": 32,
+   "execution_count": 1,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "c:\\Users\\nntin\\anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
+      "  warn(f\"Failed to load image Python extension: {e}\")\n",
+      "c:\\Users\\nntin\\anaconda3\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
+      "  \"class\": algorithms.Blowfish,\n"
+     ]
+    }
+   ],
    "source": [
     "#!/usr/bin/env python\n",
     "\n",
@@ -72,7 +83,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 33,
+   "execution_count": 2,
    "metadata": {},
    "outputs": [
     {
@@ -93,7 +104,7 @@
        " 'model': {'name': '_quartznet5x5_config', 'vocab_size': 120, 'feat_in': 64}}"
       ]
      },
-     "execution_count": 33,
+     "execution_count": 2,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -104,27 +115,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 89,
-   "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "22050"
-      ]
-     },
-     "execution_count": 89,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "config.dataset.get('sample_rate', 16000)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 34,
+   "execution_count": 3,
    "metadata": {},
    "outputs": [
     {
@@ -136,92 +127,12 @@
      ]
     },
     {
-     "data": {
-      "text/html": [
-       "Finishing last run (ID:pqzr4zzb) before initializing another..."
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "text/html": [
-       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "application/vnd.jupyter.widget-view+json": {
-       "model_id": "13cc2d4fd010482a8687ffd4606f97e0",
-       "version_major": 2,
-       "version_minor": 0
-      },
-      "text/plain": [
-       "VBox(children=(Label(value='0.042 MB of 0.042 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "text/html": [
-       " View run <strong style=\"color:#cdcd00\">worldly-butterfly-24</strong> at: <a href='https://wandb.ai/monash-deep-neuron/quartznet_ljspeech/runs/pqzr4zzb' target=\"_blank\">https://wandb.ai/monash-deep-neuron/quartznet_ljspeech/runs/pqzr4zzb</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "text/html": [
-       "Find logs at: <code>.\\wandb\\run-20230322_124221-pqzr4zzb\\logs</code>"
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "text/html": [
-       "Successfully finished last run (ID:pqzr4zzb). Initializing new run:<br/>"
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "application/vnd.jupyter.widget-view+json": {
-       "model_id": "324eb06540294ccdaaa2216854070864",
-       "version_major": 2,
-       "version_minor": 0
-      },
-      "text/plain": [
-       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
+      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnguyennhuttin\u001b[0m (\u001b[33mmonash-deep-neuron\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
+     ]
     },
     {
      "data": {
@@ -251,7 +162,7 @@
     {
      "data": {
       "text/html": [
-       "Run data is saved locally in <code>c:\\Users\\nntin\\uni\\fyp\\quartznet-pytorch\\wandb\\run-20230322_124344-o15p5dpm</code>"
+       "Run data is saved locally in <code>c:\\Users\\nntin\\uni\\fyp\\quartznet-pytorch\\wandb\\run-20230322_233220-i9xq719n</code>"
       ],
       "text/plain": [
        "<IPython.core.display.HTML object>"
@@ -263,7 +174,7 @@
     {
      "data": {
       "text/html": [
-       "Syncing run <strong><a href='https://wandb.ai/monash-deep-neuron/quartznet_ljspeech/runs/o15p5dpm' target=\"_blank\">treasured-star-25</a></strong> to <a href='https://wandb.ai/monash-deep-neuron/quartznet_ljspeech' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
+       "Syncing run <strong><a href='https://wandb.ai/monash-deep-neuron/quartznet_ljspeech/runs/i9xq719n' target=\"_blank\">eager-butterfly-29</a></strong> to <a href='https://wandb.ai/monash-deep-neuron/quartznet_ljspeech' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
       ],
       "text/plain": [
        "<IPython.core.display.HTML object>"
@@ -287,7 +198,7 @@
     {
      "data": {
       "text/html": [
-       " View run at <a href='https://wandb.ai/monash-deep-neuron/quartznet_ljspeech/runs/o15p5dpm' target=\"_blank\">https://wandb.ai/monash-deep-neuron/quartznet_ljspeech/runs/o15p5dpm</a>"
+       " View run at <a href='https://wandb.ai/monash-deep-neuron/quartznet_ljspeech/runs/i9xq719n' target=\"_blank\">https://wandb.ai/monash-deep-neuron/quartznet_ljspeech/runs/i9xq719n</a>"
       ],
       "text/plain": [
        "<IPython.core.display.HTML object>"
@@ -302,7 +213,7 @@
        "[]"
       ]
      },
-     "execution_count": 34,
+     "execution_count": 3,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -419,56 +330,43 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 114,
+   "execution_count": 6,
    "metadata": {},
    "outputs": [
     {
      "name": "stderr",
      "output_type": "stream",
      "text": [
-      "  0%|          | 0/42 [00:02<?, ?it/s]"
-     ]
-    },
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "212893\n",
-      "219293\n"
+      "  0%|          | 0/42 [00:02<?, ?it/s]\n"
      ]
     },
     {
-     "name": "stderr",
-     "output_type": "stream",
-     "text": [
-      "\n"
+     "ename": "OutOfMemoryError",
+     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 4.00 GiB total capacity; 3.33 GiB already allocated; 0 bytes free; 3.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
+      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3064\\2594709986.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;31m# batch1=batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m# print(len(batch1['audio'][0]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_transforms_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;31m# batch2=batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# print(len(batch2['audio'][0]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32mc:\\Users\\nntin\\uni\\fyp\\quartznet-pytorch\\data\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m               \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m               \u001b[1;31m# audiomentation transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32mc:\\Users\\nntin\\uni\\fyp\\quartznet-pytorch\\data\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32mc:\\Users\\nntin\\uni\\fyp\\quartznet-pytorch\\data\\transforms.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;32mc:\\Users\\nntin\\uni\\fyp\\quartznet-pytorch\\data\\transforms.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
+      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 4.00 GiB total capacity; 3.33 GiB already allocated; 0 bytes free; 3.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
      ]
     }
    ],
    "source": [
-    "# #! \n",
-    "batch_transforms_train = Compose([\n",
-    "    ToGpu('cuda' if torch.cuda.is_available() else 'cpu'),\n",
-    "    # NormalizedMelSpectrogram(\n",
-    "    #     sample_rate=config.dataset.get(\n",
-    "    #         'sample_rate', 16000),  # for LJspeech\n",
-    "    #     n_mels=config.model.feat_in,\n",
-    "    #     normalize=config.dataset.get('normalize', None)\n",
-    "    # ).to('cuda' if torch.cuda.is_available() else 'cpu'),\n",
-    "    AddLengths(),\n",
-    "    Pad()\n",
-    "])\n",
     "\n",
     "for epoch_idx in tqdm(range(config.train.get('epochs', 10))):\n",
     "    # train:\n",
     "    model.train()\n",
     "    for batch_idx, batch in enumerate(train_dataloader):\n",
-    "        batch1=batch\n",
-    "        print(len(batch1['audio'][0]))\n",
+    "        # batch1=batch\n",
+    "        # print(len(batch1['audio'][0]))\n",
     "        batch = batch_transforms_train(batch)\n",
-    "        batch2=batch\n",
-    "        print(len(batch2['audio'][0]))\n",
-    "        break\n",
+    "        # batch2=batch\n",
+    "        # print(len(batch2['audio'][0]))\n",
     "\n",
     "        optimizer.zero_grad()\n",
     "        logits = model(batch['audio'])\n",
@@ -482,7 +380,7 @@
     "        optimizer.step()\n",
     "        lr_scheduler.step()\n",
     "        # warmup_scheduler.dampen()\n",
-    "\n",
+    "        break\n",
     "        if batch_idx % config.wandb.get('log_interval', 5000) == 0:\n",
     "            target_strings = decoder.convert_to_strings(batch['text'])\n",
     "            decoded_output = decoder.decode(\n",
@@ -549,82 +447,36 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 38,
+   "execution_count": null,
    "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "dict_keys(['audio', 'text', 'sample_rate'])"
-      ]
-     },
-     "execution_count": 38,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
+   "outputs": [],
    "source": [
     "batch1.keys()"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 39,
+   "execution_count": null,
    "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "dict_keys(['audio', 'text', 'sample_rate', 'input_lengths', 'target_lengths'])"
-      ]
-     },
-     "execution_count": 39,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
+   "outputs": [],
    "source": [
     "batch2.keys()"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 41,
+   "execution_count": null,
    "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "tensor([1065,  210, 1066,  567,  895,  627,  925,  197,  833,  973,  498,  909,\n",
-       "         285, 1097, 1019,  581,  774,  826,  708,  516,  950,  778,  932,  867,\n",
-       "         978,  672, 1064,  654,  588,  763,  867,  781], device='cuda:0')"
-      ]
-     },
-     "execution_count": 41,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
+   "outputs": [],
    "source": [
     "batch2['input_lengths']"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 115,
+   "execution_count": null,
    "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "32\n",
-      "212893\n",
-      "219293\n",
-      "219293\n"
-     ]
-    }
-   ],
+   "outputs": [],
    "source": [
     "print(len(batch1['audio'])) # batchsize\n",
     "print(len(batch1['audio'][0]))\n",
@@ -634,25 +486,9 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 92,
+   "execution_count": null,
    "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "{'audio': array([ 0.00685196, -0.00380555,  0.00689523, ...,  0.0007928 ,\n",
-      "        0.00087616, -0.00228706], dtype=float32), 'text': array([ 56,  12,  44,   6,  75,  65,  43,  93,  88,  48,  49, 103,  46,\n",
-      "        62,  13,  99,  78,  13,  46,   5,  42,  47, 119,  56,  47,  11,\n",
-      "        79,  53,  50,  16,  51,   9,  52,  73,  10,  17,  17,  51,  11,\n",
-      "        57,  82,  19,  63,   8,  96,   4,  10,  17,  77,  86,  57,  82,\n",
-      "        19,  42,  98,  43,  42,  12,   6,  11,  72,  53,  12,   7,  17,\n",
-      "         6,  11,  87,  21,  47,  11,  79,  52,  65,  74,   5,  83,  27,\n",
-      "        13,  10,  24,  62,  80]), 'sample_rate': 22050}\n",
-      "(212893,)\n"
-     ]
-    }
-   ],
+   "outputs": [],
    "source": [
     "a=train_dataset.dataset.__getitem__(0)\n",
     "print(a)\n",
@@ -662,19 +498,9 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 96,
+   "execution_count": null,
    "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "{'audio': tensor([[-7.3242e-04, -7.6294e-04, -6.4087e-04,  ...,  7.3242e-04,\n",
-      "          2.1362e-04,  6.1035e-05]]), 'text': 'Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition', 'sample_rate': 22050}\n",
-      "torch.Size([1, 212893])\n"
-     ]
-    }
-   ],
+   "outputs": [],
    "source": [
     "# load datasets\n",
     "train_dataset_2 = dataset_module.get_dataset(\n",
@@ -687,20 +513,9 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 111,
+   "execution_count": null,
    "metadata": {},
-   "outputs": [
-    {
-     "data": {
-      "text/plain": [
-       "96"
-      ]
-     },
-     "execution_count": 111,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
+   "outputs": [],
    "source": [
     "len(batch1['audio'][0])\n",
     "len(batch2['audio'][0])\n",
diff --git a/wandb/run-20230322_124344-o15p5dpm/files/config.yaml b/wandb/run-20230322_124344-o15p5dpm/files/config.yaml
index fc644ef..b3c8bf4 100644
--- a/wandb/run-20230322_124344-o15p5dpm/files/config.yaml
+++ b/wandb/run-20230322_124344-o15p5dpm/files/config.yaml
@@ -8,6 +8,7 @@ _wandb:
     is_jupyter_run: true
     is_kaggle_kernel: false
     python_version: 3.9.15
+    session_history: code\_session_history.ipynb
     start_time: 1679449429.631559
     t:
       1:
@@ -24,6 +25,7 @@ _wandb:
       - 55
       3:
       - 1
+      - 2
       - 16
       - 23
       4: 3.9.15
diff --git a/wandb/run-20230322_124344-o15p5dpm/files/wandb-summary.json b/wandb/run-20230322_124344-o15p5dpm/files/wandb-summary.json
index 9e26dfe..44a4919 100644
--- a/wandb/run-20230322_124344-o15p5dpm/files/wandb-summary.json
+++ b/wandb/run-20230322_124344-o15p5dpm/files/wandb-summary.json
@@ -1 +1 @@
-{}
\ No newline at end of file
+{"_wandb": {"runtime": 81}}
\ No newline at end of file
diff --git a/wandb/run-20230322_124344-o15p5dpm/run-o15p5dpm.wandb b/wandb/run-20230322_124344-o15p5dpm/run-o15p5dpm.wandb
index 7b76311..15adb1e 100644
Binary files a/wandb/run-20230322_124344-o15p5dpm/run-o15p5dpm.wandb and b/wandb/run-20230322_124344-o15p5dpm/run-o15p5dpm.wandb differ
